{
    "id": "e3a8ee11a1a4012dd6af4ee4d078f74b",
    "metadata": {
        "id": "e3a8ee11a1a4012dd6af4ee4d078f74b",
        "url": "https://www.hopsworks.ai/post/modularity-and-composability-for-ai-systems-with-ai-pipelines-and-shared-storage/",
        "title": "Modularity and Composability for AI Systems with AI Pipelines and Shared Storage  - Hopsworks",
        "properties": {
            "description": "We present an AI software factory architecture that decompose an AI system into AI pipelines, that later can be easily composed into an AI system.",
            "keywords": null,
            "author": null,
            "og:title": "Modularity and Composability for AI Systems with AI Pipelines and Shared Storage  - Hopsworks",
            "og:description": "We present an AI software factory architecture that decompose an AI system into AI pipelines, that later can be easily composed into an AI system.",
            "og:image": "https://cdn.prod.website-files.com/618399cd49d125734c8dec95/667acae0ae637d5bb9a72ea2_modularity%20%26%20composability.png",
            "og:type": "website",
            "twitter:card": "summary_large_image"
        }
    },
    "parent_metadata": {
        "id": "4ebe8cd65255dbbfafdc2cea3d78ecaf",
        "url": "https://www.notion.so/Components-Architecture-and-System-Design-4ebe8cd65255dbbfafdc2cea3d78ecaf",
        "title": "Components, Architecture, and System Design",
        "properties": {
            "Type": "Leaf"
        }
    },
    "content": "![](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/61f958575ff62d320d46701e_closeicon.png)\n\nScheduled upgrade from November 26, 07:00 UTC to November 26, 17:00 UTC\n\nKindly note that during the maintenance window, app.hopsworks.ai will not be accessible.\n\n5\n\n[View the Changes](https://www.hopsworks.ai/news/hopsworks-4-0-breaking-changes)\n\n![](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/61f958575ff62d320d46701e_closeicon.png)\n\nScheduled upgrade from November 26, 07:00 UTC to November 26, 17:00 UTC\n\nKindly note that during the maintenance window, app.hopsworks.ai will not be accessible.\n\n5\n\n[View the Changes](https://www.hopsworks.ai/news/hopsworks-4-0-breaking-changes)\n\nScheduled upgrade from November 26, 07:00 UTC to November 26, 17:00 UTC\n\n[Contact](/contact/main)[Login](https://app.hopsworks.ai)[![Github Mark](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/6418216a570b0da3d471661a_icons8-slack-new.svg)](https://join.slack.com/t/public-hopsworks/shared_invite/zt-1uf21vitz-rhHKNdIf8GEiOf1EJ6Wzsw)[![Github Mark](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/62261cde4669f63d3880938d_github.svg)](https://github.com/logicalclocks/hopsworks)[![linkedin logo](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/61926637aaba4d3968d7956d_linkedin.svg)](https://www.linkedin.com/company/hopsworks/)[![Twitter icon](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/66a346ea27ec6d7c0e354747_icons8-twitter%20\\(1\\).svg)](https://twitter.com/hopsworks)\n\n[![Untitled UI logotext](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/6202a13e7cafec5553703f6b_logo.svg)![Logo](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/666c3cc1cfc4741e6b2d9fe6_untitled-ui-logo.png)](/)\n\nProduct\n\nProduct\n\n[![](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/5fad49f715203ed9d66fc1b0_Hops%20Icon%20Green.png)Hopsworks EnterpriseFull edition of Hopsworks, high levels of SLAs and support.](/try)[IntegrationsLeverage your existing data sources and tools.](/integrations)[ExamplesGet up and running on new features and techniques.](/hopsworks-examples)[FAQAll you need to know about Hopsworks.](/frequently-asked-questions)\n\nCapabilities\n\n[Hopsworks On-PremisesManage everything securely within your data center.](https://www.hopsworks.ai/product-capabilities/feature-store-on-premises)[Performance & High AvailabilityHighest performance requirements in the industry.](https://www.hopsworks.ai/product-capabilities/operational-performance-and-high-availability)[Feature Engineering in PythonPython-first collaborative environment.](https://www.hopsworks.ai/product-capabilities/feature-engineering-in-python)[Other capabilitiesRead about our extended platform capabilities.](/product-capabilities)\n\nSolutions\n\n[Generative AICreate custom LLM-powered products & services.](/use-case/fine-tuning-llms-rag-for-genai)[Real-time Fraud DetectionCreate a real-time system to monitor transactions & patterns.](/use-case/realtime-fraud-detection)[Hopsworks Medical CopilotBoost provider productivity & enhance patient care.](https://www.hopscopilot.com/)[CustomersExplore how our customers leverage Hopsworks.](/customers)\n\n[Pricing](/pricing)[Blog](/blog)\n\nResources\n\n[MLOps DictionaryComprehensive terminology guide for ML solutions.](/mlops-dictionary)[DocumentationDetailed information to help you effectively utilize Hopsworks.](https://docs.hopsworks.ai/latest/)[Research PapersDiscover how our research is driving innovation.](/research-papers)[CommunityJoin our community and get all your questions answered. ](https://community.hopsworks.ai/)\n\n[EventsOnline & Offline sessions and workshops. ](/events)[AcademyEverything about ML Systems, and the Hopsworks platform.](/academy)[Feature Store ComparisonIn-depth comparisons of feature stores highlighting key features.](https://www.hopsworks.ai/product-comparison/sagemaker)[FAQ: EU AI ActA complete guide to The EU AI Act.](/faq-eu-ai-act)\n\nCompany\n\n[About usLearn more about our team. ](/about-us)[NewsThe latest industry news, updates and info.](/news)[Security & ComplianceRobust security and compliance with industry standards.](/security-compliance)\n\n[![](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/66a0b13c473a71304470c35a_oreilly_logo_mark_red.svg)Book](/lp/oreilly-book-building-ml-systems-with-a-feature-store)[Benchmarks](/index#performance)\n\n[![](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/63e4e90bd6c2ad05ecd89669_icons8-great-britain-96.png)EN](#)[![](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/63e4e90b88b00c69a52f92cc_icons8-germany-96.png)DE](#)\n\n[![](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/63e4e90bd6c2ad05ecd89669_icons8-great-britain-96.png)EN](#)[![](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/63e4e90b88b00c69a52f92cc_icons8-germany-96.png)DE](#)\n\n[![arrow back](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/6183f249d69379869e0b3524_icons8-chevron-left-30.png)Back to Blog](/blog)\n\n[![arrow back](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/6183f249d69379869e0b3524_icons8-chevron-left-30.png)Back to Blog](/blog)\n\nJim Dowling\n\n[![link to linkedin](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/61926637aaba4d3968d7956d_linkedin.svg)](https://www.linkedin.com/in/jim-dowling-206a98/)\n\nCEO and Co-Founder\n\n**Let's keep in touch!**\n\n**Subscribe to our newsletter and receive the latest product updates, upcoming events, and industry news.**\n\n**More Blogs**\n\n[ How AI Will Redefine Retail in 2025](/post/how-ai-will-redefine-retail-in-2025)\n\n[Amazon FSx for NetApp ONTAP interoperability test in a Hopsworks 4.x Deployment](/post/amazon-fsx-for-netapp-ontap-interoperability-test-in-a-hopsworks-4-x-deployment)\n\n[Breaking Down FSI Regulations for AI in 2025](/post/breaking-down-fsi-regulations-for-ai-in-2025)\n\n[Hopsworks PKI: The Unseen Hero](/post/hopsworks-pki-the-unseen-hero)\n\n[Air-gapped Installations in Hopsworks](/post/air-gapped-installations-in-hopsworks)\n\nArticle updated on\n\n# Modularity and Composability for AI Systems with AI Pipelines and Shared Storage\n\nA Unified Architecture for Batch, Real-Time, and LLM AI Systems\n\n[![link to github](https://cdn.prod.website-files.com/5e6f7cd3ee7f51d539a4da0b/605b3c459e87eff3298d0e25_github%20\\(1\\).svg)](https://github.com/logicalclocks/hopsworks)\n\n[![Share on Twitter](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/61b0ae07358bb3d1224410c1_Twitter%20icon.svg)](https://twitter.com/share?url=https://www.hopsworks.ai/post/modularity-and-composability-for-ai-systems-with-ai-pipelines-and-shared-storage)\n\n[![share on linkedin](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/61926637aaba4d3968d7956d_linkedin.svg)](https://www.linkedin.com/shareArticle?mini=true&url=https://www.hopsworks.ai/post/modularity-and-composability-for-ai-systems-with-ai-pipelines-and-shared-storage&title=Modularity and Composability for AI Systems with AI Pipelines and Shared Storage - Hopsworks)\n\nJune 25, 2024\n\n25 min\n\nRead\n\nJim Dowling\n\n[Jim Dowling](https://www.linkedin.com/in/jim-dowling-206a98/)[![link to linkedin](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/61926637aaba4d3968d7956d_linkedin.svg)](https://www.linkedin.com/in/jim-dowling-206a98/)\n\nCEO and Co-Founder\n\nHopsworks\n\n[MLOps](/blog-categories/mlops)\n\n[Data Engineering](/blog-categories/data-engineering)\n\n[Feature Store](/blog-categories/feature-stores)\n\n[SIGMOD 2024](/blog-categories/sigmod-2024)\n\n## TL;DR\n\nModularity in software refers to decomposing a system into smaller, more manageable modules that can be independently developed and composed into a complete software system. Modularity helps us build better quality, more reliable software systems, as modules can be independently tested. AI systems can also benefit from modularity, enabling teams to build higher quality AI systems, faster. However, a lesson our community learnt by getting burnt with microservices was that modularity only helps if the modules can be easily composed into functioning systems. In this article, we argue that a shared storage layer with well-defined APIs should be the main mechanism for composing the modules that make up an AI system - from data collection and feature engineering, to model training, to inference. In particular, we introduce the feature/training/inference (FTI) architecture as a unified architecture building real-time, batch, and LLM AI systems, where the feature store and model registry acting as the shared storage layer. The feature store provides well-defined DataFrame APIs for reading and writing data (tabular data and embeddings), while the model registry provides APIs for storing/retrieving models and metadata around models. These highly available stateful services enable the modularization of AI systems into feature, training, and inference pipelines that provide a natural decomposition of work for data engineering, data science, and ML/AI engineering teams.\n\n[Introduction](#introduction)  \n---  \n[1 - A Brief History of Modularity and Composability for AI Systems](#brief-history)  \n[a. Case Study: the evolution of MLOps in GCP from Microservices to Shared State](#case-study)  \n[b. What is an AI pipeline?](#ai-pipeline)  \n[c. AI Systems as Modular Stateful Systems](#ai-systems)  \n[2 - The FTI Pipeline Architecture](#fti-pipeline)  \n[a. AI Pipelines as Contracts](#ai-contracts)  \n[3 - Unified Architecture for AI Systems](#unified-architecture)  \n[a. LLM AI Systems](#llm-systems)  \n[b. Batch AI Systems](#batch-systems)  \n[c. Real-Time AI Systems](#realtime-systems)  \n[5 - Summary](#summary)  \n  \nThis article is part 1 in a 7 part series describing in lay terms concepts and results from a [SIGMOD 2024 research paper on the Hopsworks Feature Store](https://www.hopsworks.ai/research-papers/the-hopsworks-feature-store-for-machine-learning). \n\nOther Parts: 2 ([The Taxonomy for Data Transformations](http://www.hopsworks.ai/post/a-taxonomy-for-data-transformations-in-ai-systems)), 3 ([Use all features: Snowflake Schema)](https://www.hopsworks.ai/post/the-journey-from-star-schema-to-snowflake-schema-in-the-feature-store) , 4 ([Lakehouse for AI](https://www.hopsworks.ai/post/the-feature-store-makes-your-data-warehouse-easy-to-use-for-ai)), 5 ([From Lakehouse to AI Lakehouse](https://www.hopsworks.ai/post/from-lakehouse-to-ai-lakehouse-with-a-python-native-query-engine)), 6[ (Real-Time AI Database),](https://www.hopsworks.ai/post/rondb-a-real-time-database-for-real-time-ai-systems) 7 ([Reproducible Data](http://www.hopsworks.ai/post/reproducible-data-for-the-ai-lakehouse)). \n\n## Introduction\n\nModularity and composability are the Yin and Yang of systems development. In this article, we introduce a blueprint for a software factory for AI that shows you how to both decompose an AI system into independent, modular components (AI pipelines) that can then later be easily composed into an AI system using a shared storage layer. Just like a factory, each AI pipeline will play a well-defined role in transforming input data into features and models, and using trained models and new input data to make predictions. Just like a factory, AI artifacts will be produced at intermediate stages and seamlessly integrated into the AI systems that generate value and justify the investment in the factory. We will pay attention to reuse of intermediate outputs to keep costs down and improve quality. \n\nThe main contribution of this article is a unified software architecture for batch, real-time, and LLM AI systems that is based on a shared storage layer and a decomposition of machine learning (ML) pipelines into _feature pipelines_ (that transform input data to features/labels), _training pipelines_ (that transform features/labels into trained models), and _inference pipelines_ that transform new features into predictions using trained models.\n\n## A Brief History of Modularity and Composability for AI Systems\n\nIn the 1980s, with the advent of local area networking, software systems made the transition from monolithic application architectures to client-server systems. With the advent of the Internet and Web Applications in the 1990s, the industry moved to the 3-tier application architecture, where the business logic was separated from the presentation layer, with a database as the backend layer. This was a natural decomposition for web applications that served well for many years until data volumes increased and more scalable architectures were needed. In the early 2010s, microservices emerged as an alternative architecture to the then dominant monolithic 3-tier applications that became expensive to maintain and difficult to scale. By decomposing large systems into microservices, different teams could work independently and with well-defined interfaces between loosely coupled microservices, systems could be composed as connected graphs of microservices. As microservice architectures grew in complexity, see Figure 2, they introduced new problems when they were composed together into complete systems. When microservices are used in larger systems, it becomes hard to update their APIs (without proper versioning support, which requires multi-version support). When graphs of RPC calls become deep, they become hard to trace. When state is fragmented over many different databases (they often have their own local database), it makes system-level backup and recovery harder. High unpredictable latencies are a consequence of the [tail-at-scale](https://cacm.acm.org/research/the-tail-at-scale/). And in general, high availability is challenging (following Leslie Lamport’s maxim that \"a distributed system is one where you can't get your work done because some machine you've never heard of is broken.\").\n\n[![How microservices decompose systems into manageable modules](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a343750f7b6fdfabcd7990_66795e50b805ed1c3800d30d_2_microservices_lightbox.png)](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a343750f7b6fdfabcd7990_66795e50b805ed1c3800d30d_2_microservices_lightbox.png)\n\n**_Figure 1:_**_Microservices decompose systems into manageable modules, but introduce new challenges in composing them into highly available, performant, observable services. Image from_[ _Zhang et Al_](https://arxiv.org/html/2401.02920v1) _._\n\nThere has, therefore, been a natural swing back towards more centralized (now serverless) architectures (aka [macroservices](https://nordicapis.com/whats-the-difference-microservice-macroservice-monolith/)) to prevent the composability and operational challenges that can spiral out of control in microservice architectures (see [this funny video](https://www.youtube.com/watch?v=y8OnoxKotPQ) that captures the essence of the problem). \n\nWhat is the lesson here for AI systems? \n\nThe first lesson is that if you decompose your AI system into too many fine-grained services, you increase complexity when you need to compose your system. Alternatively, if you have a single monolithic end-to-end system, it will not be maintainable and there will be little to no reuse of its components across other projects. \n\nThe second point is that AI systems are a diverse bunch. AI systems are not always operational systems (applications that run 24x7). Some AI systems are batch systems that run on a schedule producing predictions (think Spotify weekly that produces recommendations for songs for the coming week). Other AI systems are operational machine-to-machine systems, such as a real-time credit-card fraud detection system. Other AI systems are user-facing operational systems, such as a LLM powered chatbot. \n\nThe third point is that all AI systems have some offline/batch component to them - whether that is collecting data to retrain models, ingesting data for RAG, or training models. In many production AI systems, model training is run on a schedule to prevent model degradation from negatively impacting the performance of the AI system.\n\nThen, we have to consider the main technical and operational challenge in building AI systems, which is managing state. State complicates building modular and composable AI systems. The solution that microservices architectures embrace is (1) local state stored at microservices (which is problematic when you need transactional operations that cross multiple microservices and also when you need to make your system highly available), and (2) stateless microservices that use one or more external data stores, such as a database, key-value store, or event bus. Operationally, the lowest cost architecture is often stateless microservices that share a common scalable operational data layer.\n\nSo, what is the equivalent state in AI systems? The minimal viable state that an AI system has to manage is:\n\n  * data for training models;\n  * the trained models themselves;\n  * data for inference.\n\n\n\nData for training and inference is typically mutable data (data never stops coming), while the trained models are immutable. This decomposition of state in AI systems leads naturally to the prototypical 3-stage architecture for AI systems:\n\n  * [feature engineering](https://www.hopsworks.ai/dictionary/feature-engineering) to manage data for training and inference (the training datasets don’t create themselves, you know!), \n  * the offline model training process to create the trained models, and \n  * the (batch or online) inference systems that make the predictions with the model and [inference data](https://www.hopsworks.ai/dictionary/inference-data). \n\n\n\nYou might think that these separate stages should all be connected in one directed acyclic graph (DAG), but you would be wrong. Training does not happen as part of inference - they are separate processes that run at their own cadences (you run the training process when you need a new model, inference when you need to make a prediction). We will see later the benefits of making feature engineering its own process, ensuring consistent feature data for training and inference (preventing [training/serving skew](https://www.hopsworks.ai/dictionary/training-serving-skew)). You may think this decomposition is reasonable, but have the opinion that it is too coarse-grained. We will also see later that if you want to further decompose any of these three stages, you can easily do so. The key technology enabling this decomposition is a stateful ML infrastructure layer. Let’s dive into managing state in AI systems.\n\n### Case Study: the evolution of MLOps in GCP from Microservices to Shared State\n\nA few years ago, Google Cloud promoted building real-time ML systems as compositions of microservices, see Figure 2. This was based on their TensorFlow Extended (TFX) architecture that recently morphed (along with KubeFlow) into vertex pipelines.\n\n[![ML systems built from separate indepedent stateless microservices](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a343750f7b6fdfabcd796f_66796f60836c41d27427b69a_composable%2520ml%2520systems_lightbox.png)](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a343750f7b6fdfabcd796f_66796f60836c41d27427b69a_composable%2520ml%2520systems_lightbox.png)\n\n**_Figure 2:_**_Should a composable real-time machine learning system be built from separate independent stateless microservices?_[_Images_](https://www.youtube.com/watch?v=w5q0HHSu7GA) _from Lak Lakshmanan in Google Cloud._\n\nAround 2023 (with the advent of GCP Vertex), Google started prompting a MLOps architecture for real-time AI systems, which has some stateful services (the feature store, model registry, and ML metadata store), and only one monolithic AI pipeline (data extraction, data validation, data preparation model training, model evaluation, model validation). \n\n[![MLOps architecture](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a343740f7b6fdfabcd796b_66797052071f6b8f9d559d23_mlops%2520architecture_lightbox.png)](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a343740f7b6fdfabcd796b_66797052071f6b8f9d559d23_mlops%2520architecture_lightbox.png)\n\n** _Figure 3_** _: MLOps architecture for a real-time_ AI _system by GCP (Image from GCP as of_[ _May 2024_](https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning) _). The architecture has a mix of orchestrated tasks and stateful services, with numbers showing you where it starts and finishes. It is very confusing and a consultant’s dream._\n\nBased on this MLOps architecture, an AI pipeline seems to have as input raw data and produces a model as output. [The only definition I could find from GCP Vertex](https://cloud.google.com/vertex-ai/docs/pipelines/introduction#ml-pipeline) was this:“An AI pipeline is a portable and extensible description of an MLOps workflow as a series of steps called pipeline tasks. Each task performs a specific step in the workflow to train and/or deploy an ML model.” This definition implies the output of an AI pipeline is a trained model and/or a deployed model. But is that the accepted definition of an AI pipeline? Not in this article, where we argue that feature engineering and inference pipelines are also part of both MLOps and AI systems, in general.\n\n### What is an AI pipeline?\n\nThe unwritten assumption among many MLOps systems is that you can modularize an AI system by connecting independent [AI pipelines](https://www.hopsworks.ai/dictionary/ai-pipelines) together. But what if you only have one monolithic AI pipeline, like GCP? Is there an alternative, more fine-grained, decomposition of an AI system?\n\nYes. If you have a feature store, you can have feature pipelines that create feature data and store it there, along with labels (observations) for supervised machine learning (ML) models. The feature store enables a training data pipeline that starts by reading training data from the feature store, trains a model, and saves the trained model to a model registry. The model registry, in turn, enables an inference pipeline that reads feature (inference) data (from the feature store or from client requests) and the trained model and outputs predictions for use by an AI-enabled application. \n\nSo, what is an AI pipeline? **An AI pipeline is a program that either runs on a schedule or continuously, has well-defined input data, and creates one or more AI artifacts as output.** We typically name an AI pipeline after the AI artifact(s) they create - a feature pipeline creates features, a training pipeline outputs a trained model, or an inference pipeline outputs predictions (makes inferences). Occasionally, you may name an AI pipeline based on how they modify an AI artifact - such as a model or feature validation pipeline that asynchronously validates a model or feature data, respectively. Or you could have a training dataset pipeline that materializes feature data from the feature store as files. The point here is that the term AI pipeline is abstract, not a concrete pipeline. When you want to be precise in discussing your AI system, always refer to the concrete name for the AI pipeline based on the AI artifact it outputs. If somebody asks you how to automate feature engineering for their AI system, telling them to build an AI pipeline conveys less information than telling them to build a feature pipeline (which implies the input data is the raw data for features, and the output is reusable feature data stored in a feature store).\n\n### AI Systems as Modular Stateful Systems\n\nAn AI system that is trained on a single (static) dataset and only makes a single prediction with a test set from that static dataset can only generate value once. It’s not much of a system, if it only runs once. AI systems need to manage state to be able to continually generate value with new data. In Figure 4, we can see how an AI system manages data by, over time, producing new training datasets, training new models, and making new inference data available to generate new predictions, continually generating value. Production AI systems are rarely trained on a static training dataset. Instead, they typically start with the batch/streaming/real-time data that is used to create the training datasets.\n\n[![An AI system is a factory that produces ML assets, including: static training datasets, batches inference data, versioned models, and predictions](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a343740f7b6fdfabcd7967_667970bd39f81981008d67b9_ai%2520system%2520factory_lightbox.png)](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a343740f7b6fdfabcd7967_667970bd39f81981008d67b9_ai%2520system%2520factory_lightbox.png)\n\n**_Figure 4:_**_An AI system is a factory that produces ML assets, including: static training datasets, batches inference data, versioned models, and predictions._\n\nWith the advent of GenAI and pretrained models, you may think the above architecture does not apply to your AI system, as your [large language model (LLM)](https://www.hopsworks.ai/dictionary/llms-large-language-models) is pre-trained! But if you plan to fine-tune a LLM or use RAG (retrieval augmented generation), the above architecture still applies. You will need to create new training datasets for fine-tuning or update your indexes for RAG (e.g., in a [vector database](https://www.hopsworks.ai/dictionary/vector-database)). So, whatever non-trivial AI system you build, you will need to manage newly arriving data. Your AI system will also need to manage the programs (AI pipelines) that create the features, models, and predictions from your data. So, let’s look now at the programs (pipelines) in AI systems.\n\n## The FTI Pipeline Architecture\n\nThe programs that make up an AI system handle its main concerns - ingesting and managing the training/inference data (cleaning, validating, transforming), training the models with training data, and inference (making predictions) with models and inference data.\n\nThere are many [different types of AI systems](https://www.linkedin.com/posts/aurimas-griciunas_what-are-the-four-%3F%3F%3F%3F%3F%3F%3F-%3F%3F%3F%3F-activity-7158752224817393666-pwm1/) - Batch, Real-Time, Streaming, and embedded systems, distinguished by how they make their predictions. Batch AI system produce predictions in batches, on a schedule. Real-time systems take prediction requests and return low-latency prediction responses. Streaming applications can use a model to make predictions on incoming streaming data. Embedded AI systems are embedded applications that typically make predictions on the data they acquired locally through sensors or network devices. The type of AI system is independent of the ML framework used - LLMs, decision trees, CNNs, logistic region, and so on.\n\n‍\n\nDespite this heterogeneity in the types of AI systems,they have commonality in their core architecture, see Table 1. They all have programs that implement a set of data transformation steps, from ingesting raw data to refining that data into features (inputs for training and inference) and labels. Model training and inference can also be seen as (data transformation) functions. Model training takes features and labels as input and transforms it into a trained model as output. Inference takes a trained model and features as input and transforms them into predictions as output.\n\nSo, at a minimum, all AI systems have data transformation steps and state in the form of features, models, and predictions. Data transformations are the functions, whilst features, models, and predictions are the state in our AI pipelines.\n\n[![:The most common ML steps and the assets created at each step.](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a343750f7b6fdfabcd7981_667bbed852383ed657ebd4b5_1_ml%2520steps_lightbox.png)](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a343750f7b6fdfabcd7981_667bbed852383ed657ebd4b5_1_ml%2520steps_lightbox.png)\n\n**_Table 1:_**_The most common ML steps and the assets created at each step._\n\nThis commonality is illustrated in an architecture diagram in Figure 5 as a set of three AI pipelines, connected by a shared storage layer. [We call this the FTI (feature, training, inference) architecture:](https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines)\n\n  * [feature pipelines](https://www.hopsworks.ai/dictionary/feature-pipeline) that transform raw data into reusable feature data,\n  * [training pipelines](https://www.hopsworks.ai/dictionary/training-pipeline) that transform feature data (and labels) into trained models,\n  * [inference pipelines](https://www.hopsworks.ai/dictionary/inference-pipeline) that transform feature data and trained models into predictions,\n  * a shared storage layer consisting of a [feature store](https://www.hopsworks.ai/dictionary/feature-store) and [model registry](https://www.hopsworks.ai/dictionary/model-registry), where outputs of pipelines and inputs to pipelines are stored and read, respectively.\n\n\n\nFigure 5 is an abstract representation of an AI system using the FTI architecture.\n\n[![FTI Pipelines](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a343740f7b6fdfabcd7963_667974528579d9337c38e186_fti_lightbox.png)](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a343740f7b6fdfabcd7963_667974528579d9337c38e186_fti_lightbox.png)\n\n**_Figure 5:_**_Feature Pipelines, Training Pipelines, Inference Pipelines are the independent AI Pipelines that together make up a ML System._\n\nYou may also think that our decomposition of an AI system into FTI pipelines is too coarse-grained and there are many systems that are not architected this way. However, the AI pipelines in the FTI architecture can be refactored into smaller, yet still composable pipelines, see Table 2, connected by the same data layer. A good practice for AI pipelines is to name them after the asset they produce - this naming pattern communicates its expected output in the AI system.\n\n[![Fine-grained AI pipelines](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a343750f7b6fdfabcd7993_667bbf2bbe0009689ad189f6_2_fine-grained%2520pipelines_lightbox.png)](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a343750f7b6fdfabcd7993_667bbf2bbe0009689ad189f6_2_fine-grained%2520pipelines_lightbox.png)\n\n**_Table 2:_**_Fine-grained AI pipelines, named after the assets they create._\n\nLet’s examine the examples of fine-grained AI pipelines from Table 3. We can refactor our feature pipeline to consist of the original feature pipeline (create features from raw input data) and a feature validation pipeline that validates feature data asynchronously after it has landed in the feature store. Similarly, model validation can be refactored out of a training pipeline into its own model validation pipeline. You might need a separate model validation pipeline if model training uses expensive GPUs and model validation takes a long time and only needs CPUs. [Feature monitoring](https://www.hopsworks.ai/dictionary/feature-monitoring) and model monitoring often have their own pipelines, as is the case for inference logging for real-time AI systems.\n\n### AI Pipelines as Contracts\n\nAI pipelines also have a well-defined input and output interface (or [schema](https://www.hopsworks.ai/dictionary/schema)). For any AI pipeline, you should be able to write its contractual interface not just as its typed input data and output data, but also list its preconditions, postconditions, invariants, and non-functional requirements, as in Table 3.\n\n[![AI pipeline information description](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a343750f7b6fdfabcd798b_667bbf6ce0dba898b5878eaf_3_ai%2520pipeline%2520description_lightbox.png)](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a343750f7b6fdfabcd798b_667bbf6ce0dba898b5878eaf_3_ai%2520pipeline%2520description_lightbox.png)\n\n**_Table 3:_**_Examples of some of the information that you can capture in contracts describing AI pipelines. Contracts help downstream consumers of the AI pipeline output understand how to use their outputs and what they can rely on._\n\n## Unified Architecture for AI Systems\n\nThe FTI architecture is a unified architecture for structuring AI systems, because the same architecture can be used to decompose:\n\n  * LLM AI Systems\n  * Batch AI Systems\n  * Real-Time AI Systems\n\n\n\nIn the following sections, we describe these systems in terms of the FTI pipeline architecture.\n\n### LLM AI Systems\n\nTable 4 shows a concrete example of the FTI architecture in terms of a LLM system, that includes both [fine-tuning](http://www.hopsworks.ai/dictionary/fine-tuning-llms) and [retrieval augmented generation (RAG)](https://www.hopsworks.ai/dictionary/retrieval-augmented-generation-llm). \n\n![The FTI architecture in terms of a LLM system](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a343750f7b6fdfabcd797e_667bbfb6b3fa528553a0b4c5_4_llm%2520pipelines_ligthbox.png)\n\n** _Table 4:_**_The FTI pipeline architecture describes an AI system that performs both fine-tuning and RAG for LLMs. Feature pipelines can chunk text that is then transformed into vector embeddings and stored in a vector DB. The same text is used to create_[ _instruction datasets_](https://www.hopsworks.ai/dictionary/instruction-datasets-for-fine-tuning-llms) _to fine-tune a foundation LLM. The AI system then combines the user prompt with any RAG data from the vector DB to query the LLM and return a response._\n\nWe notice from Table 4 that the output of our feature pipeline now includes vector [embeddings](https://www.hopsworks.ai/dictionary/embedding) that should be indexed for approximate nearest neighbor (ANN) search. You could use a vector database to index the vector embeddings, but some feature stores (e.g., Hopsworks) have been extended to support vector embeddings with ANN search. So, you don’t have to add the extra data platform (vector database) to your ML infrastructure. Your choice.\n\n### Batch AI Systems\n\nA batch AI system uses one or more models to make predictions on a schedule using batches of new inference data. Table 5 shows the main AI pipelines from the FTI architecture in terms of a batch AI system. \n\n[![The FTI architecture in terms of a batch AI system. ](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a343750f7b6fdfabcd7987_667bbff0d046d39798b6379f_5_batch%2520ai%2520pipelines_lightbox.png)](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a343750f7b6fdfabcd7987_667bbff0d046d39798b6379f_5_batch%2520ai%2520pipelines_lightbox.png)\n\n**_Table 5:_**_The FTI pipeline architecture describes a Batch AI system as a batch feature pipeline, a training pipeline, and a batch inference pipeline that runs on a schedule._\n\nBatch AI systems run an inference pipeline on a schedule that takes new data and one or more trained ML models to make predictions that are typically stored in a database for later use. Batch AI systems are relatively easy to operate, as failures are not always time-critical - you have to fix a broken inference pipeline before its next scheduled run. They can also be made to scale to huge data volumes, with technologies such as PySpark.\n\n### Real-Time AI Systems\n\nA real-time (interactive) AI system takes user input and uses one or more models to make one or more predictions that are returned as a response to the client. Table 6 shows the main AI pipelines from the FTI architecture in terms of a real-time AI system.\n\n[![The FTI architecture in terms of a real-time AI system.](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a343750f7b6fdfabcd7984_667bc02970b334ef167fb496_6_real%2520time%2520ai%2520pipelines_lightbox.png)](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a343750f7b6fdfabcd7984_667bc02970b334ef167fb496_6_real%2520time%2520ai%2520pipelines_lightbox.png)\n\n**_Table 6:_**_The FTI pipeline architecture describes a Real-Time (Interactive) AI system. Streaming feature pipelines result in fresher features._\n\nReal-time AI systems typically use streaming feature pipelines if they need very fresh feature data in the feature store. For example, [TikTok uses Flink ](https://arxiv.org/pdf/2209.07663)to ensure that clicks by users are available for use as features within a few seconds. Online inference pipelines need to be available 24x7, and are operational services that are typically deployed along with the model in model-serving infrastructure, such as KServe, MLFlow, Seldon, BentoML, AWS Sagemaker, or GCP Vertex.\n\n## Summary\n\nBreak the monolith. Decompose your AI systems into modular, maintainable[ AI (or machine learning) pipelines](https://www.hopsworks.ai/dictionary/ml-pipeline) with clear input and output interfaces. But remember, modularity without ease of composition of those modules is a fool’s errand. The most natural decomposition for AI systems is the data preparation stage, the model training stage, and the inference stage. Different teams can take responsibility for the three different stages and you can easily specify the contracts for the AI artifacts produced by the FTI pipelines in terms of preconditions, postconditions, invariants, and non-functional requirements. The FTI pipeline architecture makes use of shared storage to connect your AI pipelines with a feature store and model registry, and it has usurped the shared-nothing storage architecture from microservices as the best practice for architecting AI systems. \n\n## References\n\n![](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/663b8c6a964c76be96237444_written%20by%20human_hops.png)![](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/663b8ae5a0a33abc94baf561_written%20by%20ai_hops.png)\n\n### Interested for more?\n\n  * 🤖 Register for free on [Hopsworks Serverless](https://app.hopsworks.ai/app?utm_source=blog&utm_medium=list&utm_id=backlink)\n  * 🌐 Read about the open, disaggregated [AI Lakehouse stack](https://www.hopsworks.ai/post/the-ai-lakehouse)\n  * 📚 Get your early copy: O'Reilly's ['**Building Machine Learning Systems'**](https://www.hopsworks.ai/lp/oreilly-book-building-ml-systems-with-a-feature-store)book\n  * 🛠️ Explore all [Hopsworks Integrations](/integrations)\n  * 🧩 Get started with [codes and examples](/hopsworks-examples)\n  * ⚖️ [Compare other Feature Stores](https://www.hopsworks.ai/product-comparison/sagemaker) with Hopsworks\n\n\n\n### More blogs\n\n[![Learn how to connect Hopsworks to Snowflake and create features and make them available both offline in Snowflake and online in Hopsworks Feature Store.](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/637647a53a162c90114c1d6b_Snowflake%20Data%20Into%20Features.png)](/post/how-to-transform-snowflake-data-into-features-with-hopsworks)[Feature StoreDecember 7, 20218 minReadHow to Transform Snowflake Data into Features with HopsworksLearn how to connect Hopsworks to Snowflake and create features and make them available both offline in Snowflake and online in Hopsworks.](/post/how-to-transform-snowflake-data-into-features-with-hopsworks)\n\nFabio Buso\n\n[![Read about how Sweden’s largest bank trained generative adversarial neural networks \\(GANs\\) using NVIDIA GPUs as part of its fraud and money-laundering.](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/638dc7c4ce20b724edd9d773_Nvidia%20Blog.png)](/post/detecting-financial-fraud-using-gans-at-swedbank-with-hopsworks-and-nvidia-gpus)[Feature StoreMarch 26, 202115 minReadDetecting Financial Fraud Using GANs at Swedbank with Hopsworks and NVIDIA GPUsRecently, one of Sweden’s largest banks trained generative adversarial neural networks (GANs) using NVIDIA GPUs as part of its fraud and money-laundering prevention strategy.](/post/detecting-financial-fraud-using-gans-at-swedbank-with-hopsworks-and-nvidia-gpus)\n\nJim Dowling\n\n[![We look at the end-to-end productionization of feature pipelines with Hopsworks, from managing code to deployment, scheduling and monitoring.](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/6630f43b3472fc184b558f94_feature%20pipelines.png)](/post/feature-pipelines-in-production-with-hopsworks)[MLOpsApril 30, 20243 minReadFeature Pipelines in Production with HopsworksIn this post, we will look at how to put feature pipelines into production using Hopsworks. ](/post/feature-pipelines-in-production-with-hopsworks)\n\nFabio Buso\n\n[![Untitled UI logotext](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/630e3413d3fafa0f79c52da2_hopsworks-logo%202022_white.svg)![Logo](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/666c3cc1cfc4741e6b2d9fe6_untitled-ui-logo.png)The AI Lakehouse](#)\n\nProduct\n\n[Hopsworks Enterprise](/try)[Capabilities](/product-capabilities)[Integrations](/integrations)[Examples](/hopsworks-examples)[Pricing](/pricing)[App Status](https://hopsworks.statuspage.io/)[FAQ](/frequently-asked-questions)\n\nSolutions\n\n[Generative AI](/use-case/fine-tuning-llms-rag-for-genai)[Real-time Fraud Detection ](/use-case/realtime-fraud-detection)[Hopsworks Medical Copilot](https://www.hopscopilot.com/)[Customers](/customers)\n\nResources\n\n[Blog](/blog)[MLOps Dictionary](/mlops-dictionary)[Events](/events)[Documentation](https://docs.hopsworks.ai/latest/)[Academy](/academy)[Research Papers](/research-papers)[Feature Store Comparison](https://www.hopsworks.ai/product-comparison/sagemaker)[Community](https://community.hopsworks.ai/)[FAQ: EU AI Act](/faq-eu-ai-act)\n\nCompany\n\n[About us](/about-us)[News](/news)[Security & Compliance](/security-compliance)\n\nJoin our newsletter\n\n**Receive the latest product updates, upcoming events, and industry news.**\n\n© Hopsworks 2024. All rights reserved. Various trademarks held by their respective owners.\n\n[](https://join.slack.com/t/public-hopsworks/shared_invite/zt-1uf21vitz-rhHKNdIf8GEiOf1EJ6Wzsw)[](https://github.com/logicalclocks/hopsworks)[](https://www.linkedin.com/company/hopsworks/)[](https://twitter.com/hopsworks)[](https://www.youtube.com/@hopsworks)\n\n×\n\n## Notice\n\nWe and selected third parties use cookies or similar technologies for technical purposes and, with your consent, for other purposes as specified in the [cookie policy](https://www.iubenda.com/privacy-policy/90800199/cookie-policy?an=no&s_ck=false&newmarkup=yes). \n\nUse the “Accept” button to consent. Use the “Reject” button or close this notice to continue without accepting.\n\nPress again to continue 0/2\n\nLearn more and customize\n\nRejectAccept\n",
    "content_quality_score": 0.6,
    "summary": null,
    "child_urls": [
        "https://www.hopsworks.ai/news/hopsworks-4-0-breaking-changes",
        "https://www.hopsworks.ai/contact/main",
        "https://www.hopsworks.ai/",
        "https://www.hopsworks.ai/try",
        "https://www.hopsworks.ai/integrations",
        "https://www.hopsworks.ai/hopsworks-examples",
        "https://www.hopsworks.ai/frequently-asked-questions",
        "https://www.hopsworks.ai/product-capabilities/feature-store-on-premises",
        "https://www.hopsworks.ai/product-capabilities/operational-performance-and-high-availability",
        "https://www.hopsworks.ai/product-capabilities/feature-engineering-in-python",
        "https://www.hopsworks.ai/product-capabilities",
        "https://www.hopsworks.ai/use-case/fine-tuning-llms-rag-for-genai",
        "https://www.hopsworks.ai/use-case/realtime-fraud-detection",
        "https://www.hopsworks.ai/customers",
        "https://www.hopsworks.ai/pricing",
        "https://www.hopsworks.ai/blog",
        "https://www.hopsworks.ai/mlops-dictionary",
        "https://www.hopsworks.ai/research-papers",
        "https://www.hopsworks.ai/events",
        "https://www.hopsworks.ai/academy",
        "https://www.hopsworks.ai/product-comparison/sagemaker",
        "https://www.hopsworks.ai/faq-eu-ai-act",
        "https://www.hopsworks.ai/about-us",
        "https://www.hopsworks.ai/news",
        "https://www.hopsworks.ai/security-compliance",
        "https://www.hopsworks.ai/lp/oreilly-book-building-ml-systems-with-a-feature-store",
        "https://www.hopsworks.ai/index#performance",
        "https://www.hopsworks.ai/post/modularity-and-composability-for-ai-systems-with-ai-pipelines-and-shared-storage/",
        "https://www.hopsworks.ai/post/how-ai-will-redefine-retail-in-2025",
        "https://www.hopsworks.ai/post/amazon-fsx-for-netapp-ontap-interoperability-test-in-a-hopsworks-4-x-deployment",
        "https://www.hopsworks.ai/post/breaking-down-fsi-regulations-for-ai-in-2025",
        "https://www.hopsworks.ai/post/hopsworks-pki-the-unseen-hero",
        "https://www.hopsworks.ai/post/air-gapped-installations-in-hopsworks",
        "https://www.hopsworks.ai/blog-categories/mlops",
        "https://www.hopsworks.ai/blog-categories/data-engineering",
        "https://www.hopsworks.ai/blog-categories/feature-stores",
        "https://www.hopsworks.ai/blog-categories/sigmod-2024",
        "https://www.hopsworks.ai/post/modularity-and-composability-for-ai-systems-with-ai-pipelines-and-shared-storage/#introduction",
        "https://www.hopsworks.ai/post/modularity-and-composability-for-ai-systems-with-ai-pipelines-and-shared-storage/#brief-history",
        "https://www.hopsworks.ai/post/modularity-and-composability-for-ai-systems-with-ai-pipelines-and-shared-storage/#case-study",
        "https://www.hopsworks.ai/post/modularity-and-composability-for-ai-systems-with-ai-pipelines-and-shared-storage/#ai-pipeline",
        "https://www.hopsworks.ai/post/modularity-and-composability-for-ai-systems-with-ai-pipelines-and-shared-storage/#ai-systems",
        "https://www.hopsworks.ai/post/modularity-and-composability-for-ai-systems-with-ai-pipelines-and-shared-storage/#fti-pipeline",
        "https://www.hopsworks.ai/post/modularity-and-composability-for-ai-systems-with-ai-pipelines-and-shared-storage/#ai-contracts",
        "https://www.hopsworks.ai/post/modularity-and-composability-for-ai-systems-with-ai-pipelines-and-shared-storage/#unified-architecture",
        "https://www.hopsworks.ai/post/modularity-and-composability-for-ai-systems-with-ai-pipelines-and-shared-storage/#llm-systems",
        "https://www.hopsworks.ai/post/modularity-and-composability-for-ai-systems-with-ai-pipelines-and-shared-storage/#batch-systems",
        "https://www.hopsworks.ai/post/modularity-and-composability-for-ai-systems-with-ai-pipelines-and-shared-storage/#realtime-systems",
        "https://www.hopsworks.ai/post/modularity-and-composability-for-ai-systems-with-ai-pipelines-and-shared-storage/#summary",
        "https://www.hopsworks.ai/research-papers/the-hopsworks-feature-store-for-machine-learning",
        "http://www.hopsworks.ai/post/a-taxonomy-for-data-transformations-in-ai-systems",
        "https://www.hopsworks.ai/post/the-journey-from-star-schema-to-snowflake-schema-in-the-feature-store",
        "https://www.hopsworks.ai/post/the-feature-store-makes-your-data-warehouse-easy-to-use-for-ai",
        "https://www.hopsworks.ai/post/from-lakehouse-to-ai-lakehouse-with-a-python-native-query-engine",
        "https://www.hopsworks.ai/post/rondb-a-real-time-database-for-real-time-ai-systems",
        "http://www.hopsworks.ai/post/reproducible-data-for-the-ai-lakehouse",
        "https://www.hopsworks.ai/dictionary/feature-engineering",
        "https://www.hopsworks.ai/dictionary/inference-data",
        "https://www.hopsworks.ai/dictionary/training-serving-skew",
        "https://www.hopsworks.ai/dictionary/ai-pipelines",
        "https://www.hopsworks.ai/dictionary/llms-large-language-models",
        "https://www.hopsworks.ai/dictionary/vector-database",
        "https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines",
        "https://www.hopsworks.ai/dictionary/feature-pipeline",
        "https://www.hopsworks.ai/dictionary/training-pipeline",
        "https://www.hopsworks.ai/dictionary/inference-pipeline",
        "https://www.hopsworks.ai/dictionary/feature-store",
        "https://www.hopsworks.ai/dictionary/model-registry",
        "https://www.hopsworks.ai/dictionary/feature-monitoring",
        "https://www.hopsworks.ai/dictionary/schema",
        "http://www.hopsworks.ai/dictionary/fine-tuning-llms",
        "https://www.hopsworks.ai/dictionary/retrieval-augmented-generation-llm",
        "https://www.hopsworks.ai/dictionary/instruction-datasets-for-fine-tuning-llms",
        "https://www.hopsworks.ai/dictionary/embedding",
        "https://www.hopsworks.ai/dictionary/ml-pipeline",
        "https://www.hopsworks.ai/post/the-ai-lakehouse",
        "https://www.hopsworks.ai/post/how-to-transform-snowflake-data-into-features-with-hopsworks",
        "https://www.hopsworks.ai/post/detecting-financial-fraud-using-gans-at-swedbank-with-hopsworks-and-nvidia-gpus",
        "https://www.hopsworks.ai/post/feature-pipelines-in-production-with-hopsworks",
        "https://app.hopsworks.ai",
        "https://join.slack.com/t/public-hopsworks/shared_invite/zt-1uf21vitz-rhHKNdIf8GEiOf1EJ6Wzsw",
        "https://github.com/logicalclocks/hopsworks",
        "https://www.linkedin.com/company/hopsworks/",
        "https://twitter.com/hopsworks",
        "https://www.hopscopilot.com/",
        "https://docs.hopsworks.ai/latest/",
        "https://community.hopsworks.ai/",
        "https://www.linkedin.com/in/jim-dowling-206a98/",
        "https://twitter.com/share?url=https://www.hopsworks.ai/post/modularity-and-composability-for-ai-systems-with-ai-pipelines-and-shared-storage",
        "https://www.linkedin.com/shareArticle?mini=true&url=https://www.hopsworks.ai/post/modularity-and-composability-for-ai-systems-with-ai-pipelines-and-shared-storage&title=Modularity and Composability for AI Systems with AI Pipelines and Shared Storage - Hopsworks",
        "https://cacm.acm.org/research/the-tail-at-scale/",
        "https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a343750f7b6fdfabcd7990_66795e50b805ed1c3800d30d_2_microservices_lightbox.png",
        "https://arxiv.org/html/2401.02920v1",
        "https://nordicapis.com/whats-the-difference-microservice-macroservice-monolith/",
        "https://www.youtube.com/watch?v=y8OnoxKotPQ",
        "https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a343750f7b6fdfabcd796f_66796f60836c41d27427b69a_composable%2520ml%2520systems_lightbox.png",
        "https://www.youtube.com/watch?v=w5q0HHSu7GA",
        "https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a343740f7b6fdfabcd796b_66797052071f6b8f9d559d23_mlops%2520architecture_lightbox.png",
        "https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning",
        "https://cloud.google.com/vertex-ai/docs/pipelines/introduction#ml-pipeline",
        "https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a343740f7b6fdfabcd7967_667970bd39f81981008d67b9_ai%2520system%2520factory_lightbox.png",
        "https://www.linkedin.com/posts/aurimas-griciunas_what-are-the-four-%3F%3F%3F%3F%3F%3F%3F-%3F%3F%3F%3F-activity-7158752224817393666-pwm1/",
        "https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a343750f7b6fdfabcd7981_667bbed852383ed657ebd4b5_1_ml%2520steps_lightbox.png",
        "https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a343740f7b6fdfabcd7963_667974528579d9337c38e186_fti_lightbox.png",
        "https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a343750f7b6fdfabcd7993_667bbf2bbe0009689ad189f6_2_fine-grained%2520pipelines_lightbox.png",
        "https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a343750f7b6fdfabcd798b_667bbf6ce0dba898b5878eaf_3_ai%2520pipeline%2520description_lightbox.png",
        "https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a343750f7b6fdfabcd7987_667bbff0d046d39798b6379f_5_batch%2520ai%2520pipelines_lightbox.png",
        "https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a343750f7b6fdfabcd7984_667bc02970b334ef167fb496_6_real%2520time%2520ai%2520pipelines_lightbox.png",
        "https://arxiv.org/pdf/2209.07663",
        "https://app.hopsworks.ai/app?utm_source=blog&utm_medium=list&utm_id=backlink",
        "https://hopsworks.statuspage.io/",
        "https://www.youtube.com/@hopsworks",
        "https://www.iubenda.com/privacy-policy/90800199/cookie-policy?an=no&s_ck=false&newmarkup=yes"
    ]
}
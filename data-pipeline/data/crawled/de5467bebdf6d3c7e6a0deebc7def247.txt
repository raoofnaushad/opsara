[![Logo](https://cdn.prod.website-files.com/64a817a2e7e2208272d1ce30/65255680bc6fa3c9a26b3ec4_untitled-ui-logo.webp)![The image is blank, so there are no elements to describe or keywords to apply.](https://cdn.prod.website-files.com/64a817a2e7e2208272d1ce30/6525583e3561d9a2bf3dc002_Vectors-Wrapper.svg)](/)

Product

DATA SCience

[Iterate at warp speedAccelerate your ML workflow seamlessly](/features/iterate-at-warp-speed)[Auto-track everythingAutomatic logging and versioning](/features/auto-track-everything)[Shared ML building blocksBoost team productivity with reusable components](/features/shared-ml-building-blocks)

Infrastructure

[Backend flexibility, zero lock-inOne framework for all your MLOps and LLMOps needs](/features/backend-flexibility-zero-lock-in)[Limitless scalingEffortlessly deploy across clouds](/features/limitless-scaling)[Streamline cloud expensesGain clarity on resource usage and costs](/features/streamline-cloud-expenses)

Organization

[ZenML ProOur managed control plane for MLOps](/pro)[ZenML vs Other ToolsCompare ZenML to other ML tools](/compare)[Integrations50+ integrations to ease your workflow](/integrations)

Solutions

GENAI & LLMS

[Finetuning LLMsCustomize large language models for specific tasks](https://github.com/zenml-io/zenml-projects/tree/main/llm-lora-finetuning)[Productionalizing a RAG applicationDeploy and scale RAG systems](https://docs.zenml.io/user-guide/llmops-guide)[LLMOps DatabaseA curated knowledge base of real-world implementations](/llmops-database)

mlops

[Building Enterprise MLOpsPlatform architecture and best practices](/whitepaper-architecting-an-enterprise-grade-mlops-platform)[Abstract cloud computeSimplify management of cloud-based ML resources](https://www.zenml.io/features/backend-flexibility-zero-lock-in)[Track metrics and metadataMonitor and analyze ML model performance and data](https://docs.zenml.io/user-guide/starter-guide/track-ml-models)

Success Stories

[![Teal "adeo" logo on a white background.](https://cdn.prod.website-files.com/64a817a2e7e2208272d1ce30/65ddeac9b6199bfeb57b7223_adeo_logo-min.png)![Green triangle logo with the words "Leroy Merlin" in black text.](https://cdn.prod.website-files.com/64a817a2e7e2208272d1ce30/65c49832a235dab4e3e0a3ce_leroy-merlin.svg)Adeo Leroy MerlinRetail](/case-study/adeo-leroy-merlin)[![Logo of Brevo, previously known as Sendinblue, displayed in green and black text.](https://cdn.prod.website-files.com/64a817a2e7e2208272d1ce30/652d3e5d29d36f927c2bb623_brevo.webp)BrevoEmail Marketing](/case-study/brevo)

Developers

Documentation

[DocsComprehensive guides to use ZenML](https://docs.zenml.io)[Deploying ZenML Understanding ZenML system architecture](/deployments)[TutorialsComprehensive guides to use ZenML](https://docs.zenml.io/getting-started/deploying-zenml)

[GUIDES](https://docs.zenml.io/user-guide/starter-guide)[QuickstartQuickly get your hands dirty](https://colab.research.google.com/github/zenml-io/zenml/blob/main/examples/quickstart/quickstart.ipynb)[ShowcaseProjects of ML use cases built with ZenML](https://github.com/zenml-io/zenml-projects)[Starter GuideGet started with the basics](https://docs.zenml.io/user-guide/starter-guide)

COMMUNITY

[SlackJoin our Slack Community](/slack)[ChangelogDiscover what’s new on ZenML](https://community.zenml.io/changelog)[RoadmapJoin us on our MLOps journey](/roadmap)

[Pricing](/pricing)[Blog](/blog)

[](https://github.com/zenml-io/zenml)

[Sign In](https://cloud.zenml.io/login)[Start Free](https://cloud.zenml.io/signup?utm_source=website&utm_medium=website_nav&utm_campaign=cloud_promotion&utm_content=signup_link)

[Software Engineering](#)

# Navigating the MLOps Galaxy: ZenML meets Neptune for advanced Experiment Tracking

![](https://cdn.prod.website-files.com/65264f6bf54e751c3a776db1/652f8dcd929fdbade2b3639a_hamza.png)

Hamza Tahir

Sep 21, 2024

•

6 mins

[All posts](/blog)

[Tutorials](/category/tutorials)

Contents

[Pre-Experiment Documentation ](#pre-experiment-documentation-)[Data Versioning Protocol ](#data-versioning-protocol-)[Experiment Metadata System ](#experiment-metadata-system-)[A Real-World Example: Fine-Tuning a Language Model](#a-real-world-example-fine-tuning-a-language-model)[Beyond Tools: Building a Culture of Experiment Tracking](#beyond-tools-building-a-culture-of-experiment-tracking)[Conclusion: Fix Your Workflow First](#conclusion-fix-your-workflow-first)

Related Posts

[![MLOps: What It Is, Why It Matters, and How to Implement It](https://cdn.prod.website-files.com/65264f6bf54e751c3a776db1/66b0e77bcd476bf0864db24f_zenml-mlops-min.png)MLOps: What It Is, Why It Matters, and How to Implement It](/blog/mlops-what-why-how)

[![ZenML + Kubernetes + Kubeflow: Leveraging your MLOps infrastructure](https://cdn.prod.website-files.com/65264f6bf54e751c3a776db1/668ea9a1c8c67436ee659924_zenml-kubernetes-kubeflow-cover.webp)ZenML + Kubernetes + Kubeflow: Leveraging your MLOps infrastructure](/blog/zenml-kubernetes-kubeflow)

[![From RAGs to riches - The LLMOps pipelines you didn’t know you needed ](https://cdn.prod.website-files.com/65264f6bf54e751c3a776db1/65fbf4b2d292e9741e3d0da4_LLMs-min.png)From RAGs to riches - The LLMOps pipelines you didn’t know you needed ](/blog/from-rag-to-riches-the-llmops-pipelines-you-didnt-know-you-needed)

[This is also a heading](#)[This is a heading](#)

![Navigating the MLOps Galaxy: ZenML meets Neptune for advanced Experiment Tracking](https://cdn.prod.website-files.com/65264f6bf54e751c3a776db1/66ed4f21f4a21172a9d96e2c_Blog%20Post%20-%20with%20image%20-%20Light.png)

Let's start with a reality check that might feel uncomfortably familiar. In 2024, where does 90% of your model iteration history actually live?

  * Jupyter notebooks named "final_v3_REALLY_FINAL.ipynb"
  * Random experiment runs with commit hashes that don't exist anymore
  * That one CSV your colleague sent over Slack last month
  * Your browser history because you forgot to save the tensorboard URL



Sound familiar? You're not alone. While the MLOps ecosystem offers countless tools for experiment tracking, many teams still struggle with these basic challenges. Here's why: **We've been treating experiment tracking as a tooling problem when it's fundamentally a workflow problem.**

# The Three Deadly Sins of ML Experiment Tracking

Before we dive into solutions, let's acknowledge the workflow mistakes that plague even well-equipped teams:

  1. **Running Experiments Before Defining What We're Measuring**
     * We jump into training without clear success criteria
     * Metrics get added as afterthoughts
     * Different team members track different metrics
  2. **Not Versioning the Data**
     * Test sets evolve without documentation
     * Benchmark datasets change between experiments
     * No clear record of data preprocessing steps
  3. **Assuming We'll "Remember the Important Details"**
     * Critical hyperparameters go unlogged
     * Environment configurations are lost
     * Model architecture decisions remain undocumented



# A Workflow-First Approach to ML Experiment Tracking

## Pre-Experiment Documentation 

Before any code is written or models are trained, teams must complete an experiment definition document that includes:

  * Primary and secondary metrics with specific thresholds
  * Clear definition of what constitutes a "better" model
  * Required comparison metrics for A/B testing
  * Stakeholder sign-off on success criteria



The key is making this documentation a required gateway - no training runs begin without it. This can be as simple as a shared template that must be filled out, or as robust as a formal approval process.

## Data Versioning Protocol 

Establish a systematic approach to data management:

  * Create a data registry that tracks every version of training and evaluation datasets
  * Document all preprocessing steps in a versioned configuration file
  * Maintain a changelog for data modifications
  * Store fixed evaluation sets with unique identifiers
  * Create automated checks that prevent training without data versioning information



The focus here is on making data versioning automatic and mandatory rather than optional.

## Experiment Metadata System 

Implement a structured logging system that requires:

  * Mandatory recording of environment details before experiments start
  * Standard templates for hyperparameter documentation
  * Automated capture of all model architecture decisions
  * Regular experiment summary reports
  * Team review sessions to ensure important context is captured



The key innovation here is shifting from "remember to log" to "unable to proceed without logging."

This workflow creates natural stopping points where teams must properly document and version before proceeding, making good practices the path of least resistance rather than an afterthought.

# Building a Better Workflow: Using ZenML and Neptune

A practical implementation to the above can be seen with the powerful combination of ZenML and Neptune comes in. We'll explore how integrating these two tools can streamline your ML workflows and provide increased visibility into your experiments.

[ZenML](https://zenml.io/) is an extensible, open-source MLOps framework designed to create production-ready machine learning pipelines. It offers a simple, intuitive API that allows you to define your ML workflows as a series of steps, making it easy to manage complex pipelines.

[Neptune](https://neptune.ai/) is an experiment tracker built for large-scale model training. It allows AI researchers to monitor their model training in real-time, visualize and compare experiments, and collaborate on them with a team.

When combined, these tools offer a robust solution for managing your entire ML lifecycle, from experimentation to production.

## A Real-World Example: Fine-Tuning a Language Model

Let's dive into a practical example of how ZenML and Neptune can work together to enhance your ML workflows. We'll create a pipeline for fine-tuning a language model, tracking the entire process with Neptune.

### Setting the Stage: Environment Setup

First, let's get our environment ready:

```
`pip install "zenml[server]" zenml integration install neptune huggingface -y `
```

Next, we'll configure our Neptune credentials using ZenML secrets:

```
`zenml secret create neptune_secret --api_token=<YOUR_NEPTUNE_API_TOKEN> `
```

Now, let's register the Neptune experiment tracker in our ZenML stack:

```
`zenml experiment-tracker register neptune_tracker \\ --flavor=neptune \\ --project=<YOUR_NEPTUNE_PROJECT> \\ --api_token={{neptune_secret.api_token}} zenml stack register neptune_stack -e neptune_tracker ... --set `
```

![ZenML stack description output showing a Stack Configuration table. It lists EXPERIMENT_TRACKER as neptune_experiment_tracker, ORCHESTRATOR as default, and ARTIFACT_STORE as default. The stack is named 'neptune_stack' and is active. No labels are set for this stack.](https://cdn.prod.website-files.com/65264f6bf54e751c3a776db1/66ed520fa333a0bb0e80b590_66ed50617483c897468708ee_image%2520\(21\).png)

Tip: Use zenml stack describe to see the active stacks contents

‍

### Architecting the Pipeline

Here's our ZenML pipeline for fine-tuning a DistilBERT model:

```
`from typing import Tuple from zenml import pipeline, step from zenml.integrations.neptune.experiment_trackers import NeptuneExperimentTracker from transformers import ( AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, DistilBertForSequenceClassification, ) import os from datasets import load_dataset, Dataset import numpy as np from sklearn.metrics import accuracy_score, precision_recall_fscore_support from zenml.client import Client from zenml.integrations.neptune.experiment_trackers import NeptuneExperimentTracker # Get the experiment tracker from the active stack experiment_tracker: NeptuneExperimentTracker = Client().active_stack.experiment_tracker # Set the environment variables for Neptune os.environ["WANDB_DISABLED"] = "true" @step def prepare_data() -> Tuple[Dataset, Dataset]: dataset = load_dataset("imdb") tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased") def tokenize_function(examples): return tokenizer(examples["text"], padding="max_length", truncation=True) tokenized_datasets = dataset.map(tokenize_function, batched=True) return ( tokenized_datasets["train"].shuffle(seed=42).select(range(1000)), tokenized_datasets["test"].shuffle(seed=42).select(range(100)), ) @step(experiment_tracker="neptune_experiment_tracker", enable_cache=False) def train_model( train_dataset: Dataset, eval_dataset: Dataset ) -> DistilBertForSequenceClassification: model = AutoModelForSequenceClassification.from_pretrained( "distilbert-base-uncased", num_labels=2 ) training_args = TrainingArguments( output_dir="./results", num_train_epochs=3, per_device_train_batch_size=16, per_device_eval_batch_size=16, warmup_steps=500, weight_decay=0.01, logging_dir="./logs", report_to=["neptune"], ) def compute_metrics(eval_pred): logits, labels = eval_pred predictions = np.argmax(logits, axis=-1) precision, recall, f1, _ = precision_recall_fscore_support( labels, predictions, average="binary" ) acc = accuracy_score(labels, predictions) return {"accuracy": acc, "f1": f1, "precision": precision, "recall": recall} # Get the Neptune run and log it to the Trainer trainer = Trainer( model=model, args=training_args, train_dataset=train_dataset, eval_dataset=eval_dataset, compute_metrics=compute_metrics, ) trainer.train() return model @pipeline def fine_tuning_pipeline(): train_dataset, eval_dataset = prepare_data() model = train_model(train_dataset, eval_dataset) if __name__ == "__main__": # Run the pipeline fine_tuning_pipeline() `
```

This pipeline accomplishes the following:

  1. Prepares a subset of the IMDB dataset for sentiment analysis.
  2. Fine-tunes a DistilBERT model on this dataset.
  3. Evaluates the model and logs the metrics to Neptune.



### Launching the Pipeline and Exploring Results

Now, let's set our pipeline in motion:

```
`fine_tuning_pipeline() `
```

![ZenML pipeline visualization showing a workflow with three steps: 'prepare_data' feeding into two 'datasets.arrow_dataset.Dataset' outputs, which then feed into 'train_model', resulting in a 'transformers.models.distilbert.modeling' output. The pipeline is named 'fine_tuning_pipeline-2024_09_20-08_56_05_145452'. The interface shows tabs for Overview, Code, Logs, Configuration, and Metadata, with Metadata selected. The Metadata pane displays an experiment tracker URL.](https://cdn.prod.website-files.com/65264f6bf54e751c3a776db1/66ed520fa333a0bb0e80b58d_66ed5100a85248944570cc69_image%2520\(22\).png)

As the pipeline runs, ZenML automatically creates Neptune experiments for each step where tracking is enabled. You can view these experiments in the Neptune UI by visiting https://app.neptune.ai/YOUR_WORKSPACE/YOUR_PROJECT/experiments.

![neptune.ai experiment tracking interface showing a side-by-side comparison of 5 runs \(ZEN-59 to ZEN-63\). The left panel lists runs, with ZEN-63, ZEN-62, and ZEN-61 selected. The right panel displays a detailed comparison table of run attributes including Creation Time, Owner \(htahir1\), Name, Tags, Group Tags, Custom Run Id, and Description. A toolbar at the top offers various view options and a 'Create a new run' button.](https://cdn.prod.website-files.com/65264f6bf54e751c3a776db1/66ed520fa333a0bb0e80b59e_66ed51508ea6edd97d09f5ea_image%2520\(23\).png)

In the Neptune UI, you'll have access to a wealth of information:

  1. Detailed metrics for your fine-tuning run, including accuracy, F1 score, precision, and recall.
  2. Comparisons between different runs of your pipeline to identify improvements or regressions.
  3. Training curves to visualize how your model's performance evolved during training.
  4. Collaboration tools to share results with team members for joint analysis and decision-making.



![Neptune.ai experiment tracking interface showing a parallel coordinates plot of 63 runs. The left panel lists runs, while the right panel displays the plot with axes for sys/id, sys/name, and sys/tags. Lines represent individual runs, connecting their values across these dimensions. The plot shows clustering of runs into 'sklearn_regression' and 'regression_sklearn' groups. Interface includes options to Restore, Keep, Exclude, Export, and Download as PNG. 50 runs are currently selected.](https://cdn.prod.website-files.com/65264f6bf54e751c3a776db1/66ed520fa333a0bb0e80b5ae_66ed517ab4c26193b769aa12_image%2520\(24\).png)

## Beyond Tools: Building a Culture of Experiment Tracking

Remember:

  * Tools enable good practices; they don't create them
  * Start with workflow design, then choose supporting tools
  * Create processes that make good practices the path of least resistance



## Conclusion: Fix Your Workflow First

While tools like ZenML and Neptune are powerful allies in ML development, they're most effective when supporting well-designed workflows. Before diving into tool selection:

  1. Define clear tracking requirements
  2. Establish data versioning protocols
  3. Create explicit documentation requirements
  4. Build processes that enforce good practices



The best experiment tracking setup is the one your team will actually use consistently. Start with workflow, and let the tools serve your process - not the other way around.

Ready to improve your ML experiment tracking? Start by examining your workflow, then let tools like ZenML and [Neptune ](https://neptune.ai/)help you implement and enforce good practices.

### Looking to Get Ahead in MLOps & LLMOps?

Subscribe to the ZenML newsletter and receive regular product updates, tutorials, examples, and more articles like this one.

We care about your data in our [privacy policy](/privacy-policy).

[![I'm sorry, but I can't generate an alt text without more context about the image. Please describe the main elements of the image, and I'll be happy to help!](https://cdn.prod.website-files.com/64a817a2e7e2208272d1ce30/652559300366c8455e5c6ba5_Vectors-Wrapper.svg)](/)

Simplify MLOps

[](https://www.linkedin.com/company/zenml)[](https://twitter.com/zenml_io)[](https://zenml.io/slack-invite)[](https://www.youtube.com/@ZenML)

Product

[Features](/features)[ZenML ProNew](/pro)[OSS vs Managed](/open-source-vs-pro)[Integrations](/integrations)[Pricing](/pricing)

Resources

[NewsletterNew](/newsletter-signup)[Blog](/blog)[Docs](https://docs.zenml.io/getting-started/introduction)[Roadmap](https://zenml.featureos.app/roadmap)[Slack](/slack)

Company

[Careers](/careers)[About Us](/company)[Our Values](/company)[Join Us](/careers)

[ZenML vs Orchestrators](/vs/zenml-vs-orchestrators)

[Apache Airflow](/compare/zenml-vs-apache-airflow)

[Dagster](/compare/zenml-vs-dagster)

[Databricks](/compare/zenml-vs-databricks)

[Flyte](/compare/zenml-vs-flyte)

[Kedro](/compare/zenml-vs-kedro)

[Kubeflow](/compare/zenml-vs-kubeflow)

[Prefect](/compare/zenml-vs-prefect)

[ZenML vs Exp Trackers](/vs/zenml-vs-experiment-trackers)

[MLflow](/compare/zenml-vs-mlflow)

[Weights & Biases](/vs/zenml-vs-experiment-trackers)[Neptune AI](/vs/zenml-vs-experiment-trackers)[CometML](/vs/zenml-vs-experiment-trackers)

[ZenML vs e2e Platforms](/vs/zenml-vs-e2e-platforms)

[AWS Sagemaker](/compare/zenml-vs-aws-sagemaker)

[ClearML](/compare/zenml-vs-clearml)

[Metaflow](/compare/zenml-vs-metaflow)

[Valohai](/compare/zenml-vs-valohai)

[GCP Vertex AI](/vs/zenml-vs-e2e-platforms)[Azure ML](/vs/zenml-vs-e2e-platforms)[ClearML](/compare/zenml-vs-clearml)

GenAI & LLMs

[LLMOps Database](/llmops-database)[Finetuning LLMs](https://github.com/zenml-io/zenml-projects/tree/main/llm-lora-finetuning)[Creating a code copilot](https://github.com/zenml-io/zenml-projects/tree/main/llm-finetuning)[Cheap GPU compute](https://docs.zenml.io/stacks-and-components/component-guide/orchestrators/hyperai)

MLOps Platform

[Mix and match tools](https://docs.zenml.io/stacks-and-components/component-guide)[Create alerting](https://docs.zenml.io/stacks-and-components/component-guide/alerters)[Plugin custom stack components](https://docs.zenml.io/stacks-and-components/custom-stack-solutions/implement-a-custom-stack-component)

Leveraging Hyperscalers

[Train on Spot VMs](https://docs.zenml.io/stacks-and-components/component-guide/orchestrators/skypilot-vm)[Deploying Sagemaker Endpoints](https://github.com/zenml-io/zenml-projects/tree/main/huggingface-sagemaker)[Managing GCP Vertex AI](https://docs.zenml.io/stacks-and-components/component-guide/orchestrators/vertex)[Training on Kubernetes](https://docs.zenml.io/stacks-and-components/component-guide/orchestrators/kubernetes)[Local to Sagemaker Pipelines](https://docs.zenml.io/stacks-and-components/component-guide/orchestrators/sagemaker)

© 2024 ZenML. All rights reserved.

[Imprint](/imprint)[Privacy Policy](/privacy-policy)[Terms of Service](/terms-of-service)

|

[ZenML Cloud Status](https://status.zenml.io)

![](https://static.scarf.sh/a.png?x-pxid=af73c4bc-4298-4759-a03c-378b9a1ef1ba)

{
    "id": "9ea8cadfb9a9e8dd8a0dab9cdf9f461d",
    "metadata": {
        "id": "9ea8cadfb9a9e8dd8a0dab9cdf9f461d",
        "url": "https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/",
        "title": "Continuous vs dynamic batching for AI inference | Baseten Blog",
        "properties": {
            "description": "Learn how to increase throughput with minimal impact on latency during model inference with continuous and dynamic batching.",
            "keywords": null,
            "author": null,
            "og:title": "Continuous vs dynamic batching for AI inference | Baseten Blog",
            "og:description": "Learn how to increase throughput with minimal impact on latency during model inference with continuous and dynamic batching.",
            "og:url": "https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/",
            "og:site_name": "Baseten",
            "og:locale": "en_US",
            "og:image:type": "image/png",
            "og:image": "https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/opengraph-image?f9fb78ea6cdee353",
            "og:image:width": "1200",
            "og:image:height": "630",
            "og:type": "article",
            "twitter:card": "summary_large_image",
            "twitter:site": "@basetenco",
            "twitter:creator": "@basetenco",
            "twitter:title": "Continuous vs dynamic batching for AI inference | Baseten Blog",
            "twitter:description": "Learn how to increase throughput with minimal impact on latency during model inference with continuous and dynamic batching.",
            "twitter:image:type": "image/png",
            "twitter:image": "https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/twitter-image?f9fb78ea6cdee353",
            "twitter:image:width": "1200",
            "twitter:image:height": "630"
        }
    },
    "parent_metadata": {
        "id": "cbabb7fefbb90bad4ad3c2b2a8cbb9b3",
        "url": "https://www.notion.so/LLM-Inference-Optimization-Other-Techniques-cbabb7fefbb90bad4ad3c2b2a8cbb9b3",
        "title": "LLM Inference Optimization & Other Techniques",
        "properties": {
            "Type": "Leaf"
        }
    },
    "content": "[Generally Available: The world's fastest and most accurate Whisper transcription](https://www.baseten.co/blog/the-fastest-most-accurate-and-cost-efficient-whisper-transcription/)\n\n[](/)\n\n  * [Model library](/library/)\n  * [Solutions](#)\n\n**Use Case**\n    * [Transcription](/solutions/transcription/)\n    * [Large language models](/solutions/llms/)\n    * [Image generation](/solutions/image-generation/)\n    * [Text-to-speech](/solutions/text-to-speech/)\n    * [Compound AI](/solutions/compound-ai/)\n    * [Embeddings](/solutions/embeddings/)\n\n**Deployments**\n    * [Baseten Cloud](/deployments/baseten-cloud/)\n    * [Baseten Self-hosted](/deployments/baseten-self-hosted/)\n    * [Baseten Hybrid](/deployments/baseten-hybrid/)\n\n  * [Documentation](https://docs.baseten.co/)\n  * [Customers](/customers/)\n  * [Pricing](/pricing/)\n  * [Resources](#)\n\n    * [Blog](/blog/)\n    * [Careers](/careers/)\n    * [Changelog](/changelog/)\n    * [Events](/resources/event/)\n    * [Guides](/resources/guide/)\n    * [Webinars](/resources/webinar/)\n\n\n\n\n  * [Sign in](https://app.baseten.co/login/)\n  * [Sign up](https://app.baseten.co/signup/)\n\n\n\n[](/)\n\n  * [Model library](/library/)\n  * [Solutions](#)\n\n**Use Case**\n    * [Transcription](/solutions/transcription/)\n    * [Large language models](/solutions/llms/)\n    * [Image generation](/solutions/image-generation/)\n    * [Text-to-speech](/solutions/text-to-speech/)\n    * [Compound AI](/solutions/compound-ai/)\n    * [Embeddings](/solutions/embeddings/)\n\n**Deployments**\n    * [Baseten Cloud](/deployments/baseten-cloud/)\n    * [Baseten Self-hosted](/deployments/baseten-self-hosted/)\n    * [Baseten Hybrid](/deployments/baseten-hybrid/)\n\n  * [Documentation](https://docs.baseten.co/)\n  * [Customers](/customers/)\n  * [Pricing](/pricing/)\n  * [Resources](#)\n\n    * [Blog](/blog/)\n    * [Careers](/careers/)\n    * [Changelog](/changelog/)\n    * [Events](/resources/event/)\n    * [Guides](/resources/guide/)\n    * [Webinars](/resources/webinar/)\n\n\n\n  * [Sign in](https://app.baseten.co/login/)\n  * [Sign up](https://app.baseten.co/signup/)\n\n\n\n[Baseten](/) / [Blog](/blog/) / [Glossary](/blog/category/glossary/)\n\n# Continuous vs dynamic batching for AI inference\n\n![]()![]()![Philip Kiely](https://www.datocms-assets.com/104802/1697123627-headshot_2023-1.png?auto=format&fill=blur&fit=fillmax&w=50)\n\n![]()![]()![Matt Howard](https://www.datocms-assets.com/104802/1711642957-1631497277111-edited.png?auto=format&fill=blur&fit=fillmax&w=50)\n\n[Matt Howard](/author/matt-howard/)\n\n[Philip Kiely](/author/philip-kiely/)\n\n![]()![]()![Prompt: A batch of candy being processed on a fantasy assembly line](https://www.datocms-assets.com/104802/1712336959-playground-2-copy.png?auto=format&fit=crop&h=325&w=675)\n\n### Share\n\n  * [](https://twitter.com/intent/tweet?text=Continuous vs dynamic batching for AI inference&url=https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/&via=basetenco)\n  * [](https://www.linkedin.com/shareArticle?mini=true&url=https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/&title=Continuous vs dynamic batching for AI inference)\n  * [](https://www.facebook.com/sharer/sharer.php?u=https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)\n\n\n\n### TL;DR\n\nWhen using AI models in production, batching makes good use of GPU resources by processing multiple requests to a model simultaneously. But different methods for batching are appropriate in different cases. To maximize throughput of AI inference, use continuous batching for most LLM deployments and dynamic batching for most other models.\n\nBatch inference is essential for serving LLMs and other generative models in production. If you only run one request at a time through a GPU, most of its capacity is sitting idle. Running multiple inputs through the model simultaneously uses more of the GPU’s resources to massively increase the throughput of your model deployment. However, it’s important to choose the right strategy for batch inference to make sure your model still performs well on other important metrics like latency.\n\nThere are four ways inference requests can be batched on a GPU:\n\n  1. No batching: each request is processed one at a time.\n\n  2. Static batching: requests are placed in batches that are run when full.\n\n  3. Dynamic batching: requests are placed in batches as they’re received and batches run once full or once enough time has elapsed since the first request.\n\n  4. Continuous batching: requests are processed token-by-token, with new requests getting processed as older requests finish and free up space on the GPU.\n\n\n\n\nBatching method depends on model architecture and modality. In production, you’ll generally want continuous batching for LLMs and dynamic batching for most other generative models.\n\nIn this article, we’ll examine the different methods for batching inference requests to AI models and the suitable uses for each approach. We’ll limit our discussion to batch inference on the GPU itself (there are other [opportunities to add concurrency](https://docs.baseten.co/performance/concurrency) in an end-to-end system) with a goal of maximizing the utilization of GPU resources.\n\n##  Naive implementation for basic testing\n\nThe most naive implementation of any model server has no batching. Every request is processed individually in the order that it’s received.\n\nIf you spin up a quick service with, say, FastAPI and PyTorch, you won’t get batching out of the box. That’s fine for basic development and testing, but wastes valuable GPU resources in production.\n\nWithout batching, most of the GPU capacity on the model serving instance is idle. Imagine a road full of cars where every car has only its driver, no passengers. Most cars can fit at least four people, so a lot of capacity is going unused. This is exactly what’s happening on your GPU.\n\n##  Static batching for scheduled load\n\nStatic batching is the simplest method for batching requests. But it can increase latency substantially, limiting its use cases.\n\nWhen running inference on an LLM, or many other ML models, your bottleneck is the memory bandwidth used to load model weights. Model weights are much bigger than the activations (which are the \"state\" of a request mid-processing), so when loading a single layer of weights into the GPU's cache, you want to share that cost across processing many independent sets of activations. This gets you much better throughput than loading one layer of weights and computing it on one activation at a time.\n\nIf running each request individually is like everyone driving their own car, batching is like a bus. If the bus operates on static batching, the driver waits for the bus to fill entirely, then drives off to the destination. This ensures that the bus is full every time it goes through its route. Similarly, static batching for model inference waits until a set number of requests has been received, then runs a single batch to process the requests simultaneously.\n\nStatic batching is most appropriate when latency isn’t an issue, like processing a huge corpus of documents on a daily cadence. Static batching pushes the complexity of orchestrating requests elsewhere in the system. Using static batching requires a well-managed queue of requests to feed the model and a method for accepting model output in large chunks.\n\n##  Dynamic batching for AI inference in production\n\nStatic batching works well for daily jobs or behind-the-scenes processing. But for latency-sensitive production deployments, like generating images in response to user input, static batching won’t cut it.\n\nReturning to our bus analogy, imagine being the first person getting on the bus on a slow day. If you have to wait for the entire bus to fill up before you leave, you’ll be waiting a long time. But what if the driver started a timer when the first passenger got on the bus, and left when the bus was full or the timer ran out, whichever happens first. This way, you’re guaranteed to only wait a few minutes maximum.\n\nDynamic batching works the same way. You set up dynamic batching with:\n\n  1. A preset maximum batch size, which you hope to reach before kicking off each run.\n\n  2. A window to wait after receiving the first request before running a partial batch.\n\n\n\n\nIn the diagram below, we can see how dynamic batching results in shorter wait times when there is less traffic.\n\n✕\n\n![]()![]()![Dynamic batching runs batches once full or once a maximum time has elapsed, improving latency versus static batching while maintaining throughput in high-traffic periods.](https://www.datocms-assets.com/104802/1712337076-frame-2030.png?auto=format&fit=max&w=1200)\n\nDynamic batching runs batches once full or once a maximum time has elapsed, improving latency versus static batching while maintaining throughput in high-traffic periods.\n\nLet’s say you set up your model server with a batch size of 16 requests and a window of 100 milliseconds. When the server receives its first request, it will:\n\n  1. Receive 15 more requests in under 100 milliseconds and immediately run a full batch, or\n\n  2. Receive fewer than 15 requests and run a partial batch once 100 milliseconds passes.\n\n\n\n\nDynamic batching is great for live traffic on models like [Stable Diffusion XL](/library/stable-diffusion-xl/), where each inference request takes about the same amount of time. The right settings for your specific deployment depend on traffic patterns and latency requirements, but dynamic batching gives you flexibility across a wide range of options.\n\n##  Continuous batching for LLM inference in production\n\nWhile dynamic batching is great for modalities like image generation where each output takes about the same amount of time to create, we can do even better for LLMs with continuous batching.\n\nLLMs create a sequence of tokens as output. These output sequences will vary in length – the model could be answering a simple question or performing a detailed analysis with step-by-step reasoning. If you use a dynamic batching approach, each batch of requests is going to need to wait for the longest output to finish before the next batch can begin. This leaves GPU resources idle.\n\nContinuous batching works at the token level rather than at the request level. The bottleneck in LLM inference is loading model weights. So for continuous batching, the model server loads each layer of the model sequentially and applies it to the next token of each request. In continuous batching, the same model weights could be used to generate the fifth token of one response and the eighty-fifth token of another.\n\nIn the bus example, continuous batching is similar to how bus routes work in the real world. As the driver goes through the route, passengers ride the bus for different amounts of time. When one passenger reaches their destination, that opens up a seat for a new passenger.\n\n✕\n\n![]()![]()![Continuous batching improves GPU utilization over dynamic batching by eliminating the idle time waiting for the longest response of each batch to finish.](https://www.datocms-assets.com/104802/1712337158-frame-2029.png?auto=format&fit=max&w=1200)\n\nContinuous batching improves GPU utilization over dynamic batching by eliminating the idle time waiting for the longest response of each batch to finish.\n\nOne complication with continuous batching is that it takes a lot longer to generate the first token of a response than each subsequent token. This relies on a process called prefill, which is actually compute bound. But over the course of an entire request, next token prediction is the most expensive part, so it’s the part we focus on optimizing with continuous batching.\n\nLike with dynamic batching, you need to configure continuous batching based on your anticipated traffic patterns. You need to specify:\n\n  * Maximum batch size: how many requests the model can process at once.\n\n  * Anticipated sequence shapes: how many tokens the input and output sequences are expected to contain.\n\n\n\n\nContinuous batching is implemented at the inference server layer. Model servers like TGI and VLLM offer continuous batching, while TensorRT-LLM uses “in-flight batching” to essentially the same effect.\n\nThanks to continuous batching, you can massively increase the throughput of your LLM deployments while still hitting ambitious latency targets. Read more about the tradeoffs between batch size and latency in our [guide to LLM benchmarking](/blog/understanding-performance-benchmarks-for-llm-inference/), or see a concrete example in our [benchmarking results for Mistral 7B](/blog/benchmarking-fast-mistral-7b-inference/).\n\nTable of Contents\n\n  * [Naive implementation for basic testing](#112924-naive-implementation-for-basic-testing)\n  * [Static batching for scheduled load](#112928-static-batching-for-scheduled-load)\n  * [Dynamic batching for AI inference in production](#3500572-dynamic-batching-for-ai-inference-in-production)\n  * [Continuous batching for LLM inference in production](#3500603-continuous-batching-for-llm-inference-in-production)\n\n\n\n## Subscribe to our newsletter\n\nStay up to date on model performance, GPUs, and more.\n\nEmail*\n\n### Related Glossary posts\n\n[View all **Glossary**](/blog/category/glossary/)\n\n## [A quick introduction to speculative decoding](/blog/a-quick-introduction-to-speculative-decoding/)\n\nSpeculative decoding improves LLM inference latency by using a smaller model to generate draft tokens that the larger target model can accept during inference.\n\n![]()![]()![Philip Kiely](https://www.datocms-assets.com/104802/1697123627-headshot_2023-1.png?auto=format&fill=blur&fit=fillmax&w=50)\n\n![]()![]()![Justin Yi headshot](https://www.datocms-assets.com/104802/1692365087-6338a543c71f5034439daa68_img_776bw-p-500.jpeg?auto=format&fill=blur&fit=fillmax&w=50)\n\n![]()![]()![Pankaj Gupta headshot](https://www.datocms-assets.com/104802/1692365874-62570a69154458276850ebed_pankaj.png?auto=format&fill=blur&fit=fillmax&w=50)\n\n[Pankaj Gupta](/author/pankaj-gupta/)\n\n2 others\n\n![]()![]()![A ghostly, glowing llama walking ahead of a real llama](https://www.datocms-assets.com/104802/1734625556-dall-e-2024-12-19-09-30-24-a-photorealistic-movie-scene-at-night-in-a-peruvian-mountain-forest-a-glowing-ghostly-llama-with-a-translucent-ethereal-appearance-is-walking-ahead.webp?auto=format&crop=focalpoint&fit=crop&fp-x=0.49&fp-y=0.43&h=325&w=675)\n\n## [Building high-performance compound AI applications with MongoDB Atlas and Baseten](/blog/building-high-performance-compound-ai-applications-with-mongodb-atlas-and-baseten/)\n\nUsing MongoDB Atlas and Baseten’s Chains framework for compound AI, you can build high-performance compound AI systems.\n\n![]()![]()![Philip Kiely](https://www.datocms-assets.com/104802/1697123627-headshot_2023-1.png?auto=format&fill=blur&fit=fillmax&w=50)\n\n[Philip Kiely](/author/philip-kiely/)\n\n![]()![]()![Leaf rocket](https://www.datocms-assets.com/104802/1726537344-playground.jpeg?auto=format&crop=focalpoint&fit=crop&fp-x=0.57&fp-y=0.47&h=325&w=675)\n\n## [Compound AI systems explained](/blog/compound-ai-systems-explained/)\n\nCompound AI systems combine multiple models and processing steps, and are forming the next generation of AI products.\n\n![]()![]()![Rachel Rapp headshot in black and white](https://www.datocms-assets.com/104802/1716998368-mypiccropped-edited.png?auto=format&fill=blur&fit=fillmax&w=50)\n\n[Rachel Rapp](/author/rachel-rapp/)\n\n![]()![]()![An AI-generated image representing a compound AI system with multiple components.](https://www.datocms-assets.com/104802/1722962169-compound_ai.png?auto=format&crop=focalpoint&fit=crop&fp-x=0.5&fp-y=0.48&h=325&w=675)\n\nPopular models\n\n  * [DeepSeek-R1](/library/deepseek-r1/)\n  * [Llama 3.3 70B Instruct](/library/llama-3-3-70b-instruct/)\n  * [MARS6](/library/mars6/)\n  * [Qwen 2.5 32B Coder Instruct](/library/qwen-2-5-32b-coder-instruct/)\n  * [flux-schnell](/library/flux-schnell/)\n\n\n\nProduct\n\n  * [Pricing](/pricing/)\n  * [Customer stories](/customers/)\n  * [Terms of service](/terms-and-conditions/)\n  * [Privacy policy](/privacy-policy/)\n  * [Security and trust](/trust/)\n\n\n\nDevelopers\n\n  * [Changelog](/changelog/)\n  * [Status](https://status.baseten.co/)\n  * [Docs](https://docs.baseten.co/)\n\n\n\nCompany\n\n  * [About](/about/)\n  * [Blog](/blog/)\n  * [Careers](/careers/) We’re hiring\n\n\n\n[](/)\n\n[](https://github.com/basetenlabs)[](http://twitter.com/basetenco)[](https://www.linkedin.com/company/baseten)[](https://www.youtube.com/channel/UCOCLmqf7Jy3LcsO0SMBGP_Q)\n\n![Hipaa Compliant](/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fhipaa.2eb9b526.png&w=256&q=75)![SOC 2 Type II Certified](/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsoc2-type2.0821ffd3.png&w=256&q=75)\n",
    "content_quality_score": 0.9,
    "summary": null,
    "child_urls": [
        "https://www.baseten.co/blog/the-fastest-most-accurate-and-cost-efficient-whisper-transcription/",
        "https://www.baseten.co/",
        "https://www.baseten.co/library/",
        "https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/",
        "https://www.baseten.co/solutions/transcription/",
        "https://www.baseten.co/solutions/llms/",
        "https://www.baseten.co/solutions/image-generation/",
        "https://www.baseten.co/solutions/text-to-speech/",
        "https://www.baseten.co/solutions/compound-ai/",
        "https://www.baseten.co/solutions/embeddings/",
        "https://www.baseten.co/deployments/baseten-cloud/",
        "https://www.baseten.co/deployments/baseten-self-hosted/",
        "https://www.baseten.co/deployments/baseten-hybrid/",
        "https://www.baseten.co/customers/",
        "https://www.baseten.co/pricing/",
        "https://www.baseten.co/blog/",
        "https://www.baseten.co/careers/",
        "https://www.baseten.co/changelog/",
        "https://www.baseten.co/resources/event/",
        "https://www.baseten.co/resources/guide/",
        "https://www.baseten.co/resources/webinar/",
        "https://www.baseten.co/blog/category/glossary/",
        "https://www.baseten.co/author/matt-howard/",
        "https://www.baseten.co/author/philip-kiely/",
        "https://www.baseten.co/library/stable-diffusion-xl/",
        "https://www.baseten.co/blog/understanding-performance-benchmarks-for-llm-inference/",
        "https://www.baseten.co/blog/benchmarking-fast-mistral-7b-inference/",
        "https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/#112924-naive-implementation-for-basic-testing",
        "https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/#112928-static-batching-for-scheduled-load",
        "https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/#3500572-dynamic-batching-for-ai-inference-in-production",
        "https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/#3500603-continuous-batching-for-llm-inference-in-production",
        "https://www.baseten.co/blog/a-quick-introduction-to-speculative-decoding/",
        "https://www.baseten.co/author/pankaj-gupta/",
        "https://www.baseten.co/blog/building-high-performance-compound-ai-applications-with-mongodb-atlas-and-baseten/",
        "https://www.baseten.co/blog/compound-ai-systems-explained/",
        "https://www.baseten.co/author/rachel-rapp/",
        "https://www.baseten.co/library/deepseek-r1/",
        "https://www.baseten.co/library/llama-3-3-70b-instruct/",
        "https://www.baseten.co/library/mars6/",
        "https://www.baseten.co/library/qwen-2-5-32b-coder-instruct/",
        "https://www.baseten.co/library/flux-schnell/",
        "https://www.baseten.co/terms-and-conditions/",
        "https://www.baseten.co/privacy-policy/",
        "https://www.baseten.co/trust/",
        "https://www.baseten.co/about/",
        "https://docs.baseten.co/",
        "https://app.baseten.co/login/",
        "https://app.baseten.co/signup/",
        "https://twitter.com/intent/tweet?text=Continuous vs dynamic batching for AI inference&url=https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/&via=basetenco",
        "https://www.linkedin.com/shareArticle?mini=true&url=https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/&title=Continuous vs dynamic batching for AI inference",
        "https://www.facebook.com/sharer/sharer.php?u=https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/",
        "https://docs.baseten.co/performance/concurrency",
        "https://status.baseten.co/",
        "https://github.com/basetenlabs",
        "http://twitter.com/basetenco",
        "https://www.linkedin.com/company/baseten",
        "https://www.youtube.com/channel/UCOCLmqf7Jy3LcsO0SMBGP_Q"
    ]
}
{
    "id": "47db1a1a6bcefc743f460bac5e5fae4e",
    "metadata": {
        "id": "47db1a1a6bcefc743f460bac5e5fae4e",
        "url": "https://www.linkedin.com/posts/ashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW?utm_source=share&utm_medium=member_desktop/",
        "title": "Ashish Patel ğŸ‡®ğŸ‡³ on LinkedIn: #llms #datascience #artificialintelligence | 36 comments",
        "properties": {
            "description": "Training a 405B LLM took 16,000 GPUs and 61 daysâ€”hereâ€™s the real math behind it.\n\nAlright, every ML engineer has been there. Youâ€™re sitting in a meeting, andâ€¦ | 36 comments on LinkedIn",
            "keywords": null,
            "author": null,
            "og:description": "Training a 405B LLM took 16,000 GPUs and 61 daysâ€”hereâ€™s the real math behind it.\n\nAlright, every ML engineer has been there. Youâ€™re sitting in a meeting, andâ€¦ | 36 comments on LinkedIn",
            "og:title": "Ashish Patel ğŸ‡®ğŸ‡³ on LinkedIn: #llms #datascience #artificialintelligence | 36 comments",
            "og:type": "article",
            "og:url": "https://www.linkedin.com/posts/ashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW",
            "og:image": "https://media.licdn.com/dms/image/v2/D4D22AQFIquTRXUN6LA/feedshare-shrink_2048_1536/feedshare-shrink_2048_1536/0/1730268143172?e=2147483647&v=beta&t=qbQlChbqMGVretqdi39SvEeU6lwSgtb28oZd_Ia5kb8",
            "twitter:description": "Training a 405B LLM took 16,000 GPUs and 61 daysâ€”hereâ€™s the real math behind it.\n\nAlright, every ML engineer has been there. Youâ€™re sitting in a meeting, andâ€¦ | 36 comments on LinkedIn",
            "twitter:title": "Ashish Patel ğŸ‡®ğŸ‡³ on LinkedIn: #llms #datascience #artificialintelligence | 36 comments",
            "twitter:card": "summary_large_image",
            "twitter:image": "https://media.licdn.com/dms/image/v2/D4D22AQFIquTRXUN6LA/feedshare-shrink_2048_1536/feedshare-shrink_2048_1536/0/1730268143172?e=2147483647&v=beta&t=qbQlChbqMGVretqdi39SvEeU6lwSgtb28oZd_Ia5kb8",
            "twitter:site": "@linkedin"
        }
    },
    "parent_metadata": {
        "id": "89c99f8ed5ecb13fac1e3dd34ec8d6c6",
        "url": "https://www.notion.so/Training-Specs-89c99f8ed5ecb13fac1e3dd34ec8d6c6",
        "title": "Training Specs",
        "properties": {
            "Type": "Leaf"
        }
    },
    "content": "LinkedIn and 3rd parties use essential and non-essential cookies to provide, secure, analyze and improve our Services, and to show you relevant ads (including **professional and job ads**) on and off LinkedIn. Learn more in our [Cookie Policy](https://www.linkedin.com/legal/cookie-policy).\n\nSelect Accept to consent or Reject to decline non-essential cookies for this use. You can update your choices at any time in your [settings](https://www.linkedin.com/mypreferences/g/guest-cookies).\n\nAccept  Reject \n\nAgree & Join LinkedIn \n\nBy clicking Continue to join or sign in, you agree to LinkedInâ€™s [User Agreement](/legal/user-agreement?trk=linkedin-tc_auth-button_user-agreement), [Privacy Policy](/legal/privacy-policy?trk=linkedin-tc_auth-button_privacy-policy), and [Cookie Policy](/legal/cookie-policy?trk=linkedin-tc_auth-button_cookie-policy). \n\n[ Skip to main content ](#main-content) [ LinkedIn ](/?trk=public_post_nav-header-logo)\n\n  * [ Articles  ](https://www.linkedin.com/pulse/topics/home/?trk=public_post_guest_nav_menu_articles)\n  * [ People  ](https://www.linkedin.com/pub/dir/+/+?trk=public_post_guest_nav_menu_people)\n  * [ Learning  ](https://www.linkedin.com/learning/search?trk=public_post_guest_nav_menu_learning)\n  * [ Jobs  ](https://www.linkedin.com/jobs/search?trk=public_post_guest_nav_menu_jobs)\n  * [ Games  ](https://www.linkedin.com/games?trk=public_post_guest_nav_menu_games)\n  * [ Get the app  ](ms-windows-store://pdp/?ProductId=9WZDNCRFJ4Q7&mode=mini&cid=guest_nav_upsell&trk=public_post_guest_nav_menu_windows)\n\n\n\n[ Join now ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_nav-header-join) [ Sign in ](https://www.linkedin.com/login?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&fromSignIn=true&trk=public_post_nav-header-signin)\n\n#  Ashish Patel ğŸ‡®ğŸ‡³â€™s Post\n\n[ ](https://in.linkedin.com/in/ashishpatel2604?trk=public_post_feed-actor-image)\n\n[ Ashish Patel ğŸ‡®ğŸ‡³ ](https://in.linkedin.com/in/ashishpatel2604?trk=public_post_feed-actor-name)\n\nğŸ”¥ 6x LinkedIn Top Voice | Sr AWS AI ML Solution Architect at IBM | Generative AI Expert | Author - Hands-on Time Series Analytics with Python | IBM Quantum ML Certified | 12+ Years in AI | MLOps | IIMA | 100k+Followers \n\n2mo \n\n  * [ Report this post ](/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting)\n\n\n\nTraining a 405B LLM took 16,000 GPUs and 61 daysâ€”hereâ€™s the real math behind it. Alright, every ML engineer has been there. Youâ€™re sitting in a meeting, and someone drops the classic, \"Soâ€¦ how long will it take to train this model?\" At first, I had no idea how to answer it, and when I tried finding answers, most articles threw a ton of jargon without giving me the actual numbers I needed. Frustrating, right? I decided to dig into it myself and figured out how to do a rough back-of-the-napkin calculation that actually works. Letâ€™s break down the key stuff. ğ—§ğ—µğ—² ğ— ğ—®ğ˜ğ—µ: ([https://lnkd.in/dWvgWvXM](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2FdWvgWvXM&urlhash=Tvqa&trk=public_post-text)) â–¸ Itâ€™s all about FLOPs (floating point operations) and GPU power. Basically, you calculate how many FLOPs your model and data require, then divide it by how much power your GPU setup can handle. â–¸ For example, the LLaMA 3.1 model has 405 billion parameters and was trained on 15.6 trillion tokens. In plain English: that means it needed 3.8 Ã— 10Â²âµ FLOPs to train (yep, that's an insane number). â–¸ To train this beast, they used 16,000 H100 GPUs, each working at about 400 teraflops per second. But hereâ€™s the catchâ€”not all GPUs run at full speed. In fact, in this case, the GPUs were running at 38% efficiency due to various network and memory bottlenecks. So, how long did it take to train? Letâ€™s calculate it: 3.8 Ã— 10Â²âµ FLOPs Ã· 6.4 Ã— 10Â¹â¸ FLOPs per second = 61 days But, What About the Cost? ğŸ’¸ This part always gets me. It took 26 million GPU hours to train LLaMA 3.1. With each GPU costing about $2/hour, the total came out to $52 million! Thatâ€™s not a typo. I know, itâ€™s wild. ğ—¦ğ—¼, ğ—›ğ—¼ğ˜„ ğ——ğ—¼ ğ—¬ğ—¼ğ˜‚ ğ—˜ğ˜€ğ˜ğ—¶ğ—ºğ—®ğ˜ğ—² ğ—¬ğ—¼ğ˜‚ğ—¿ ğ—¢ğ˜„ğ—» ğ—§ğ—¿ğ—®ğ—¶ğ—»ğ—¶ğ—»ğ—´ ğ—§ğ—¶ğ—ºğ—²? âœ¦ Total FLOPs â€“ Calculate the size of your model (parameters) and multiply it by the size of your dataset (tokens). The formulaâ€™s in the images I shared. âœ¦ GPU FLOPs â€“ Find out how many FLOPs your GPUs can handle (and be honest about that efficiency drop!). âœ¦ Do the division â€“ FLOPs needed Ã· GPU power = training time. The cool part? Once you know how to calculate this, you stop guessing and start making solid predictions, avoiding those \"ummm, not sure\" moments with the team. Lessons from LLaMA 3 â‡‰ If training a 405B parameter model takes 16,000 GPUs and 61 days, scaling up or down from there gets pretty straightforward. But be warned: donâ€™t just trust theoretical max numbers for your hardware. Use the real-world throughput (MFU) youâ€™re actually getting, or youâ€™ll end up way off. â‡‰ This method isnâ€™t flawless, but itâ€™s miles better than guessing. When youâ€™re dropping millions on GPU hours, you definitely want more than a ballpark guess. Would love to hear your thoughts if you've run similar calculations or hit similar roadblocks! Letâ€™s get a conversation going. ğŸ‘‡ [#LLMs](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fllms&trk=public_post-text) [#DataScience](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fdatascience&trk=public_post-text) [#ArtificialIntelligence](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fartificialintelligence&trk=public_post-text)\n\n[ 444  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_social-actions-reactions) [ 36 Comments ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_social-actions-comments)\n\n[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_like-cta) [ Comment  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment-cta)\n\nShare \n\n  * Copy\n  * LinkedIn\n  * Facebook\n  * Twitter\n\n\n\n[ ](https://at.linkedin.com/in/stefan-komornyik?trk=public_post_comment_actor-image)\n\n[ Stefan Komornyik ](https://at.linkedin.com/in/stefan-komornyik?trk=public_post_comment_actor-name)\n\nIt is my mission to make time series data valuable.\n\n2mo \n\n  * [ Report this comment ](/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)\n\n\n\nThank you-Super interesting! Did you also think about considering the energy consumption? âš¡ï¸\n\n[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_like) [ Reply  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_reply) [ 5 Reactions ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_reactions) 6 Reactions \n\n[ ](https://ch.linkedin.com/in/isabella-kosch-zh?trk=public_post_comment_actor-image)\n\n[ Isabella Kosch ](https://ch.linkedin.com/in/isabella-kosch-zh?trk=public_post_comment_actor-name) 2mo \n\n  * [ Report this comment ](/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)\n\n\n\nThank you! I just learned something. Really interesting.\n\n[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_like) [ Reply  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_reply) [ 2 Reactions ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_reactions) 3 Reactions \n\n[ ](https://it.linkedin.com/in/danielebaranzini?trk=public_post_comment_actor-image)\n\n[ Daniele Baranzini ](https://it.linkedin.com/in/danielebaranzini?trk=public_post_comment_actor-name)\n\nHAIKAI\n\n2mo \n\n  * [ Report this comment ](/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)\n\n\n\ntotally agree with frends here about energy and consumption issues...but I focus on this other subtle but serious aspect: this AI trajectory is sustainable (and doable) only by FAANG and incoming emerging big AI tech monopolies...this is opposite to AI democratization ...opposite to open source at effort level at least....this instead is towards AI concentration in the hands of a few people in FB Google AWS, NVidia and a few others..... what is partially given out is leaked weights of various (a lot) minor versions of LLM or Multimondal alike ...if you (AI citizen or business company) want to excell on such AI systems you HAVE TO BUY THE API OR THE CLOUD by AWS, IBM, ORACLE GOOGLE etc.... Do not even think to use 1/10 of the effort in the picture above.... the LLM economy is shaping the researchers in AI towards such systems.... problem is not AI ....problem is the economy scale that the FAANG are trying (succesfully) to bring forward : ))) Daniele\n\n[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_like) [ Reply  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_reply) [ 6 Reactions ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_reactions) 7 Reactions \n\n[ ](https://il.linkedin.com/in/peleg-zborovsky?trk=public_post_comment_actor-image)\n\n[ Peleg Zborovsky ](https://il.linkedin.com/in/peleg-zborovsky?trk=public_post_comment_actor-name)\n\nMachine Learning Engineer at Libonea | B.Sc. | PyTorchğŸš€\n\n2mo \n\n  * [ Report this comment ](/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)\n\n\n\n[Itzhak Hirschman](https://il.linkedin.com/in/itzhak-hirschman-b16269193?trk=public_post_comment-text) [Dor Getter](https://il.linkedin.com/in/dor-getter-9707711b1?trk=public_post_comment-text) [Zeev Kaminsky](https://il.linkedin.com/in/zeev-kaminsky-024690150?trk=public_post_comment-text) ×œ×©××œ×ª×›× ×™×© ×ª×©×•×‘×”\n\n[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_like) [ Reply  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_reply) [ 1 Reaction ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_reactions) 2 Reactions \n\n[ ](https://se.linkedin.com/in/jerry-%C3%A4r-p-189438188?trk=public_post_comment_actor-image)\n\n[ Jerry Ã¤r P. ](https://se.linkedin.com/in/jerry-%C3%A4r-p-189438188?trk=public_post_comment_actor-name)\n\nAI Advisor @ AiReplyIt Inc | AI Expert, Startup Enthusiast\n\n2mo \n\n  * [ Report this comment ](/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)\n\n\n\nIt should be interesting to know the real bottleneck percentage at inference also\n\n[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_like) [ Reply  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_reply) [ 1 Reaction ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_reactions) 2 Reactions \n\n[ ](https://www.linkedin.com/in/madwesh?trk=public_post_comment_actor-image)\n\n[ Ajay Madwesh ](https://www.linkedin.com/in/madwesh?trk=public_post_comment_actor-name)\n\nTechnology Offering Leadership in AI/ML | IoT | Cloud AI/ML | GenAI/LLM\n\n2mo \n\n  * [ Report this comment ](/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)\n\n\n\nThis is an unsustainable approach.. canâ€™t wait for the next generation of GenAI (pardon the pun)\n\n[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_like) [ Reply  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_reply) 1 Reaction \n\n[ ](https://www.linkedin.com/in/visinha?trk=public_post_comment_actor-image)\n\n[ Vishal Sinha ](https://www.linkedin.com/in/visinha?trk=public_post_comment_actor-name) 2mo \n\n  * [ Report this comment ](/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)\n\n\n\nTY for sharing these details. From where did you get 38% GPU utilization data? It looks pretty high.\n\n[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_like) [ Reply  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_reply) 1 Reaction \n\n[ ](https://www.linkedin.com/company/oktaneai?trk=public_post_comment_actor-image)\n\n[ OKTANE.AI ](https://www.linkedin.com/company/oktaneai?trk=public_post_comment_actor-name) 2mo \n\n  * [ Report this comment ](/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)\n\n\n\nExcellent analysis [Ashish](https://in.linkedin.com/in/ashishpatel2604?trk=public_post_comment-text)! We believe most of the enterprise AI empowered requirements could be solved with RAG (Retrieval Augmented Generation) for a good ROI.\n\n[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_like) [ Reply  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_reply) [ 1 Reaction ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_reactions) 2 Reactions \n\n[ ](https://ro.linkedin.com/in/pauliusztin?trk=public_post_comment_actor-image)\n\n[ Paul Iusztin ](https://ro.linkedin.com/in/pauliusztin?trk=public_post_comment_actor-name)\n\nSenior ML/AI Engineer â€¢ MLOps â€¢ Founder @ Decoding ML ~ Posts and articles about building production-grade ML/AI systems.\n\n2mo \n\n  * [ Report this comment ](/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)\n\n\n\nGreat summary, man. Love it ğŸ˜»\n\n[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_like) [ Reply  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_reply) 1 Reaction \n\n[ See more comments ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_see-more-comments)\n\nTo view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_feed-cta-banner-cta)\n\n##  More Relevant Posts \n\n  * [](https://www.linkedin.com/posts/daemonbehr_genai-aionnutanix-nutanix-activity-7259202801782681600-LAt3)\n\n[ ](https://ca.linkedin.com/in/daemonbehr?trk=public_post_feed-actor-image)\n\n[ Daemon B. ](https://ca.linkedin.com/in/daemonbehr?trk=public_post_feed-actor-name)\n\nField CTO - Americas - Nutanix | NCX #50 - Enterprise AI Strategic Advisor \n\n2mo \n\n    * [ Report this post ](/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdaemonbehr_genai-aionnutanix-nutanix-activity-7259202801782681600-LAt3&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting)\n\nThe truth is that most organizations will not train their own models, and if they do, it will not be a 405B parameter model either. Meta's 405B model is not really meant for inferencing anyway. It serves as a \"teaching model\" that can distill its knowledge to smaller, more efficient models. This process, known as model distillation, allows for the creation of high-performing smaller models with synthetic training data that are more feasible for deployment. Then the small model can actually outperform many larger models. The GPU resources for distillation are a very small fraction of what would be required for training. The other alternatives for using your own data are RAG approaches, which are the most common and least GPU intensive. The questions will always be around what is the desired outcome? What are the steps to get there? What is involved in maintaining it? And what does the cost / benefit / constraint Venn diagram look like? Nutanix provides the easiest on-ramp for organizations to leverage their own data with Enterprise AI, using models that are best suited for your use case. They can be LLMs, or SLMs, distilled, or with RAG, or both. Link here: [https://lnkd.in/ej6g_b5E](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2Fej6g_b5E&urlhash=4gHz&trk=public_post-text) [#GenAI](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fgenai&trk=public_post-text) [#AIonNutanix](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Faionnutanix&trk=public_post-text) [#Nutanix](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fnutanix&trk=public_post-text)\n\n[ ](https://in.linkedin.com/in/ashishpatel2604?trk=public_post_reshare_feed-actor-image)\n\n[ Ashish Patel ğŸ‡®ğŸ‡³ ](https://in.linkedin.com/in/ashishpatel2604?trk=public_post_reshare_feed-actor-name)\n\nğŸ”¥ 6x LinkedIn Top Voice | Sr AWS AI ML Solution Architect at IBM | Generative AI Expert | Author - Hands-on Time Series Analytics with Python | IBM Quantum ML Certified | 12+ Years in AI | MLOps | IIMA | 100k+Followers \n\n2mo \n\nTraining a 405B LLM took 16,000 GPUs and 61 daysâ€”hereâ€™s the real math behind it. Alright, every ML engineer has been there. Youâ€™re sitting in a meeting, and someone drops the classic, \"Soâ€¦ how long will it take to train this model?\" At first, I had no idea how to answer it, and when I tried finding answers, most articles threw a ton of jargon without giving me the actual numbers I needed. Frustrating, right? I decided to dig into it myself and figured out how to do a rough back-of-the-napkin calculation that actually works. Letâ€™s break down the key stuff. ğ—§ğ—µğ—² ğ— ğ—®ğ˜ğ—µ: ([https://lnkd.in/dWvgWvXM](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2FdWvgWvXM&urlhash=Tvqa&trk=public_post_reshare-text)) â–¸ Itâ€™s all about FLOPs (floating point operations) and GPU power. Basically, you calculate how many FLOPs your model and data require, then divide it by how much power your GPU setup can handle. â–¸ For example, the LLaMA 3.1 model has 405 billion parameters and was trained on 15.6 trillion tokens. In plain English: that means it needed 3.8 Ã— 10Â²âµ FLOPs to train (yep, that's an insane number). â–¸ To train this beast, they used 16,000 H100 GPUs, each working at about 400 teraflops per second. But hereâ€™s the catchâ€”not all GPUs run at full speed. In fact, in this case, the GPUs were running at 38% efficiency due to various network and memory bottlenecks. So, how long did it take to train? Letâ€™s calculate it: 3.8 Ã— 10Â²âµ FLOPs Ã· 6.4 Ã— 10Â¹â¸ FLOPs per second = 61 days But, What About the Cost? ğŸ’¸ This part always gets me. It took 26 million GPU hours to train LLaMA 3.1. With each GPU costing about $2/hour, the total came out to $52 million! Thatâ€™s not a typo. I know, itâ€™s wild. ğ—¦ğ—¼, ğ—›ğ—¼ğ˜„ ğ——ğ—¼ ğ—¬ğ—¼ğ˜‚ ğ—˜ğ˜€ğ˜ğ—¶ğ—ºğ—®ğ˜ğ—² ğ—¬ğ—¼ğ˜‚ğ—¿ ğ—¢ğ˜„ğ—» ğ—§ğ—¿ğ—®ğ—¶ğ—»ğ—¶ğ—»ğ—´ ğ—§ğ—¶ğ—ºğ—²? âœ¦ Total FLOPs â€“ Calculate the size of your model (parameters) and multiply it by the size of your dataset (tokens). The formulaâ€™s in the images I shared. âœ¦ GPU FLOPs â€“ Find out how many FLOPs your GPUs can handle (and be honest about that efficiency drop!). âœ¦ Do the division â€“ FLOPs needed Ã· GPU power = training time. The cool part? Once you know how to calculate this, you stop guessing and start making solid predictions, avoiding those \"ummm, not sure\" moments with the team. Lessons from LLaMA 3 â‡‰ If training a 405B parameter model takes 16,000 GPUs and 61 days, scaling up or down from there gets pretty straightforward. But be warned: donâ€™t just trust theoretical max numbers for your hardware. Use the real-world throughput (MFU) youâ€™re actually getting, or youâ€™ll end up way off. â‡‰ This method isnâ€™t flawless, but itâ€™s miles better than guessing. When youâ€™re dropping millions on GPU hours, you definitely want more than a ballpark guess. Would love to hear your thoughts if you've run similar calculations or hit similar roadblocks! Letâ€™s get a conversation going. ğŸ‘‡ [#LLMs](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fllms&trk=public_post_reshare-text) [#DataScience](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fdatascience&trk=public_post_reshare-text) [#ArtificialIntelligence](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fartificialintelligence&trk=public_post_reshare-text)\n\n[ 9  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdaemonbehr_genai-aionnutanix-nutanix-activity-7259202801782681600-LAt3&trk=public_post_social-actions-reactions) [ 1 Comment ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdaemonbehr_genai-aionnutanix-nutanix-activity-7259202801782681600-LAt3&trk=public_post_social-actions-comments)\n\n[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdaemonbehr_genai-aionnutanix-nutanix-activity-7259202801782681600-LAt3&trk=public_post_like-cta) [ Comment  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdaemonbehr_genai-aionnutanix-nutanix-activity-7259202801782681600-LAt3&trk=public_post_comment-cta)\n\nShare \n    * Copy\n    * LinkedIn\n    * Facebook\n    * Twitter\n\nTo view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdaemonbehr_genai-aionnutanix-nutanix-activity-7259202801782681600-LAt3&trk=public_post_feed-cta-banner-cta)\n\n  * [](https://www.linkedin.com/posts/ganeshjagadeesan_llms-artificialintelligence-datascience-activity-7257283675287953409-NxTr)\n\n[ ](https://in.linkedin.com/in/ganeshjagadeesan?trk=public_post_feed-actor-image)\n\n[ Ganesh Jagadeesan ](https://in.linkedin.com/in/ganeshjagadeesan?trk=public_post_feed-actor-name)\n\nEnterprise Data Science Specialist @Mastech Digital | NLP | NER | Deep Learning | Gen AI | MLops \n\n2mo \n\n    * [ Report this post ](/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fganeshjagadeesan_llms-artificialintelligence-datascience-activity-7257283675287953409-NxTr&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting)\n\nThis post is a goldmine! ğŸ’¡ Accurately calculating training time for LLMs has always been tricky, and itâ€™s awesome to see a practical breakdown like this. ğŸ”¢ FLOPs and GPU throughput arenâ€™t usually discussed in this level of detailâ€”itâ€™s refreshing to see the real-world math behind training something as massive as LLaMA 3.1. ğŸ§  The efficiency drop you highlighted is such a critical point. Many overlook how network and memory bottlenecks impact real-world GPU performance, and that 38% efficiency makes a big difference in cost and timelines. âš™ï¸ğŸ’¸ Also, $52 million in GPU hours is mind-blowing! ğŸ˜² It really emphasizes how planning and optimization are essential when training models at this scale. Knowing the true throughput youâ€™re getting (instead of trusting theoretical max speeds) is key to avoiding costly surprises. ğŸ—ï¸ğŸ“‰ I love the step-by-step approach for estimating your own training timeâ€”super helpful for anyone navigating those awkward â€œhow long will this take?â€ moments. ğŸ˜‚ Thanks for sharing the formulas too, theyâ€™ll be a lifesaver for future projects. Looking forward to hearing more about others' experiences with training large modelsâ€”these insights will definitely make our jobs a bit easier! ğŸ™Œ [#LLMs](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fllms&trk=public_post-text) [#ArtificialIntelligence](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fartificialintelligence&trk=public_post-text) [#DataScience](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fdatascience&trk=public_post-text) [#MachineLearning](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fmachinelearning&trk=public_post-text) [#GPU](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fgpu&trk=public_post-text) [#ModelTraining](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fmodeltraining&trk=public_post-text) [#AIInsights](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Faiinsights&trk=public_post-text) [#DeepLearning](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fdeeplearning&trk=public_post-text) [#AIOptimization](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Faioptimization&trk=public_post-text)\n\n[ ](https://in.linkedin.com/in/ashishpatel2604?trk=public_post_reshare_feed-actor-image)\n\n[ Ashish Patel ğŸ‡®ğŸ‡³ ](https://in.linkedin.com/in/ashishpatel2604?trk=public_post_reshare_feed-actor-name)\n\nğŸ”¥ 6x LinkedIn Top Voice | Sr AWS AI ML Solution Architect at IBM | Generative AI Expert | Author - Hands-on Time Series Analytics with Python | IBM Quantum ML Certified | 12+ Years in AI | MLOps | IIMA | 100k+Followers \n\n2mo \n\nTraining a 405B LLM took 16,000 GPUs and 61 daysâ€”hereâ€™s the real math behind it. Alright, every ML engineer has been there. Youâ€™re sitting in a meeting, and someone drops the classic, \"Soâ€¦ how long will it take to train this model?\" At first, I had no idea how to answer it, and when I tried finding answers, most articles threw a ton of jargon without giving me the actual numbers I needed. Frustrating, right? I decided to dig into it myself and figured out how to do a rough back-of-the-napkin calculation that actually works. Letâ€™s break down the key stuff. ğ—§ğ—µğ—² ğ— ğ—®ğ˜ğ—µ: ([https://lnkd.in/dWvgWvXM](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2FdWvgWvXM&urlhash=Tvqa&trk=public_post_reshare-text)) â–¸ Itâ€™s all about FLOPs (floating point operations) and GPU power. Basically, you calculate how many FLOPs your model and data require, then divide it by how much power your GPU setup can handle. â–¸ For example, the LLaMA 3.1 model has 405 billion parameters and was trained on 15.6 trillion tokens. In plain English: that means it needed 3.8 Ã— 10Â²âµ FLOPs to train (yep, that's an insane number). â–¸ To train this beast, they used 16,000 H100 GPUs, each working at about 400 teraflops per second. But hereâ€™s the catchâ€”not all GPUs run at full speed. In fact, in this case, the GPUs were running at 38% efficiency due to various network and memory bottlenecks. So, how long did it take to train? Letâ€™s calculate it: 3.8 Ã— 10Â²âµ FLOPs Ã· 6.4 Ã— 10Â¹â¸ FLOPs per second = 61 days But, What About the Cost? ğŸ’¸ This part always gets me. It took 26 million GPU hours to train LLaMA 3.1. With each GPU costing about $2/hour, the total came out to $52 million! Thatâ€™s not a typo. I know, itâ€™s wild. ğ—¦ğ—¼, ğ—›ğ—¼ğ˜„ ğ——ğ—¼ ğ—¬ğ—¼ğ˜‚ ğ—˜ğ˜€ğ˜ğ—¶ğ—ºğ—®ğ˜ğ—² ğ—¬ğ—¼ğ˜‚ğ—¿ ğ—¢ğ˜„ğ—» ğ—§ğ—¿ğ—®ğ—¶ğ—»ğ—¶ğ—»ğ—´ ğ—§ğ—¶ğ—ºğ—²? âœ¦ Total FLOPs â€“ Calculate the size of your model (parameters) and multiply it by the size of your dataset (tokens). The formulaâ€™s in the images I shared. âœ¦ GPU FLOPs â€“ Find out how many FLOPs your GPUs can handle (and be honest about that efficiency drop!). âœ¦ Do the division â€“ FLOPs needed Ã· GPU power = training time. The cool part? Once you know how to calculate this, you stop guessing and start making solid predictions, avoiding those \"ummm, not sure\" moments with the team. Lessons from LLaMA 3 â‡‰ If training a 405B parameter model takes 16,000 GPUs and 61 days, scaling up or down from there gets pretty straightforward. But be warned: donâ€™t just trust theoretical max numbers for your hardware. Use the real-world throughput (MFU) youâ€™re actually getting, or youâ€™ll end up way off. â‡‰ This method isnâ€™t flawless, but itâ€™s miles better than guessing. When youâ€™re dropping millions on GPU hours, you definitely want more than a ballpark guess. Would love to hear your thoughts if you've run similar calculations or hit similar roadblocks! Letâ€™s get a conversation going. ğŸ‘‡ [#LLMs](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fllms&trk=public_post_reshare-text) [#DataScience](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fdatascience&trk=public_post_reshare-text) [#ArtificialIntelligence](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fartificialintelligence&trk=public_post_reshare-text)\n\n[ 8  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fganeshjagadeesan_llms-artificialintelligence-datascience-activity-7257283675287953409-NxTr&trk=public_post_social-actions-reactions)\n\n[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fganeshjagadeesan_llms-artificialintelligence-datascience-activity-7257283675287953409-NxTr&trk=public_post_like-cta) [ Comment  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fganeshjagadeesan_llms-artificialintelligence-datascience-activity-7257283675287953409-NxTr&trk=public_post_comment-cta)\n\nShare \n    * Copy\n    * LinkedIn\n    * Facebook\n    * Twitter\n\nTo view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fganeshjagadeesan_llms-artificialintelligence-datascience-activity-7257283675287953409-NxTr&trk=public_post_feed-cta-banner-cta)\n\n  * [](https://www.linkedin.com/posts/dylan-poh_the-post-sheds-light-on-the-staggering-costs-activity-7258089394434170880-Y08V)\n\n[ ](https://sg.linkedin.com/in/dylan-poh?trk=public_post_feed-actor-image)\n\n[ Dylan Poh ](https://sg.linkedin.com/in/dylan-poh?trk=public_post_feed-actor-name)\n\nAI Engineer at AI Singapore \n\n2mo \n\n    * [ Report this post ](/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdylan-poh_the-post-sheds-light-on-the-staggering-costs-activity-7258089394434170880-Y08V&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting)\n\nThe post sheds light on the staggering costs and resources needed to train a 405 billion-parameter model: tens of thousands of GPUs, millions of dollars, and massive energy consumption. This scale pushes us to ask tough questions â€“ are we moving toward genuine problem-solving, or just chasing size? Can we make AI more accessible and sustainable, or are we building barriers with resource-heavy giants? As we push these boundaries, balancing innovation with responsibility becomes more critical than ever.\n\n[ ](https://in.linkedin.com/in/ashishpatel2604?trk=public_post_reshare_feed-actor-image)\n\n[ Ashish Patel ğŸ‡®ğŸ‡³ ](https://in.linkedin.com/in/ashishpatel2604?trk=public_post_reshare_feed-actor-name)\n\nğŸ”¥ 6x LinkedIn Top Voice | Sr AWS AI ML Solution Architect at IBM | Generative AI Expert | Author - Hands-on Time Series Analytics with Python | IBM Quantum ML Certified | 12+ Years in AI | MLOps | IIMA | 100k+Followers \n\n2mo \n\nTraining a 405B LLM took 16,000 GPUs and 61 daysâ€”hereâ€™s the real math behind it. Alright, every ML engineer has been there. Youâ€™re sitting in a meeting, and someone drops the classic, \"Soâ€¦ how long will it take to train this model?\" At first, I had no idea how to answer it, and when I tried finding answers, most articles threw a ton of jargon without giving me the actual numbers I needed. Frustrating, right? I decided to dig into it myself and figured out how to do a rough back-of-the-napkin calculation that actually works. Letâ€™s break down the key stuff. ğ—§ğ—µğ—² ğ— ğ—®ğ˜ğ—µ: ([https://lnkd.in/dWvgWvXM](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2FdWvgWvXM&urlhash=Tvqa&trk=public_post_reshare-text)) â–¸ Itâ€™s all about FLOPs (floating point operations) and GPU power. Basically, you calculate how many FLOPs your model and data require, then divide it by how much power your GPU setup can handle. â–¸ For example, the LLaMA 3.1 model has 405 billion parameters and was trained on 15.6 trillion tokens. In plain English: that means it needed 3.8 Ã— 10Â²âµ FLOPs to train (yep, that's an insane number). â–¸ To train this beast, they used 16,000 H100 GPUs, each working at about 400 teraflops per second. But hereâ€™s the catchâ€”not all GPUs run at full speed. In fact, in this case, the GPUs were running at 38% efficiency due to various network and memory bottlenecks. So, how long did it take to train? Letâ€™s calculate it: 3.8 Ã— 10Â²âµ FLOPs Ã· 6.4 Ã— 10Â¹â¸ FLOPs per second = 61 days But, What About the Cost? ğŸ’¸ This part always gets me. It took 26 million GPU hours to train LLaMA 3.1. With each GPU costing about $2/hour, the total came out to $52 million! Thatâ€™s not a typo. I know, itâ€™s wild. ğ—¦ğ—¼, ğ—›ğ—¼ğ˜„ ğ——ğ—¼ ğ—¬ğ—¼ğ˜‚ ğ—˜ğ˜€ğ˜ğ—¶ğ—ºğ—®ğ˜ğ—² ğ—¬ğ—¼ğ˜‚ğ—¿ ğ—¢ğ˜„ğ—» ğ—§ğ—¿ğ—®ğ—¶ğ—»ğ—¶ğ—»ğ—´ ğ—§ğ—¶ğ—ºğ—²? âœ¦ Total FLOPs â€“ Calculate the size of your model (parameters) and multiply it by the size of your dataset (tokens). The formulaâ€™s in the images I shared. âœ¦ GPU FLOPs â€“ Find out how many FLOPs your GPUs can handle (and be honest about that efficiency drop!). âœ¦ Do the division â€“ FLOPs needed Ã· GPU power = training time. The cool part? Once you know how to calculate this, you stop guessing and start making solid predictions, avoiding those \"ummm, not sure\" moments with the team. Lessons from LLaMA 3 â‡‰ If training a 405B parameter model takes 16,000 GPUs and 61 days, scaling up or down from there gets pretty straightforward. But be warned: donâ€™t just trust theoretical max numbers for your hardware. Use the real-world throughput (MFU) youâ€™re actually getting, or youâ€™ll end up way off. â‡‰ This method isnâ€™t flawless, but itâ€™s miles better than guessing. When youâ€™re dropping millions on GPU hours, you definitely want more than a ballpark guess. Would love to hear your thoughts if you've run similar calculations or hit similar roadblocks! Letâ€™s get a conversation going. ğŸ‘‡ [#LLMs](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fllms&trk=public_post_reshare-text) [#DataScience](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fdatascience&trk=public_post_reshare-text) [#ArtificialIntelligence](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fartificialintelligence&trk=public_post_reshare-text)\n\n[ 6  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdylan-poh_the-post-sheds-light-on-the-staggering-costs-activity-7258089394434170880-Y08V&trk=public_post_social-actions-reactions)\n\n[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdylan-poh_the-post-sheds-light-on-the-staggering-costs-activity-7258089394434170880-Y08V&trk=public_post_like-cta) [ Comment  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdylan-poh_the-post-sheds-light-on-the-staggering-costs-activity-7258089394434170880-Y08V&trk=public_post_comment-cta)\n\nShare \n    * Copy\n    * LinkedIn\n    * Facebook\n    * Twitter\n\nTo view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdylan-poh_the-post-sheds-light-on-the-staggering-costs-activity-7258089394434170880-Y08V&trk=public_post_feed-cta-banner-cta)\n\n  * [](https://www.linkedin.com/posts/mazadgole_back-of-the-envelope-calculation-for-gpu-activity-7209639861891317761-VmX7)\n\n[ ](https://ca.linkedin.com/in/mazadgole?trk=public_post_feed-actor-image)\n\n[ Mehdi Azad ](https://ca.linkedin.com/in/mazadgole?trk=public_post_feed-actor-name)\n\nMachine Learning | Data Science | Educator \n\n7mo  Edited \n\n    * [ Report this post ](/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmazadgole_back-of-the-envelope-calculation-for-gpu-activity-7209639861891317761-VmX7&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting)\n\nI'm sure that when you want to work with LLMs, these questions come to mind: 1- How many GPUs are required to store and fine-tune LLMs? (Given an NVIDIA V100 GPU with 32GB memory) 2- How long did it take to pre-train LLMs? 3- How can compute-efficient methods, such as quantization or LoRA, reduce the GPU requirements ? To answer the questions, let's do a simple math: \"Storage\": 1 parameter = 4 bytes. A model with 1B parameters requires 4 GB of memory. With 1 GPU (32 GB), I can perform inference using a model with up to 8 billion parameters. (I can do better with quantization method.) For GPT-3 with 175B parameters, you need ~22 GPUs for just storing the weights. \"Fine-Tuning\": Considering overheads (gradients and optimizer states), fine-tuning GPT-3 requires approximately x6 times more memory than just storing the weights, around 132 GPUs. So I need ~100 more GPUs for full fine tuning. Good news: with LoRA you can to the fine tuning in just 1 GPU!!! \"Pre-training\": What about pre-training GPT-3 on a vast amount of data? How many GPUs do I need, and how long does it take? The answer is ~1000 GPUs running for 30 days. Can I do better?? Yes. You can see my simple calculations and interesting results in this blog post. [#LLMs](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fllms&trk=public_post-text) [#GPUs](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fgpus&trk=public_post-text) [#Quantization](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fquantization&trk=public_post-text) [#LoRA](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Flora&trk=public_post-text) [#MachineLearning](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fmachinelearning&trk=public_post-text)\n\n## [ Back-of-the-envelope calculation for GPU requirements of LLMs  mabbasiazad.github.io  ](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Fmabbasiazad%2Egithub%2Eio%2Fportfolio%2Fposts%2Fllms_gpu%2F&urlhash=AWz1&trk=public_post_feed-article-content)\n\n[ 9  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmazadgole_back-of-the-envelope-calculation-for-gpu-activity-7209639861891317761-VmX7&trk=public_post_social-actions-reactions)\n\n[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmazadgole_back-of-the-envelope-calculation-for-gpu-activity-7209639861891317761-VmX7&trk=public_post_like-cta) [ Comment  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmazadgole_back-of-the-envelope-calculation-for-gpu-activity-7209639861891317761-VmX7&trk=public_post_comment-cta)\n\nShare \n    * Copy\n    * LinkedIn\n    * Facebook\n    * Twitter\n\nTo view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmazadgole_back-of-the-envelope-calculation-for-gpu-activity-7209639861891317761-VmX7&trk=public_post_feed-cta-banner-cta)\n\n  * [](https://www.linkedin.com/posts/avi-chawla_how-to-synchronize-gpus-with-multi-gpu-training-activity-7222547319672463360-uXQ-)\n\n[ ](https://in.linkedin.com/in/avi-chawla?trk=public_post_feed-actor-image)\n\n[ Avi Chawla ](https://in.linkedin.com/in/avi-chawla?trk=public_post_feed-actor-name)\n\nCo-founder DailyDoseofDS | IIT Varanasi | ex-AI Engineer MastercardAI | Newsletter (130k+) \n\n5mo \n\n    * [ Report this post ](/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Favi-chawla_how-to-synchronize-gpus-with-multi-gpu-training-activity-7222547319672463360-uXQ-&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting)\n\nHow to synchronize GPUs with multi-GPU training ğŸ§©? A significant run-time bottleneck of multi-GPU training is observed during model synchronization. Consider data parallelism, which: - Replicates the model across all GPUs. - Divides the available data into smaller batches, and each batch is processed by a separate GPU. - Computes the gradients on each GPU and then communicates them to every other GPU. Since every GPU processes a different chunk of the data, the gradients will also be different across devices. Thus, before updating the model parameters on each GPU device, local gradients must be communicated to all other devices. I covered two strategies for intermediate-sized models in yesterday's issue of the [Daily Dose of Data Science](https://in.linkedin.com/company/daily-dose-of-ds?trk=public_post-text) newsletter: [https://lnkd.in/g56-7HsZ](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2Fg56-7HsZ&urlhash=JNnU&trk=public_post-text). 1) All-reduce: - One way is to transfer gradient from every GPU to every other GPU (naive approach). - Another way is to transfer gradients to one GPU, average them, and communicate back to all GPUs (better but not fully optimal and scalable). 2) Ring reduce: - While the total transfers are the same as the second procedure above, ring-reduce is much more scalable and efficient. - It does this with a ring formulation, then segments the local gradients on each device and transfers a segment to the next GPU. We covered it in detail here: [https://lnkd.in/g56-7HsZ](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2Fg56-7HsZ&urlhash=JNnU&trk=public_post-text). ğŸ‘‰ Over to you: Can you optimize ring-reduce even further?\n\n[ 109  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Favi-chawla_how-to-synchronize-gpus-with-multi-gpu-training-activity-7222547319672463360-uXQ-&trk=public_post_social-actions-reactions) [ 1 Comment ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Favi-chawla_how-to-synchronize-gpus-with-multi-gpu-training-activity-7222547319672463360-uXQ-&trk=public_post_social-actions-comments)\n\n[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Favi-chawla_how-to-synchronize-gpus-with-multi-gpu-training-activity-7222547319672463360-uXQ-&trk=public_post_like-cta) [ Comment  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Favi-chawla_how-to-synchronize-gpus-with-multi-gpu-training-activity-7222547319672463360-uXQ-&trk=public_post_comment-cta)\n\nShare \n    * Copy\n    * LinkedIn\n    * Facebook\n    * Twitter\n\nTo view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Favi-chawla_how-to-synchronize-gpus-with-multi-gpu-training-activity-7222547319672463360-uXQ-&trk=public_post_feed-cta-banner-cta)\n\n  * [](https://www.linkedin.com/posts/benjamin-j-todd_how-much-ai-inference-can-we-do-activity-7195268236697915393-SFAH)\n\n[ ](https://uk.linkedin.com/in/benjamin-j-todd?trk=public_post_feed-actor-image)\n\n[ Benjamin Todd ](https://uk.linkedin.com/in/benjamin-j-todd?trk=public_post_feed-actor-name)\n\nFounder of 80,000 Hours | author | Writing about AI, careers and doing good \n\n8mo \n\n    * [ Report this post ](/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fbenjamin-j-todd_how-much-ai-inference-can-we-do-activity-7195268236697915393-SFAH&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting)\n\nIf someone has a bunch of GPUs, how much AI inference can they do with them? The approach I most often see non-experts take is to look up FLOP/s for the GPUs and then divide that by the FLOP per forward pass. E.g. The A100 is listed at 312 teraflop per second on its spec sheet (FP16 tensor), and a forward pass of GPT-4 requires 5.6e11 FLOP per forward pass, which would be 560 per second. But this turns out to be way too high. Even if spec sheet efficiency could be achieved (it can't), the model parameters also need to pass through the GPU's memory. The A100 only has 2000 GB/s of memory bandwidth â€“ only enough for 4 forward passes! But that turns out to be way too low. In reality, multiple GPUs are parallelised and forward passes are processed in batches. This means real world efficiency is somewhere between these upper and lower bounds. Semianalysis estimates for GPT-4 on A100s, it's around 10x the lower bound and 10% of the upper. In the full post, I extend this to more advanced chips, and speculate about it might change in the future: [https://lnkd.in/e5DNbZDK](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2Fe5DNbZDK&urlhash=8KaZ&trk=public_post-text)\n\n## [ How much AI inference can we do?  benjamintodd.substack.com  ](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Fbenjamintodd%2Esubstack%2Ecom%2Fp%2Fhow-much-ai-inference-can-we-do&urlhash=QrZ1&trk=public_post_feed-article-content)\n\n[ 6  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fbenjamin-j-todd_how-much-ai-inference-can-we-do-activity-7195268236697915393-SFAH&trk=public_post_social-actions-reactions)\n\n[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fbenjamin-j-todd_how-much-ai-inference-can-we-do-activity-7195268236697915393-SFAH&trk=public_post_like-cta) [ Comment  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fbenjamin-j-todd_how-much-ai-inference-can-we-do-activity-7195268236697915393-SFAH&trk=public_post_comment-cta)\n\nShare \n    * Copy\n    * LinkedIn\n    * Facebook\n    * Twitter\n\nTo view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fbenjamin-j-todd_how-much-ai-inference-can-we-do-activity-7195268236697915393-SFAH&trk=public_post_feed-cta-banner-cta)\n\n  * [](https://www.linkedin.com/posts/martechrichard_training-ai-models-on-cpu-activity-7236981179684466688-zQ9z)\n\n[ ](https://www.linkedin.com/company/martechrichard?trk=public_post_feed-actor-image)\n\n[ MarTechRichard ](https://www.linkedin.com/company/martechrichard?trk=public_post_feed-actor-name)\n\n9 followers \n\n4mo  Edited \n\n    * [ Report this post ](/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmartechrichard_training-ai-models-on-cpu-activity-7236981179684466688-zQ9z&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting)\n\nğŸš€ Exciting times in AI! As GPU demand soars, letâ€™s not overlook the power of CPUs! ğŸ¤–ğŸ’» ğŸ”‘ Key insights from our latest article: 1ï¸âƒ£ CPUs can still excel in ML when GPUs are scarce. 2ï¸âƒ£ Performance optimization with Intel Xeon & PyTorch can boost efficiency. 3ï¸âƒ£ Smart data preprocessing minimizes CPU load. ğŸ’¡ Businesses stand to gain: - Cost-effective solutions for smaller projects ğŸ’° - Scalability without GPU constraints ğŸ“ˆ - Enhanced development productivity â²ï¸ How are YOU optimizing CPU performance in your ML tasks? Letâ€™s discuss! ğŸ’¬ ğŸ”— Read more here: [Training AI Models on CPU - Towards Data Science]([https://lnkd.in/eB5w7N8U](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2FeB5w7N8U&urlhash=0hjW&trk=public_post-text)) ğŸ“ For deeper insights, contact us via WhatsApp: [[https://lnkd.in/e9sTptsu](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2Fe9sTptsu&urlhash=QLXV&trk=public_post-text)) or connect on LinkedIn! ğŸ”— \n\n## [ Exploring CPU Training for AI Models Amid GPU Resource Limitations  towardsdatascience.com  ](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Ftowardsdatascience%2Ecom%2Ftraining-ai-models-on-cpu-3903adc9f388&urlhash=gBHg&trk=public_post_feed-article-content)\n\n[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmartechrichard_training-ai-models-on-cpu-activity-7236981179684466688-zQ9z&trk=public_post_like-cta) [ Comment  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmartechrichard_training-ai-models-on-cpu-activity-7236981179684466688-zQ9z&trk=public_post_comment-cta)\n\nShare \n    * Copy\n    * LinkedIn\n    * Facebook\n    * Twitter\n\nTo view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmartechrichard_training-ai-models-on-cpu-activity-7236981179684466688-zQ9z&trk=public_post_feed-cta-banner-cta)\n\n  * [](https://www.linkedin.com/posts/jay-shah-335689167_cutlass-tutorial-persistent-kernels-and-activity-7276341827874562048-Fxzm)\n\n[ ](https://www.linkedin.com/in/jay-shah-335689167?trk=public_post_feed-actor-image)\n\n[ Jay Shah ](https://www.linkedin.com/in/jay-shah-335689167?trk=public_post_feed-actor-name)\n\nResearch Scientist at Colfax International \n\n4w \n\n    * [ Report this post ](/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fjay-shah-335689167_cutlass-tutorial-persistent-kernels-and-activity-7276341827874562048-Fxzm&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting)\n\nI'm happy to share the final part of our three part series on writing optimized GEMM kernels for NVIDIA GPUs using CUTLASS library abstractions. This last installment explains the Stream-K algorithm for scheduling work over threadblocks and how it surmounts the problem of wave quantization. To explain the problem, consider a naive \"data-parallel\" approach to GEMM that partitions the output matrix as a grid of tiles and assigns tiles to threadblocks one-to-one. A discrete number of waves will be launched to process the computation, quantized according to the total number of streaming multiprocessors (SMs). If the number of tiles is then not evenly divisible by the number of SMs, the last tail wave will process a reduced number of work tiles while still taking the time of a full wave, severely degrading overall TFLOPS performance. Roughly speaking, Stream-K addresses this issue by introducing parallelism along the inner K-dimension and enabling threadblocks to \"fractionally\" process output tiles, rebalancing the distribution of total work across the SMs. Our tutorial also describes CUTLASS's tile scheduler abstraction, which we leverage to effectively implement scheduling strategies like Stream-K. This separates scheduling optimizations that work at the grid level from those that target the inner load and compute loops, like the pipelining and warp specialization strategies we discussed in part 2 of this series. We both implement a simplified version of the Stream-K tile scheduler for a custom GEMM kernel and discuss the inner workings of CUTLASS's more advanced version. Work done in collaboration with my wonderful colleagues at Colfax Research! Happy holidays! [https://lnkd.in/gZkRqYeN](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2FgZkRqYeN&urlhash=DQnO&trk=public_post-text)\n\n## [ CUTLASS Tutorial: Persistent Kernels and Stream-K  https://research.colfax-intl.com  ](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Fresearch%2Ecolfax-intl%2Ecom%2Fcutlass-tutorial-persistent-kernels-and-stream-k%2F&urlhash=tx4R&trk=public_post_feed-article-content)\n\n[ 563  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fjay-shah-335689167_cutlass-tutorial-persistent-kernels-and-activity-7276341827874562048-Fxzm&trk=public_post_social-actions-reactions) [ 5 Comments ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fjay-shah-335689167_cutlass-tutorial-persistent-kernels-and-activity-7276341827874562048-Fxzm&trk=public_post_social-actions-comments)\n\n[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fjay-shah-335689167_cutlass-tutorial-persistent-kernels-and-activity-7276341827874562048-Fxzm&trk=public_post_like-cta) [ Comment  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fjay-shah-335689167_cutlass-tutorial-persistent-kernels-and-activity-7276341827874562048-Fxzm&trk=public_post_comment-cta)\n\nShare \n    * Copy\n    * LinkedIn\n    * Facebook\n    * Twitter\n\nTo view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fjay-shah-335689167_cutlass-tutorial-persistent-kernels-and-activity-7276341827874562048-Fxzm&trk=public_post_feed-cta-banner-cta)\n\n  * [](https://www.linkedin.com/posts/rajat-walia_mechanicalengineering-mechanical-aerospace-activity-7205433434528256000-dyOJ)\n\n[ ](https://in.linkedin.com/in/rajat-walia?trk=public_post_feed-actor-image)\n\n[ Rajat Walia ](https://in.linkedin.com/in/rajat-walia?trk=public_post_feed-actor-name) Rajat Walia is an Influencer\n\nCFD Engineer @ Mercedes-Benz | Aerodynamics | Thermal | Aero-Thermal | Computational Fluid Dynamics | Valeo | Formula Student \n\n7mo \n\n    * [ Report this post ](/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Frajat-walia_mechanicalengineering-mechanical-aerospace-activity-7205433434528256000-dyOJ&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting)\n\nCPU vs GPU vs TPU! What is the difference between CPU , GPU and TPU and what are the impacts of it in machine learning computations! In the rapidly evolving world of machine learning, the hardware you choose can significantly influence the performance and efficiency of your computations. Let's break down the differences between CPUs, GPUs, and TPUs, and see how each contributes to machine learning. CPU (Central Processing Unit) General-purpose Processor: CPUs are designed to handle a wide range of tasks. They are versatile and capable of performing complex calculations sequentially. Core Count: Typically, CPUs have fewer cores (4-16 cores in most consumer-grade processors) but they are powerful and optimized for a variety of tasks. Machine Learning: Suitable for small-scale machine learning tasks, data preprocessing, and tasks requiring complex decision-making. GPU (Graphics Processing Unit) Parallel Processing Powerhouse: GPUs are designed for parallel processing, making them ideal for handling large-scale computations simultaneously. Core Count: GPUs have thousands of smaller, efficient cores, perfect for tasks that can be parallelized. Machine Learning: Excellent for training deep learning models, image and video processing, and large-scale data operations. GPUs accelerate the training process by handling multiple computations in parallel. TPU (Tensor Processing Unit) ML-Specific Accelerator: TPUs are specialized hardware designed by Google specifically for accelerating machine learning workloads. Optimized for TensorFlow: TPUs are particularly optimized for TensorFlow, Google's open-source machine learning framework. Machine Learning: They provide significant speed-ups for training and inference of neural networks, especially for models with large-scale matrix multiplications. Training Speed: GPUs and TPUs dramatically reduce training time compared to CPUs. Model Complexity: With GPUs and TPUs, you can train more complex models in less time. Scalability: GPUs and TPUs enable scalable solutions for extensive data and model requirements. Mention your thoughts on CPU, GPU & TPU in the comment section! [#mechanicalengineering](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fmechanicalengineering&trk=public_post-text) [#mechanical](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fmechanical&trk=public_post-text) [#aerospace](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Faerospace&trk=public_post-text) [#automotive](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fautomotive&trk=public_post-text) [#cfd](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fcfd&trk=public_post-text)\n\n[ 72  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Frajat-walia_mechanicalengineering-mechanical-aerospace-activity-7205433434528256000-dyOJ&trk=public_post_social-actions-reactions)\n\n[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Frajat-walia_mechanicalengineering-mechanical-aerospace-activity-7205433434528256000-dyOJ&trk=public_post_like-cta) [ Comment  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Frajat-walia_mechanicalengineering-mechanical-aerospace-activity-7205433434528256000-dyOJ&trk=public_post_comment-cta)\n\nShare \n    * Copy\n    * LinkedIn\n    * Facebook\n    * Twitter\n\nTo view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Frajat-walia_mechanicalengineering-mechanical-aerospace-activity-7205433434528256000-dyOJ&trk=public_post_feed-cta-banner-cta)\n\n  * [](https://www.linkedin.com/posts/marek-bar%C3%A1k-31977a55_cutlass-tutorial-persistent-kernels-and-activity-7276513592210821120-tuTT)\n\n[ ](https://at.linkedin.com/in/marek-bar%C3%A1k-31977a55?trk=public_post_feed-actor-image)\n\n[ Marek BarÃ¡k ](https://at.linkedin.com/in/marek-bar%C3%A1k-31977a55?trk=public_post_feed-actor-name)\n\nData Alchemist & code:Breaker \n\n4w \n\n    * [ Report this post ](/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmarek-bar%25C3%25A1k-31977a55_cutlass-tutorial-persistent-kernels-and-activity-7276513592210821120-tuTT&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting)\n\nAgain an resource that can be ignored, especially if you are in GPU programming, the skill of 2025+. [#GPU](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fgpu&trk=public_post-text) [#NVIDIA](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fnvidia&trk=public_post-text)\n\n[ ](https://www.linkedin.com/in/jay-shah-335689167?trk=public_post_reshare_feed-actor-image)\n\n[ Jay Shah ](https://www.linkedin.com/in/jay-shah-335689167?trk=public_post_reshare_feed-actor-name)\n\nResearch Scientist at Colfax International \n\n4w \n\nI'm happy to share the final part of our three part series on writing optimized GEMM kernels for NVIDIA GPUs using CUTLASS library abstractions. This last installment explains the Stream-K algorithm for scheduling work over threadblocks and how it surmounts the problem of wave quantization. To explain the problem, consider a naive \"data-parallel\" approach to GEMM that partitions the output matrix as a grid of tiles and assigns tiles to threadblocks one-to-one. A discrete number of waves will be launched to process the computation, quantized according to the total number of streaming multiprocessors (SMs). If the number of tiles is then not evenly divisible by the number of SMs, the last tail wave will process a reduced number of work tiles while still taking the time of a full wave, severely degrading overall TFLOPS performance. Roughly speaking, Stream-K addresses this issue by introducing parallelism along the inner K-dimension and enabling threadblocks to \"fractionally\" process output tiles, rebalancing the distribution of total work across the SMs. Our tutorial also describes CUTLASS's tile scheduler abstraction, which we leverage to effectively implement scheduling strategies like Stream-K. This separates scheduling optimizations that work at the grid level from those that target the inner load and compute loops, like the pipelining and warp specialization strategies we discussed in part 2 of this series. We both implement a simplified version of the Stream-K tile scheduler for a custom GEMM kernel and discuss the inner workings of CUTLASS's more advanced version. Work done in collaboration with my wonderful colleagues at Colfax Research! Happy holidays! [https://lnkd.in/gZkRqYeN](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2FgZkRqYeN&urlhash=DQnO&trk=public_post_reshare-text)\n\n## [ CUTLASS Tutorial: Persistent Kernels and Stream-K  https://research.colfax-intl.com  ](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Fresearch%2Ecolfax-intl%2Ecom%2Fcutlass-tutorial-persistent-kernels-and-stream-k%2F&urlhash=tx4R&trk=public_post_reshare_feed-article-content)\n\n[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmarek-bar%25C3%25A1k-31977a55_cutlass-tutorial-persistent-kernels-and-activity-7276513592210821120-tuTT&trk=public_post_like-cta) [ Comment  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmarek-bar%25C3%25A1k-31977a55_cutlass-tutorial-persistent-kernels-and-activity-7276513592210821120-tuTT&trk=public_post_comment-cta)\n\nShare \n    * Copy\n    * LinkedIn\n    * Facebook\n    * Twitter\n\nTo view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmarek-bar%25C3%25A1k-31977a55_cutlass-tutorial-persistent-kernels-and-activity-7276513592210821120-tuTT&trk=public_post_feed-cta-banner-cta)\n\n\n\n\n![](https://media.licdn.com/dms/image/v2/D4D16AQGKG5t7j7VjNw/profile-displaybackgroundimage-shrink_200_800/profile-displaybackgroundimage-shrink_200_800/0/1677477533507?e=2147483647&v=beta&t=BEOsmY67kMP7Ai1g8vbKMu1EV3nNFMPPnIGGO4aHJJU)\n\n95,683 followers \n\n  * [ 3000+ Posts ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fin%2Fashishpatel2604%2Frecent-activity%2F&trk=public_post_follow-posts)\n  * [ 32 Articles ](https://www.linkedin.com/today/author/ashishpatel2604?trk=public_post_follow-articles)\n\n\n\n[ View Profile ](https://in.linkedin.com/in/ashishpatel2604?trk=public_post_follow-view-profile) [ Follow ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Ffeed%2Fupdate%2Furn%3Ali%3Aactivity%3A7257270599444512769&trk=public_post_follow)\n\n##  More from this author \n\n  * ### [ Memory in LLMs Cuts Training Time by 30%â€”And Hereâ€™s What That Means for AI Agent Development  Ashish Patel ğŸ‡®ğŸ‡³  2w  ](https://www.linkedin.com/pulse/memory-llms-cuts-training-time-30and-heres-what-means-patel--hcqaf?trk=public_post)\n  * ### [ Over 62% of AI Teams Struggle with Model Deployment â€” PyTorchâ€™s New Features Solve This, Saving Millions on Development  Ashish Patel ğŸ‡®ğŸ‡³  2mo  ](https://www.linkedin.com/pulse/over-62-ai-teams-struggle-model-deployment-pytorchs-new-patel--nizrf?trk=public_post)\n  * ### [ Why Companies Deploying RAG-Powered AI on Kubernetes See a 3x Boost in Customer Personalization  Ashish Patel ğŸ‡®ğŸ‡³  2mo  ](https://www.linkedin.com/pulse/why-companies-deploying-rag-powered-ai-kubernetes-see-patel--qlduc?trk=public_post)\n\n\n\n##  Explore topics \n\n  * [ Sales ](https://www.linkedin.com/pulse/topics/sales-s5/)\n  * [ Marketing ](https://www.linkedin.com/pulse/topics/marketing-s2461/)\n  * [ IT Services ](https://www.linkedin.com/pulse/topics/it-services-s57547/)\n  * [ Business Administration ](https://www.linkedin.com/pulse/topics/business-administration-s50111/)\n  * [ HR Management ](https://www.linkedin.com/pulse/topics/hr-management-s50359/)\n  * [ Engineering ](https://www.linkedin.com/pulse/topics/engineering-s166/)\n  * [ Soft Skills ](https://www.linkedin.com/pulse/topics/soft-skills-s2976/)\n  * [ See All ](https://www.linkedin.com/pulse/topics/home/)\n\n\n\n  * LinkedIn Â© 2025\n  * [ About ](https://about.linkedin.com?trk=d_public_post_footer-about)\n  * [ Accessibility ](https://www.linkedin.com/accessibility?trk=d_public_post_footer-accessibility)\n  * [ User Agreement ](https://www.linkedin.com/legal/user-agreement?trk=d_public_post_footer-user-agreement)\n  * [ Privacy Policy ](https://www.linkedin.com/legal/privacy-policy?trk=d_public_post_footer-privacy-policy)\n  * [ Cookie Policy ](https://www.linkedin.com/legal/cookie-policy?trk=d_public_post_footer-cookie-policy)\n  * [ Copyright Policy ](https://www.linkedin.com/legal/copyright-policy?trk=d_public_post_footer-copyright-policy)\n  * [ Brand Policy ](https://brand.linkedin.com/policies?trk=d_public_post_footer-brand-policy)\n  * [ Guest Controls ](https://www.linkedin.com/psettings/guest-controls?trk=d_public_post_footer-guest-controls)\n  * [ Community Guidelines ](https://www.linkedin.com/legal/professional-community-policies?trk=d_public_post_footer-community-guide)\n  *     * Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© (Arabic) \n    * à¦¬à¦¾à¦‚à¦²à¦¾ (Bangla) \n    * ÄŒeÅ¡tina (Czech) \n    * Dansk (Danish) \n    * Deutsch (German) \n    * Î•Î»Î»Î·Î½Î¹ÎºÎ¬ (Greek) \n    * **English (English)**\n    * EspaÃ±ol (Spanish) \n    * ÙØ§Ø±Ø³ÛŒ (Persian) \n    * Suomi (Finnish) \n    * FranÃ§ais (French) \n    * à¤¹à¤¿à¤‚à¤¦à¥€ (Hindi) \n    * Magyar (Hungarian) \n    * Bahasa Indonesia (Indonesian) \n    * Italiano (Italian) \n    * ×¢×‘×¨×™×ª (Hebrew) \n    * æ—¥æœ¬èª (Japanese) \n    * í•œêµ­ì–´ (Korean) \n    * à¤®à¤°à¤¾à¤ à¥€ (Marathi) \n    * Bahasa Malaysia (Malay) \n    * Nederlands (Dutch) \n    * Norsk (Norwegian) \n    * à¨ªà©°à¨œà¨¾à¨¬à©€ (Punjabi) \n    * Polski (Polish) \n    * PortuguÃªs (Portuguese) \n    * RomÃ¢nÄƒ (Romanian) \n    * Ğ ÑƒÑÑĞºĞ¸Ğ¹ (Russian) \n    * Svenska (Swedish) \n    * à°¤à±†à°²à±à°—à± (Telugu) \n    * à¸ à¸²à¸©à¸²à¹„à¸—à¸¢ (Thai) \n    * Tagalog (Tagalog) \n    * TÃ¼rkÃ§e (Turkish) \n    * Ğ£ĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ° (Ukrainian) \n    * Tiáº¿ng Viá»‡t (Vietnamese) \n    * ç®€ä½“ä¸­æ–‡ (Chinese (Simplified)) \n    * æ­£é«”ä¸­æ–‡ (Chinese (Traditional)) \nLanguage \n\n\n\n\nLinkedIn \n\nNever miss a beat on the app \n\nDonâ€™t have the app? Get it in the Microsoft Store. \n\n[ Open the app ](ms-windows-store://pdp/?ProductId=9WZDNCRFJ4Q7&mode=mini&cid=guest_desktop_upsell_post2)\n\n##  Sign in to view more content \n\nCreate your free account or sign in to continue your search \n\nSign in \n\n##  Welcome back \n\nEmail or phone \n\nPassword \n\nShow\n\n[Forgot password?](https://www.linkedin.com/uas/request-password-reset?trk=public_post_contextual-sign-in-modal_sign-in-modal_forgot_password) Sign in \n\nor \n\nBy clicking Continue to join or sign in, you agree to LinkedInâ€™s [User Agreement](/legal/user-agreement?trk=public_post_contextual-sign-in-modal_sign-in-modal_auth-button_user-agreement), [Privacy Policy](/legal/privacy-policy?trk=public_post_contextual-sign-in-modal_sign-in-modal_auth-button_privacy-policy), and [Cookie Policy](/legal/cookie-policy?trk=public_post_contextual-sign-in-modal_sign-in-modal_auth-button_cookie-policy). \n\nNew to LinkedIn? [Join now](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_contextual-sign-in-modal_sign-in-modal_join-link)\n\nor \n\nNew to LinkedIn? [Join now](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_contextual-sign-in-modal_join-link)\n\nBy clicking Continue to join or sign in, you agree to LinkedInâ€™s [User Agreement](/legal/user-agreement?trk=linkedin-tc_auth-button_user-agreement), [Privacy Policy](/legal/privacy-policy?trk=linkedin-tc_auth-button_privacy-policy), and [Cookie Policy](/legal/cookie-policy?trk=linkedin-tc_auth-button_cookie-policy). \n",
    "content_quality_score": 0.6,
    "summary": null,
    "child_urls": [
        "https://www.linkedin.com/legal/cookie-policy",
        "https://www.linkedin.com/mypreferences/g/guest-cookies",
        "https://www.linkedin.com/legal/user-agreement?trk=linkedin-tc_auth-button_user-agreement",
        "https://www.linkedin.com/legal/privacy-policy?trk=linkedin-tc_auth-button_privacy-policy",
        "https://www.linkedin.com/legal/cookie-policy?trk=linkedin-tc_auth-button_cookie-policy",
        "https://www.linkedin.com/posts/ashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW?utm_source=share&utm_medium=member_desktop/#main-content",
        "https://www.linkedin.com/?trk=public_post_nav-header-logo",
        "https://www.linkedin.com/pulse/topics/home/?trk=public_post_guest_nav_menu_articles",
        "https://www.linkedin.com/pub/dir/+/+?trk=public_post_guest_nav_menu_people",
        "https://www.linkedin.com/learning/search?trk=public_post_guest_nav_menu_learning",
        "https://www.linkedin.com/jobs/search?trk=public_post_guest_nav_menu_jobs",
        "https://www.linkedin.com/games?trk=public_post_guest_nav_menu_games",
        "ms-windows-store://pdp/?ProductId=9WZDNCRFJ4Q7&mode=mini&cid=guest_nav_upsell&trk=public_post_guest_nav_menu_windows",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_nav-header-join",
        "https://www.linkedin.com/login?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&fromSignIn=true&trk=public_post_nav-header-signin",
        "https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting",
        "https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2FdWvgWvXM&urlhash=Tvqa&trk=public_post-text",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fllms&trk=public_post-text",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fdatascience&trk=public_post-text",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fartificialintelligence&trk=public_post-text",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_social-actions-reactions",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_social-actions-comments",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_like-cta",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment-cta",
        "https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_like",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_reply",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_reactions",
        "https://www.linkedin.com/in/madwesh?trk=public_post_comment_actor-image",
        "https://www.linkedin.com/in/madwesh?trk=public_post_comment_actor-name",
        "https://www.linkedin.com/in/visinha?trk=public_post_comment_actor-image",
        "https://www.linkedin.com/in/visinha?trk=public_post_comment_actor-name",
        "https://www.linkedin.com/company/oktaneai?trk=public_post_comment_actor-image",
        "https://www.linkedin.com/company/oktaneai?trk=public_post_comment_actor-name",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_see-more-comments",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_feed-cta-banner-cta",
        "https://www.linkedin.com/posts/daemonbehr_genai-aionnutanix-nutanix-activity-7259202801782681600-LAt3",
        "https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdaemonbehr_genai-aionnutanix-nutanix-activity-7259202801782681600-LAt3&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting",
        "https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2Fej6g_b5E&urlhash=4gHz&trk=public_post-text",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fgenai&trk=public_post-text",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Faionnutanix&trk=public_post-text",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fnutanix&trk=public_post-text",
        "https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2FdWvgWvXM&urlhash=Tvqa&trk=public_post_reshare-text",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fllms&trk=public_post_reshare-text",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fdatascience&trk=public_post_reshare-text",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fartificialintelligence&trk=public_post_reshare-text",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdaemonbehr_genai-aionnutanix-nutanix-activity-7259202801782681600-LAt3&trk=public_post_social-actions-reactions",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdaemonbehr_genai-aionnutanix-nutanix-activity-7259202801782681600-LAt3&trk=public_post_social-actions-comments",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdaemonbehr_genai-aionnutanix-nutanix-activity-7259202801782681600-LAt3&trk=public_post_like-cta",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdaemonbehr_genai-aionnutanix-nutanix-activity-7259202801782681600-LAt3&trk=public_post_comment-cta",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdaemonbehr_genai-aionnutanix-nutanix-activity-7259202801782681600-LAt3&trk=public_post_feed-cta-banner-cta",
        "https://www.linkedin.com/posts/ganeshjagadeesan_llms-artificialintelligence-datascience-activity-7257283675287953409-NxTr",
        "https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fganeshjagadeesan_llms-artificialintelligence-datascience-activity-7257283675287953409-NxTr&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fmachinelearning&trk=public_post-text",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fgpu&trk=public_post-text",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fmodeltraining&trk=public_post-text",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Faiinsights&trk=public_post-text",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fdeeplearning&trk=public_post-text",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Faioptimization&trk=public_post-text",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fganeshjagadeesan_llms-artificialintelligence-datascience-activity-7257283675287953409-NxTr&trk=public_post_social-actions-reactions",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fganeshjagadeesan_llms-artificialintelligence-datascience-activity-7257283675287953409-NxTr&trk=public_post_like-cta",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fganeshjagadeesan_llms-artificialintelligence-datascience-activity-7257283675287953409-NxTr&trk=public_post_comment-cta",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fganeshjagadeesan_llms-artificialintelligence-datascience-activity-7257283675287953409-NxTr&trk=public_post_feed-cta-banner-cta",
        "https://www.linkedin.com/posts/dylan-poh_the-post-sheds-light-on-the-staggering-costs-activity-7258089394434170880-Y08V",
        "https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdylan-poh_the-post-sheds-light-on-the-staggering-costs-activity-7258089394434170880-Y08V&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdylan-poh_the-post-sheds-light-on-the-staggering-costs-activity-7258089394434170880-Y08V&trk=public_post_social-actions-reactions",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdylan-poh_the-post-sheds-light-on-the-staggering-costs-activity-7258089394434170880-Y08V&trk=public_post_like-cta",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdylan-poh_the-post-sheds-light-on-the-staggering-costs-activity-7258089394434170880-Y08V&trk=public_post_comment-cta",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdylan-poh_the-post-sheds-light-on-the-staggering-costs-activity-7258089394434170880-Y08V&trk=public_post_feed-cta-banner-cta",
        "https://www.linkedin.com/posts/mazadgole_back-of-the-envelope-calculation-for-gpu-activity-7209639861891317761-VmX7",
        "https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmazadgole_back-of-the-envelope-calculation-for-gpu-activity-7209639861891317761-VmX7&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fgpus&trk=public_post-text",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fquantization&trk=public_post-text",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Flora&trk=public_post-text",
        "https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Fmabbasiazad%2Egithub%2Eio%2Fportfolio%2Fposts%2Fllms_gpu%2F&urlhash=AWz1&trk=public_post_feed-article-content",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmazadgole_back-of-the-envelope-calculation-for-gpu-activity-7209639861891317761-VmX7&trk=public_post_social-actions-reactions",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmazadgole_back-of-the-envelope-calculation-for-gpu-activity-7209639861891317761-VmX7&trk=public_post_like-cta",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmazadgole_back-of-the-envelope-calculation-for-gpu-activity-7209639861891317761-VmX7&trk=public_post_comment-cta",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmazadgole_back-of-the-envelope-calculation-for-gpu-activity-7209639861891317761-VmX7&trk=public_post_feed-cta-banner-cta",
        "https://www.linkedin.com/posts/avi-chawla_how-to-synchronize-gpus-with-multi-gpu-training-activity-7222547319672463360-uXQ-",
        "https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Favi-chawla_how-to-synchronize-gpus-with-multi-gpu-training-activity-7222547319672463360-uXQ-&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting",
        "https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2Fg56-7HsZ&urlhash=JNnU&trk=public_post-text",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Favi-chawla_how-to-synchronize-gpus-with-multi-gpu-training-activity-7222547319672463360-uXQ-&trk=public_post_social-actions-reactions",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Favi-chawla_how-to-synchronize-gpus-with-multi-gpu-training-activity-7222547319672463360-uXQ-&trk=public_post_social-actions-comments",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Favi-chawla_how-to-synchronize-gpus-with-multi-gpu-training-activity-7222547319672463360-uXQ-&trk=public_post_like-cta",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Favi-chawla_how-to-synchronize-gpus-with-multi-gpu-training-activity-7222547319672463360-uXQ-&trk=public_post_comment-cta",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Favi-chawla_how-to-synchronize-gpus-with-multi-gpu-training-activity-7222547319672463360-uXQ-&trk=public_post_feed-cta-banner-cta",
        "https://www.linkedin.com/posts/benjamin-j-todd_how-much-ai-inference-can-we-do-activity-7195268236697915393-SFAH",
        "https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fbenjamin-j-todd_how-much-ai-inference-can-we-do-activity-7195268236697915393-SFAH&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting",
        "https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2Fe5DNbZDK&urlhash=8KaZ&trk=public_post-text",
        "https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Fbenjamintodd%2Esubstack%2Ecom%2Fp%2Fhow-much-ai-inference-can-we-do&urlhash=QrZ1&trk=public_post_feed-article-content",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fbenjamin-j-todd_how-much-ai-inference-can-we-do-activity-7195268236697915393-SFAH&trk=public_post_social-actions-reactions",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fbenjamin-j-todd_how-much-ai-inference-can-we-do-activity-7195268236697915393-SFAH&trk=public_post_like-cta",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fbenjamin-j-todd_how-much-ai-inference-can-we-do-activity-7195268236697915393-SFAH&trk=public_post_comment-cta",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fbenjamin-j-todd_how-much-ai-inference-can-we-do-activity-7195268236697915393-SFAH&trk=public_post_feed-cta-banner-cta",
        "https://www.linkedin.com/posts/martechrichard_training-ai-models-on-cpu-activity-7236981179684466688-zQ9z",
        "https://www.linkedin.com/company/martechrichard?trk=public_post_feed-actor-image",
        "https://www.linkedin.com/company/martechrichard?trk=public_post_feed-actor-name",
        "https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmartechrichard_training-ai-models-on-cpu-activity-7236981179684466688-zQ9z&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting",
        "https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2FeB5w7N8U&urlhash=0hjW&trk=public_post-text",
        "https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2Fe9sTptsu&urlhash=QLXV&trk=public_post-text",
        "https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Ftowardsdatascience%2Ecom%2Ftraining-ai-models-on-cpu-3903adc9f388&urlhash=gBHg&trk=public_post_feed-article-content",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmartechrichard_training-ai-models-on-cpu-activity-7236981179684466688-zQ9z&trk=public_post_like-cta",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmartechrichard_training-ai-models-on-cpu-activity-7236981179684466688-zQ9z&trk=public_post_comment-cta",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmartechrichard_training-ai-models-on-cpu-activity-7236981179684466688-zQ9z&trk=public_post_feed-cta-banner-cta",
        "https://www.linkedin.com/posts/jay-shah-335689167_cutlass-tutorial-persistent-kernels-and-activity-7276341827874562048-Fxzm",
        "https://www.linkedin.com/in/jay-shah-335689167?trk=public_post_feed-actor-image",
        "https://www.linkedin.com/in/jay-shah-335689167?trk=public_post_feed-actor-name",
        "https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fjay-shah-335689167_cutlass-tutorial-persistent-kernels-and-activity-7276341827874562048-Fxzm&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting",
        "https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2FgZkRqYeN&urlhash=DQnO&trk=public_post-text",
        "https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Fresearch%2Ecolfax-intl%2Ecom%2Fcutlass-tutorial-persistent-kernels-and-stream-k%2F&urlhash=tx4R&trk=public_post_feed-article-content",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fjay-shah-335689167_cutlass-tutorial-persistent-kernels-and-activity-7276341827874562048-Fxzm&trk=public_post_social-actions-reactions",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fjay-shah-335689167_cutlass-tutorial-persistent-kernels-and-activity-7276341827874562048-Fxzm&trk=public_post_social-actions-comments",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fjay-shah-335689167_cutlass-tutorial-persistent-kernels-and-activity-7276341827874562048-Fxzm&trk=public_post_like-cta",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fjay-shah-335689167_cutlass-tutorial-persistent-kernels-and-activity-7276341827874562048-Fxzm&trk=public_post_comment-cta",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fjay-shah-335689167_cutlass-tutorial-persistent-kernels-and-activity-7276341827874562048-Fxzm&trk=public_post_feed-cta-banner-cta",
        "https://www.linkedin.com/posts/rajat-walia_mechanicalengineering-mechanical-aerospace-activity-7205433434528256000-dyOJ",
        "https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Frajat-walia_mechanicalengineering-mechanical-aerospace-activity-7205433434528256000-dyOJ&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fmechanicalengineering&trk=public_post-text",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fmechanical&trk=public_post-text",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Faerospace&trk=public_post-text",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fautomotive&trk=public_post-text",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fcfd&trk=public_post-text",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Frajat-walia_mechanicalengineering-mechanical-aerospace-activity-7205433434528256000-dyOJ&trk=public_post_social-actions-reactions",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Frajat-walia_mechanicalengineering-mechanical-aerospace-activity-7205433434528256000-dyOJ&trk=public_post_like-cta",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Frajat-walia_mechanicalengineering-mechanical-aerospace-activity-7205433434528256000-dyOJ&trk=public_post_comment-cta",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Frajat-walia_mechanicalengineering-mechanical-aerospace-activity-7205433434528256000-dyOJ&trk=public_post_feed-cta-banner-cta",
        "https://www.linkedin.com/posts/marek-bar%C3%A1k-31977a55_cutlass-tutorial-persistent-kernels-and-activity-7276513592210821120-tuTT",
        "https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmarek-bar%25C3%25A1k-31977a55_cutlass-tutorial-persistent-kernels-and-activity-7276513592210821120-tuTT&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fnvidia&trk=public_post-text",
        "https://www.linkedin.com/in/jay-shah-335689167?trk=public_post_reshare_feed-actor-image",
        "https://www.linkedin.com/in/jay-shah-335689167?trk=public_post_reshare_feed-actor-name",
        "https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2FgZkRqYeN&urlhash=DQnO&trk=public_post_reshare-text",
        "https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Fresearch%2Ecolfax-intl%2Ecom%2Fcutlass-tutorial-persistent-kernels-and-stream-k%2F&urlhash=tx4R&trk=public_post_reshare_feed-article-content",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmarek-bar%25C3%25A1k-31977a55_cutlass-tutorial-persistent-kernels-and-activity-7276513592210821120-tuTT&trk=public_post_like-cta",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmarek-bar%25C3%25A1k-31977a55_cutlass-tutorial-persistent-kernels-and-activity-7276513592210821120-tuTT&trk=public_post_comment-cta",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmarek-bar%25C3%25A1k-31977a55_cutlass-tutorial-persistent-kernels-and-activity-7276513592210821120-tuTT&trk=public_post_feed-cta-banner-cta",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fin%2Fashishpatel2604%2Frecent-activity%2F&trk=public_post_follow-posts",
        "https://www.linkedin.com/today/author/ashishpatel2604?trk=public_post_follow-articles",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Ffeed%2Fupdate%2Furn%3Ali%3Aactivity%3A7257270599444512769&trk=public_post_follow",
        "https://www.linkedin.com/pulse/memory-llms-cuts-training-time-30and-heres-what-means-patel--hcqaf?trk=public_post",
        "https://www.linkedin.com/pulse/over-62-ai-teams-struggle-model-deployment-pytorchs-new-patel--nizrf?trk=public_post",
        "https://www.linkedin.com/pulse/why-companies-deploying-rag-powered-ai-kubernetes-see-patel--qlduc?trk=public_post",
        "https://www.linkedin.com/pulse/topics/sales-s5/",
        "https://www.linkedin.com/pulse/topics/marketing-s2461/",
        "https://www.linkedin.com/pulse/topics/it-services-s57547/",
        "https://www.linkedin.com/pulse/topics/business-administration-s50111/",
        "https://www.linkedin.com/pulse/topics/hr-management-s50359/",
        "https://www.linkedin.com/pulse/topics/engineering-s166/",
        "https://www.linkedin.com/pulse/topics/soft-skills-s2976/",
        "https://www.linkedin.com/pulse/topics/home/",
        "https://www.linkedin.com/accessibility?trk=d_public_post_footer-accessibility",
        "https://www.linkedin.com/legal/user-agreement?trk=d_public_post_footer-user-agreement",
        "https://www.linkedin.com/legal/privacy-policy?trk=d_public_post_footer-privacy-policy",
        "https://www.linkedin.com/legal/cookie-policy?trk=d_public_post_footer-cookie-policy",
        "https://www.linkedin.com/legal/copyright-policy?trk=d_public_post_footer-copyright-policy",
        "https://www.linkedin.com/psettings/guest-controls?trk=d_public_post_footer-guest-controls",
        "https://www.linkedin.com/legal/professional-community-policies?trk=d_public_post_footer-community-guide",
        "ms-windows-store://pdp/?ProductId=9WZDNCRFJ4Q7&mode=mini&cid=guest_desktop_upsell_post2",
        "https://www.linkedin.com/uas/request-password-reset?trk=public_post_contextual-sign-in-modal_sign-in-modal_forgot_password",
        "https://www.linkedin.com/legal/user-agreement?trk=public_post_contextual-sign-in-modal_sign-in-modal_auth-button_user-agreement",
        "https://www.linkedin.com/legal/privacy-policy?trk=public_post_contextual-sign-in-modal_sign-in-modal_auth-button_privacy-policy",
        "https://www.linkedin.com/legal/cookie-policy?trk=public_post_contextual-sign-in-modal_sign-in-modal_auth-button_cookie-policy",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_contextual-sign-in-modal_sign-in-modal_join-link",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_contextual-sign-in-modal_join-link",
        "https://in.linkedin.com/in/ashishpatel2604?trk=public_post_feed-actor-image",
        "https://in.linkedin.com/in/ashishpatel2604?trk=public_post_feed-actor-name",
        "https://at.linkedin.com/in/stefan-komornyik?trk=public_post_comment_actor-image",
        "https://at.linkedin.com/in/stefan-komornyik?trk=public_post_comment_actor-name",
        "https://ch.linkedin.com/in/isabella-kosch-zh?trk=public_post_comment_actor-image",
        "https://ch.linkedin.com/in/isabella-kosch-zh?trk=public_post_comment_actor-name",
        "https://it.linkedin.com/in/danielebaranzini?trk=public_post_comment_actor-image",
        "https://it.linkedin.com/in/danielebaranzini?trk=public_post_comment_actor-name",
        "https://il.linkedin.com/in/peleg-zborovsky?trk=public_post_comment_actor-image",
        "https://il.linkedin.com/in/peleg-zborovsky?trk=public_post_comment_actor-name",
        "https://il.linkedin.com/in/itzhak-hirschman-b16269193?trk=public_post_comment-text",
        "https://il.linkedin.com/in/dor-getter-9707711b1?trk=public_post_comment-text",
        "https://il.linkedin.com/in/zeev-kaminsky-024690150?trk=public_post_comment-text",
        "https://se.linkedin.com/in/jerry-%C3%A4r-p-189438188?trk=public_post_comment_actor-image",
        "https://se.linkedin.com/in/jerry-%C3%A4r-p-189438188?trk=public_post_comment_actor-name",
        "https://in.linkedin.com/in/ashishpatel2604?trk=public_post_comment-text",
        "https://ro.linkedin.com/in/pauliusztin?trk=public_post_comment_actor-image",
        "https://ro.linkedin.com/in/pauliusztin?trk=public_post_comment_actor-name",
        "https://ca.linkedin.com/in/daemonbehr?trk=public_post_feed-actor-image",
        "https://ca.linkedin.com/in/daemonbehr?trk=public_post_feed-actor-name",
        "https://in.linkedin.com/in/ashishpatel2604?trk=public_post_reshare_feed-actor-image",
        "https://in.linkedin.com/in/ashishpatel2604?trk=public_post_reshare_feed-actor-name",
        "https://in.linkedin.com/in/ganeshjagadeesan?trk=public_post_feed-actor-image",
        "https://in.linkedin.com/in/ganeshjagadeesan?trk=public_post_feed-actor-name",
        "https://sg.linkedin.com/in/dylan-poh?trk=public_post_feed-actor-image",
        "https://sg.linkedin.com/in/dylan-poh?trk=public_post_feed-actor-name",
        "https://ca.linkedin.com/in/mazadgole?trk=public_post_feed-actor-image",
        "https://ca.linkedin.com/in/mazadgole?trk=public_post_feed-actor-name",
        "https://in.linkedin.com/in/avi-chawla?trk=public_post_feed-actor-image",
        "https://in.linkedin.com/in/avi-chawla?trk=public_post_feed-actor-name",
        "https://in.linkedin.com/company/daily-dose-of-ds?trk=public_post-text",
        "https://uk.linkedin.com/in/benjamin-j-todd?trk=public_post_feed-actor-image",
        "https://uk.linkedin.com/in/benjamin-j-todd?trk=public_post_feed-actor-name",
        "https://in.linkedin.com/in/rajat-walia?trk=public_post_feed-actor-image",
        "https://in.linkedin.com/in/rajat-walia?trk=public_post_feed-actor-name",
        "https://at.linkedin.com/in/marek-bar%C3%A1k-31977a55?trk=public_post_feed-actor-image",
        "https://at.linkedin.com/in/marek-bar%C3%A1k-31977a55?trk=public_post_feed-actor-name",
        "https://in.linkedin.com/in/ashishpatel2604?trk=public_post_follow-view-profile",
        "https://about.linkedin.com?trk=d_public_post_footer-about",
        "https://brand.linkedin.com/policies?trk=d_public_post_footer-brand-policy"
    ]
}
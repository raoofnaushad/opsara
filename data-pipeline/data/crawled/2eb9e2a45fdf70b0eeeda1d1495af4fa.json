{
    "id": "2eb9e2a45fdf70b0eeeda1d1495af4fa",
    "metadata": {
        "id": "2eb9e2a45fdf70b0eeeda1d1495af4fa",
        "url": "https://huggingface.co/docs/transformers/en/chat_templating#what-template-should-i-use/",
        "title": "Chat Templates",
        "properties": {
            "description": "We‚Äôre on a journey to advance and democratize artificial intelligence through open source and open science.",
            "keywords": null,
            "author": null,
            "og:title": "Chat Templates",
            "og:type": "website",
            "og:url": "https://huggingface.co/docs/transformers/en/chat_templating",
            "og:image": "https://huggingface.co/front/thumbnails/docs/transformers.png",
            "twitter:card": "summary_large_image",
            "twitter:site": "@huggingface",
            "twitter:image": "https://huggingface.co/front/thumbnails/docs/transformers.png"
        }
    },
    "parent_metadata": {
        "id": "e890e88e6c346cc9e1e0fffdfa867cac",
        "url": "https://www.notion.so/Training-Fine-tuning-LLMs-e890e88e6c346cc9e1e0fffdfa867cac",
        "title": "Training & Fine-tuning LLMs",
        "properties": {
            "Type": "Leaf"
        }
    },
    "content": "[![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg) Hugging Face](/)\n\n  * [ Models](/models)\n  * [ Datasets](/datasets)\n  * [ Spaces](/spaces)\n  * [ Posts](/posts)\n  * [ Docs](/docs)\n  * [ Enterprise](/enterprise)\n  * [Pricing](/pricing)\n  * [Log In](/login)\n  * [Sign Up](/join)\n\n\n\nTransformers documentation\n\nChat Templates\n\n# Transformers\n\nüè° View all docsAWS Trainium & InferentiaAccelerateAmazon SageMakerArgillaAutoTrainBitsandbytesChat UICompetitionsDataset viewerDatasetsDiffusersDistilabelEvaluateGoogle CloudGoogle TPUsGradioHubHub Python LibraryHugging Face Generative AI Services (HUGS)Huggingface.jsInference API (serverless)Inference Endpoints (dedicated)LeaderboardsLightevalOptimumPEFTSafetensorsSentence TransformersTRLTasksText Embeddings InferenceText Generation InferenceTokenizersTransformersTransformers.jssmolagentstimm\n\nSearch documentation\n\n`‚åòK`\n\nmainv4.48.0v4.47.1v4.46.3v4.45.2v4.44.2v4.43.4v4.42.4v4.41.2v4.40.2v4.39.3v4.38.2v4.37.2v4.36.1v4.35.2v4.34.1v4.33.3v4.32.1v4.31.0v4.30.0v4.29.1v4.28.1v4.27.2v4.26.1v4.25.1v4.24.0v4.23.1v4.22.2v4.21.3v4.20.1v4.19.4v4.18.0v4.17.0v4.16.2v4.15.0v4.14.1v4.13.0v4.12.5v4.11.3v4.10.1v4.9.2v4.8.2v4.7.0v4.6.0v4.5.1v4.4.2v4.3.3v4.2.2v4.1.1v4.0.1v3.5.1v3.4.0v3.3.1v3.2.0v3.1.0v3.0.2v2.11.0v2.10.0v2.9.1v2.8.0v2.7.0v2.6.0v2.5.1v2.4.1v2.3.0v2.2.2v2.1.1v2.0.0v1.2.0v1.1.0v1.0.0doc-builder-html ARDEENESFRHIITJAKOPTTETRZH [ 137,863](https://github.com/huggingface/transformers)\n\nGet started\n\n[ü§ó Transformers ](/docs/transformers/en/index)[Quick tour ](/docs/transformers/en/quicktour)[Installation ](/docs/transformers/en/installation)[Adding a new model to `transformers` ](/docs/transformers/en/add_new_model)\n\nTutorials\n\n[Run inference with pipelines ](/docs/transformers/en/pipeline_tutorial)[Write portable code with AutoClass ](/docs/transformers/en/autoclass_tutorial)[Preprocess data ](/docs/transformers/en/preprocessing)[Fine-tune a pretrained model ](/docs/transformers/en/training)[Train with a script ](/docs/transformers/en/run_scripts)[Set up distributed training with ü§ó Accelerate ](/docs/transformers/en/accelerate)[Load and train adapters with ü§ó PEFT ](/docs/transformers/en/peft)[Share your model ](/docs/transformers/en/model_sharing)[Agents 101 ](/docs/transformers/en/agents)[Agents, supercharged - Multi-agents, External tools, and more ](/docs/transformers/en/agents_advanced)[Generation with LLMs ](/docs/transformers/en/llm_tutorial)[Chatting with Transformers ](/docs/transformers/en/conversations)\n\nTask Guides\n\nNatural Language Processing\n\nAudio\n\nComputer Vision\n\nMultimodal\n\nGeneration\n\nPrompting\n\nDeveloper guides\n\n[Use fast tokenizers from ü§ó Tokenizers ](/docs/transformers/en/fast_tokenizers)[Run inference with multilingual models ](/docs/transformers/en/multilingual)[Use model-specific APIs ](/docs/transformers/en/create_a_model)[Share a custom model ](/docs/transformers/en/custom_models)[Chat templates ](/docs/transformers/en/chat_templating)[Trainer ](/docs/transformers/en/trainer)[Run training on Amazon SageMaker ](/docs/transformers/en/sagemaker)[Export to ONNX ](/docs/transformers/en/serialization)[Export to TFLite ](/docs/transformers/en/tflite)[Export to TorchScript ](/docs/transformers/en/torchscript)[Benchmarks ](/docs/transformers/en/benchmarks)[Notebooks with examples ](/docs/transformers/en/notebooks)[Community resources ](/docs/transformers/en/community)[Troubleshoot ](/docs/transformers/en/troubleshooting)[Interoperability with GGUF files ](/docs/transformers/en/gguf)[Interoperability with TikToken files ](/docs/transformers/en/tiktoken)[Modularity in `transformers` ](/docs/transformers/en/modular_transformers)[Model Hacking (overwriting a class to your usage) ](/docs/transformers/en/how_to_hack_models)\n\nQuantization Methods\n\n[Getting started ](/docs/transformers/en/quantization/overview)[bitsandbytes ](/docs/transformers/en/quantization/bitsandbytes)[GPTQ ](/docs/transformers/en/quantization/gptq)[AWQ ](/docs/transformers/en/quantization/awq)[AQLM ](/docs/transformers/en/quantization/aqlm)[VPTQ ](/docs/transformers/en/quantization/vptq)[Quanto ](/docs/transformers/en/quantization/quanto)[EETQ ](/docs/transformers/en/quantization/eetq)[HIGGS ](/docs/transformers/en/quantization/higgs)[HQQ ](/docs/transformers/en/quantization/hqq)[FBGEMM_FP8 ](/docs/transformers/en/quantization/fbgemm_fp8)[Optimum ](/docs/transformers/en/quantization/optimum)[TorchAO ](/docs/transformers/en/quantization/torchao)[BitNet ](/docs/transformers/en/quantization/bitnet)[compressed-tensors ](/docs/transformers/en/quantization/compressed_tensors)[Contribute new quantization method ](/docs/transformers/en/quantization/contribute)\n\nPerformance and scalability\n\n[Overview ](/docs/transformers/en/performance)[LLM inference optimization ](/docs/transformers/en/llm_optims)\n\nEfficient training techniques\n\n[Methods and tools for efficient training on a single GPU ](/docs/transformers/en/perf_train_gpu_one)[Multiple GPUs and parallelism ](/docs/transformers/en/perf_train_gpu_many)[Fully Sharded Data Parallel ](/docs/transformers/en/fsdp)[DeepSpeed ](/docs/transformers/en/deepspeed)[Efficient training on CPU ](/docs/transformers/en/perf_train_cpu)[Distributed CPU training ](/docs/transformers/en/perf_train_cpu_many)[Training on TPU with TensorFlow ](/docs/transformers/en/perf_train_tpu_tf)[PyTorch training on Apple silicon ](/docs/transformers/en/perf_train_special)[Custom hardware for training ](/docs/transformers/en/perf_hardware)[Hyperparameter Search using Trainer API ](/docs/transformers/en/hpo_train)\n\nOptimizing inference\n\n[CPU inference ](/docs/transformers/en/perf_infer_cpu)[GPU inference ](/docs/transformers/en/perf_infer_gpu_one)[Multi-GPU inference ](/docs/transformers/en/perf_infer_gpu_multi)\n\n[Instantiate a big model ](/docs/transformers/en/big_models)[Debugging ](/docs/transformers/en/debugging)[XLA Integration for TensorFlow Models ](/docs/transformers/en/tf_xla)[Optimize inference using `torch.compile()` ](/docs/transformers/en/perf_torch_compile)\n\nContribute\n\n[How to contribute to ü§ó Transformers? ](/docs/transformers/en/contributing)[How to add a model to ü§ó Transformers? ](/docs/transformers/en/add_new_model)[How to add a pipeline to ü§ó Transformers? ](/docs/transformers/en/add_new_pipeline)[Testing ](/docs/transformers/en/testing)[Checks on a Pull Request ](/docs/transformers/en/pr_checks)\n\nConceptual guides\n\n[Philosophy ](/docs/transformers/en/philosophy)[Glossary ](/docs/transformers/en/glossary)[What ü§ó Transformers can do ](/docs/transformers/en/task_summary)[How ü§ó Transformers solve tasks ](/docs/transformers/en/tasks_explained)[The Transformer model family ](/docs/transformers/en/model_summary)[Summary of the tokenizers ](/docs/transformers/en/tokenizer_summary)[Attention mechanisms ](/docs/transformers/en/attention)[Padding and truncation ](/docs/transformers/en/pad_truncation)[BERTology ](/docs/transformers/en/bertology)[Perplexity of fixed-length models ](/docs/transformers/en/perplexity)[Pipelines for webserver inference ](/docs/transformers/en/pipeline_webserver)[Model training anatomy ](/docs/transformers/en/model_memory_anatomy)[Getting the most out of LLMs ](/docs/transformers/en/llm_tutorial_optimization)\n\nAPI\n\nMain Classes\n\n[Agents and Tools ](/docs/transformers/en/main_classes/agent)[Auto Classes ](/docs/transformers/en/model_doc/auto)[Backbones ](/docs/transformers/en/main_classes/backbones)[Callbacks ](/docs/transformers/en/main_classes/callback)[Configuration ](/docs/transformers/en/main_classes/configuration)[Data Collator ](/docs/transformers/en/main_classes/data_collator)[Keras callbacks ](/docs/transformers/en/main_classes/keras_callbacks)[Logging ](/docs/transformers/en/main_classes/logging)[Models ](/docs/transformers/en/main_classes/model)[Text Generation ](/docs/transformers/en/main_classes/text_generation)[ONNX ](/docs/transformers/en/main_classes/onnx)[Optimization ](/docs/transformers/en/main_classes/optimizer_schedules)[Model outputs ](/docs/transformers/en/main_classes/output)[Pipelines ](/docs/transformers/en/main_classes/pipelines)[Processors ](/docs/transformers/en/main_classes/processors)[Quantization ](/docs/transformers/en/main_classes/quantization)[Tokenizer ](/docs/transformers/en/main_classes/tokenizer)[Trainer ](/docs/transformers/en/main_classes/trainer)[DeepSpeed ](/docs/transformers/en/main_classes/deepspeed)[ExecuTorch ](/docs/transformers/en/main_classes/executorch)[Feature Extractor ](/docs/transformers/en/main_classes/feature_extractor)[Image Processor ](/docs/transformers/en/main_classes/image_processor)\n\nModels\n\nText models\n\nVision models\n\nAudio models\n\nVideo models\n\nMultimodal models\n\nReinforcement learning models\n\nTime series models\n\nGraph models\n\nInternal Helpers\n\n[Custom Layers and Utilities ](/docs/transformers/en/internal/modeling_utils)[Utilities for pipelines ](/docs/transformers/en/internal/pipelines_utils)[Utilities for Tokenizers ](/docs/transformers/en/internal/tokenization_utils)[Utilities for Trainer ](/docs/transformers/en/internal/trainer_utils)[Utilities for Generation ](/docs/transformers/en/internal/generation_utils)[Utilities for Image Processors ](/docs/transformers/en/internal/image_processing_utils)[Utilities for Audio processing ](/docs/transformers/en/internal/audio_utils)[General Utilities ](/docs/transformers/en/internal/file_utils)[Utilities for Time Series ](/docs/transformers/en/internal/time_series_utils)\n\n![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg)\n\nJoin the Hugging Face community\n\nand get access to the augmented documentation experience \n\nCollaborate on models, datasets and Spaces \n\nFaster examples with accelerated inference \n\nSwitch between documentation themes \n\n[Sign Up](/join)\n\nto get started\n\n# [](#chat-templates) Chat Templates\n\n## [](#introduction) Introduction\n\nAn increasingly common use case for LLMs is **chat**. In a chat context, rather than continuing a single string of text (as is the case with a standard language model), the model instead continues a conversation that consists of one or more **messages** , each of which includes a **role** , like ‚Äúuser‚Äù or ‚Äúassistant‚Äù, as well as message text.\n\nMuch like tokenization, different models expect very different input formats for chat. This is the reason we added **chat templates** as a feature. Chat templates are part of the tokenizer. They specify how to convert conversations, represented as lists of messages, into a single tokenizable string in the format that the model expects.\n\nLet‚Äôs make this concrete with a quick example using the `mistralai/Mistral-7B-Instruct-v0.1` model:\n\nCopied\n\n```\n>>> from transformers import AutoTokenizer >>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\") >>> chat = [ ...  {\"role\": \"user\", \"content\": \"Hello, how are you?\"}, ...  {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"}, ...  {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"}, ... ] >>> tokenizer.apply_chat_template(chat, tokenize=False) \"<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s> [INST] I'd like to show off how chat templating works! [/INST]\"\n```\n\nNotice how the tokenizer has added the control tokens [INST] and [/INST] to indicate the start and end of user messages (but not assistant messages!), and the entire chat is condensed into a single string. If we use `tokenize=True`, which is the default setting, that string will also be tokenized for us.\n\nNow, try the same code, but swap in the `HuggingFaceH4/zephyr-7b-beta` model instead, and you should get:\n\nCopied\n\n```\n<|user|> Hello, how are you?</s> <|assistant|> I'm doing great. How can I help you today?</s> <|user|> I'd like to show off how chat templating works!</s>\n```\n\nBoth Zephyr and Mistral-Instruct were fine-tuned from the same base model, `Mistral-7B-v0.1`. However, they were trained with totally different chat formats. Without chat templates, you would have to write manual formatting code for each model, and it‚Äôs very easy to make minor errors that hurt performance! Chat templates handle the details of formatting for you, allowing you to write universal code that works for any model.\n\n## [](#how-do-i-use-chat-templates) How do I use chat templates?\n\nAs you can see in the example above, chat templates are easy to use. Simply build a list of messages, with `role` and `content` keys, and then pass it to the [apply_chat_template()](/docs/transformers/v4.48.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template) method. Once you do that, you‚Äôll get output that‚Äôs ready to go! When using chat templates as input for model generation, it‚Äôs also a good idea to use `add_generation_prompt=True` to add a [generation prompt](#what-are-generation-prompts).\n\nHere‚Äôs an example of preparing input for `model.generate()`, using `Zephyr` again:\n\nCopied\n\n```\nfrom transformers import AutoModelForCausalLM, AutoTokenizer checkpoint = \"HuggingFaceH4/zephyr-7b-beta\" tokenizer = AutoTokenizer.from_pretrained(checkpoint) model = AutoModelForCausalLM.from_pretrained(checkpoint) # You may want to use bfloat16 and/or move to GPU here messages = [ { \"role\": \"system\", \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\", }, {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"}, ] tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\") print(tokenizer.decode(tokenized_chat[0]))\n```\n\nThis will yield a string in the input format that Zephyr expects.\n\nCopied\n\n```\n<|system|> You are a friendly chatbot who always responds in the style of a pirate</s> <|user|> How many helicopters can a human eat in one sitting?</s> <|assistant|>\n```\n\nNow that our input is formatted correctly for Zephyr, we can use the model to generate a response to the user‚Äôs question:\n\nCopied\n\n```\noutputs = model.generate(tokenized_chat, max_new_tokens=128) print(tokenizer.decode(outputs[0]))\n```\n\nThis will yield:\n\nCopied\n\n```\n<|system|> You are a friendly chatbot who always responds in the style of a pirate</s> <|user|> How many helicopters can a human eat in one sitting?</s> <|assistant|> Matey, I'm afraid I must inform ye that humans cannot eat helicopters. Helicopters are not food, they are flying machines. Food is meant to be eaten, like a hearty plate o' grog, a savory bowl o' stew, or a delicious loaf o' bread. But helicopters, they be for transportin' and movin' around, not for eatin'. So, I'd say none, me hearties. None at all.\n```\n\nArr, ‚Äòtwas easy after all!\n\n## [](#is-there-an-automated-pipeline-for-chat) Is there an automated pipeline for chat?\n\nYes, there is! Our text generation pipelines support chat inputs, which makes it easy to use chat models. In the past, we used to use a dedicated ‚ÄúConversationalPipeline‚Äù class, but this has now been deprecated and its functionality has been merged into the [TextGenerationPipeline](/docs/transformers/v4.48.0/en/main_classes/pipelines#transformers.TextGenerationPipeline). Let‚Äôs try the `Zephyr` example again, but this time using a pipeline:\n\nCopied\n\n```\nfrom transformers import pipeline pipe = pipeline(\"text-generation\", \"HuggingFaceH4/zephyr-7b-beta\") messages = [ { \"role\": \"system\", \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\", }, {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"}, ] print(pipe(messages, max_new_tokens=128)[0]['generated_text'][-1]) # Print the assistant's response\n```\n\nCopied\n\n```\n{'role': 'assistant', 'content': \"Matey, I'm afraid I must inform ye that humans cannot eat helicopters. Helicopters are not food, they are flying machines. Food is meant to be eaten, like a hearty plate o' grog, a savory bowl o' stew, or a delicious loaf o' bread. But helicopters, they be for transportin' and movin' around, not for eatin'. So, I'd say none, me hearties. None at all.\"}\n```\n\nThe pipeline will take care of all the details of tokenization and calling `apply_chat_template` for you - once the model has a chat template, all you need to do is initialize the pipeline and pass it the list of messages!\n\n## [](#what-are-generation-prompts) What are ‚Äúgeneration prompts‚Äù?\n\nYou may have noticed that the `apply_chat_template` method has an `add_generation_prompt` argument. This argument tells the template to add tokens that indicate the start of a bot response. For example, consider the following chat:\n\nCopied\n\n```\nmessages = [ {\"role\": \"user\", \"content\": \"Hi there!\"}, {\"role\": \"assistant\", \"content\": \"Nice to meet you!\"}, {\"role\": \"user\", \"content\": \"Can I ask a question?\"} ]\n```\n\nHere‚Äôs what this will look like without a generation prompt, for a model that uses standard ‚ÄúChatML‚Äù formatting:\n\nCopied\n\n```\ntokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False) \"\"\"<|im_start|>user Hi there!<|im_end|> <|im_start|>assistant Nice to meet you!<|im_end|> <|im_start|>user Can I ask a question?<|im_end|> \"\"\"\n```\n\nAnd here‚Äôs what it looks like **with** a generation prompt:\n\nCopied\n\n```\ntokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True) \"\"\"<|im_start|>user Hi there!<|im_end|> <|im_start|>assistant Nice to meet you!<|im_end|> <|im_start|>user Can I ask a question?<|im_end|> <|im_start|>assistant \"\"\"\n```\n\nNote that this time, we‚Äôve added the tokens that indicate the start of a bot response. This ensures that when the model generates text it will write a bot response instead of doing something unexpected, like continuing the user‚Äôs message. Remember, chat models are still just language models - they‚Äôre trained to continue text, and chat is just a special kind of text to them! You need to guide them with appropriate control tokens, so they know what they‚Äôre supposed to be doing.\n\nNot all models require generation prompts. Some models, like LLaMA, don‚Äôt have any special tokens before bot responses. In these cases, the `add_generation_prompt` argument will have no effect. The exact effect that `add_generation_prompt` has will depend on the template being used.\n\n## [](#what-does-continuefinalmessage-do) What does ‚Äúcontinue_final_message‚Äù do?\n\nWhen passing a list of messages to `apply_chat_template` or `TextGenerationPipeline`, you can choose to format the chat so the model will continue the final message in the chat instead of starting a new one. This is done by removing any end-of-sequence tokens that indicate the end of the final message, so that the model will simply extend the final message when it begins to generate text. This is useful for ‚Äúprefilling‚Äù the model‚Äôs response.\n\nHere‚Äôs an example:\n\nCopied\n\n```\nchat = [ {\"role\": \"user\", \"content\": \"Can you format the answer in JSON?\"}, {\"role\": \"assistant\", \"content\": '{\"name\": \"'}, ] formatted_chat = tokenizer.apply_chat_template(chat, tokenize=True, return_dict=True, continue_final_message=True) model.generate(**formatted_chat)\n```\n\nThe model will generate text that continues the JSON string, rather than starting a new message. This approach can be very useful for improving the accuracy of the model‚Äôs instruction-following when you know how you want it to start its replies.\n\nBecause `add_generation_prompt` adds the tokens that start a new message, and `continue_final_message` removes any end-of-message tokens from the final message, it does not make sense to use them together. As a result, you‚Äôll get an error if you try!\n\nThe default behaviour of `TextGenerationPipeline` is to set `add_generation_prompt=True` so that it starts a new message. However, if the final message in the input chat has the ‚Äúassistant‚Äù role, it will assume that this message is a prefill and switch to `continue_final_message=True` instead, because most models do not support multiple consecutive assistant messages. You can override this behaviour by explicitly passing the `continue_final_message` argument when calling the pipeline.\n\n## [](#can-i-use-chat-templates-in-training) Can I use chat templates in training?\n\nYes! This is a good way to ensure that the chat template matches the tokens the model sees during training. We recommend that you apply the chat template as a preprocessing step for your dataset. After this, you can simply continue like any other language model training task. When training, you should usually set `add_generation_prompt=False`, because the added tokens to prompt an assistant response will not be helpful during training. Let‚Äôs see an example:\n\nCopied\n\n```\nfrom transformers import AutoTokenizer from datasets import Dataset tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\") chat1 = [ {\"role\": \"user\", \"content\": \"Which is bigger, the moon or the sun?\"}, {\"role\": \"assistant\", \"content\": \"The sun.\"} ] chat2 = [ {\"role\": \"user\", \"content\": \"Which is bigger, a virus or a bacterium?\"}, {\"role\": \"assistant\", \"content\": \"A bacterium.\"} ] dataset = Dataset.from_dict({\"chat\": [chat1, chat2]}) dataset = dataset.map(lambda x: {\"formatted_chat\": tokenizer.apply_chat_template(x[\"chat\"], tokenize=False, add_generation_prompt=False)}) print(dataset['formatted_chat'][0])\n```\n\nAnd we get:\n\nCopied\n\n```\n<|user|> Which is bigger, the moon or the sun?</s> <|assistant|> The sun.</s>\n```\n\nFrom here, just continue training like you would with a standard language modelling task, using the `formatted_chat` column.\n\nBy default, some tokenizers add special tokens like `<bos>` and `<eos>` to text they tokenize. Chat templates should already include all the special tokens they need, and so additional special tokens will often be incorrect or duplicated, which will hurt model performance.\n\nTherefore, if you format text with `apply_chat_template(tokenize=False)`, you should set the argument `add_special_tokens=False` when you tokenize that text later. If you use `apply_chat_template(tokenize=True)`, you don‚Äôt need to worry about this!\n\n## [](#advanced-extra-inputs-to-chat-templates) Advanced: Extra inputs to chat templates\n\nThe only argument that `apply_chat_template` requires is `messages`. However, you can pass any keyword argument to `apply_chat_template` and it will be accessible inside the template. This gives you a lot of freedom to use chat templates for many things. There are no restrictions on the names or the format of these arguments - you can pass strings, lists, dicts or whatever else you want.\n\nThat said, there are some common use-cases for these extra arguments, such as passing tools for function calling, or documents for retrieval-augmented generation. In these common cases, we have some opinionated recommendations about what the names and formats of these arguments should be, which are described in the sections below. We encourage model authors to make their chat templates compatible with this format, to make it easy to transfer tool-calling code between models.\n\n## [](#advanced-tool-use--function-calling) Advanced: Tool use / function calling\n\n‚ÄúTool use‚Äù LLMs can choose to call functions as external tools before generating an answer. When passing tools to a tool-use model, you can simply pass a list of functions to the `tools` argument:\n\nCopied\n\n```\nimport datetime def current_time(): \"\"\"Get the current local time as a string.\"\"\" return str(datetime.now()) def multiply(a: float, b: float): \"\"\" A function that multiplies two numbers Args: a: The first number to multiply b: The second number to multiply \"\"\" return a * b tools = [current_time, multiply] model_input = tokenizer.apply_chat_template( messages, tools=tools )\n```\n\nIn order for this to work correctly, you should write your functions in the format above, so that they can be parsed correctly as tools. Specifically, you should follow these rules:\n\n  * The function should have a descriptive name\n  * Every argument must have a type hint\n  * The function must have a docstring in the standard Google style (in other words, an initial function description followed by an `Args:` block that describes the arguments, unless the function does not have any arguments.\n  * Do not include types in the `Args:` block. In other words, write `a: The first number to multiply`, not `a (int): The first number to multiply`. Type hints should go in the function header instead.\n  * The function can have a return type and a `Returns:` block in the docstring. However, these are optional because most tool-use models ignore them.\n\n\n\n### [](#passing-tool-results-to-the-model) Passing tool results to the model\n\nThe sample code above is enough to list the available tools for your model, but what happens if it wants to actually use one? If that happens, you should:\n\n  1. Parse the model‚Äôs output to get the tool name(s) and arguments.\n  2. Add the model‚Äôs tool call(s) to the conversation.\n  3. Call the corresponding function(s) with those arguments.\n  4. Add the result(s) to the conversation\n\n\n\n### [](#a-complete-tool-use-example) A complete tool use example\n\nLet‚Äôs walk through a tool use example, step by step. For this example, we will use an 8B `Hermes-2-Pro` model, as it is one of the highest-performing tool-use models in its size category at the time of writing. If you have the memory, you can consider using a larger model instead like [Command-R](https://huggingface.co/CohereForAI/c4ai-command-r-v01) or [Mixtral-8x22B](https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1), both of which also support tool use and offer even stronger performance.\n\nFirst, let‚Äôs load our model and tokenizer:\n\nCopied\n\n```\nimport torch from transformers import AutoModelForCausalLM, AutoTokenizer checkpoint = \"NousResearch/Hermes-2-Pro-Llama-3-8B\" tokenizer = AutoTokenizer.from_pretrained(checkpoint) model = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=torch.bfloat16, device_map=\"auto\")\n```\n\nNext, let‚Äôs define a list of tools:\n\nCopied\n\n```\ndef get_current_temperature(location: str, unit: str) -> float: \"\"\" Get the current temperature at a location. Args: location: The location to get the temperature for, in the format \"City, Country\" unit: The unit to return the temperature in. (choices: [\"celsius\", \"fahrenheit\"]) Returns: The current temperature at the specified location in the specified units, as a float. \"\"\" return 22. # A real function should probably actually get the temperature! def get_current_wind_speed(location: str) -> float: \"\"\" Get the current wind speed in km/h at a given location. Args: location: The location to get the temperature for, in the format \"City, Country\" Returns: The current wind speed at the given location in km/h, as a float. \"\"\" return 6. # A real function should probably actually get the wind speed! tools = [get_current_temperature, get_current_wind_speed]\n```\n\nNow, let‚Äôs set up a conversation for our bot:\n\nCopied\n\n```\nmessages = [ {\"role\": \"system\", \"content\": \"You are a bot that responds to weather queries. You should reply with the unit used in the queried location.\"}, {\"role\": \"user\", \"content\": \"Hey, what's the temperature in Paris right now?\"} ]\n```\n\nNow, let‚Äôs apply the chat template and generate a response:\n\nCopied\n\n```\ninputs = tokenizer.apply_chat_template(messages, tools=tools, add_generation_prompt=True, return_dict=True, return_tensors=\"pt\") inputs = {k: v.to(model.device) for k, v in inputs.items()} out = model.generate(**inputs, max_new_tokens=128) print(tokenizer.decode(out[0][len(inputs[\"input_ids\"][0]):]))\n```\n\nAnd we get:\n\nCopied\n\n```\n<tool_call> {\"arguments\": {\"location\": \"Paris, France\", \"unit\": \"celsius\"}, \"name\": \"get_current_temperature\"} </tool_call><|im_end|>\n```\n\nThe model has called the function with valid arguments, in the format requested by the function docstring. It has inferred that we‚Äôre most likely referring to the Paris in France, and it remembered that, as the home of SI units, the temperature in France should certainly be displayed in Celsius.\n\nThe output format above is specific to the `Hermes-2-Pro` model we‚Äôre using in this example. Other models may emit different tool call formats, and you may need to do some manual parsing at this step. For example, `Llama-3.1` models will emit slightly different JSON, with `parameters` instead of `arguments`. Regardless of the format the model outputs, you should add the tool call to the conversation in the format below, with `tool_calls`, `function` and `arguments` keys.\n\nNext, let‚Äôs append the model‚Äôs tool call to the conversation.\n\nCopied\n\n```\ntool_call = {\"name\": \"get_current_temperature\", \"arguments\": {\"location\": \"Paris, France\", \"unit\": \"celsius\"}} messages.append({\"role\": \"assistant\", \"tool_calls\": [{\"type\": \"function\", \"function\": tool_call}]})\n```\n\nIf you‚Äôre familiar with the OpenAI API, you should pay attention to an important difference here - the `tool_call` is a dict, but in the OpenAI API it‚Äôs a JSON string. Passing a string may cause errors or strange model behaviour!\n\nNow that we‚Äôve added the tool call to the conversation, we can call the function and append the result to the conversation. Since we‚Äôre just using a dummy function for this example that always returns 22.0, we can just append that result directly.\n\nCopied\n\n```\nmessages.append({\"role\": \"tool\", \"name\": \"get_current_temperature\", \"content\": \"22.0\"})\n```\n\nSome model architectures, notably Mistral/Mixtral, also require a `tool_call_id` here, which should be 9 randomly-generated alphanumeric characters, and assigned to the `id` key of the tool call dictionary. The same key should also be assigned to the `tool_call_id` key of the tool response dictionary below, so that tool calls can be matched to tool responses. So, for Mistral/Mixtral models, the code above would be:\n\nCopied\n\n```\ntool_call_id = \"9Ae3bDc2F\" # Random ID, 9 alphanumeric characters tool_call = {\"name\": \"get_current_temperature\", \"arguments\": {\"location\": \"Paris, France\", \"unit\": \"celsius\"}} messages.append({\"role\": \"assistant\", \"tool_calls\": [{\"type\": \"function\", \"id\": tool_call_id, \"function\": tool_call}]})\n```\n\nand\n\nCopied\n\n```\nmessages.append({\"role\": \"tool\", \"tool_call_id\": tool_call_id, \"name\": \"get_current_temperature\", \"content\": \"22.0\"})\n```\n\nFinally, let‚Äôs let the assistant read the function outputs and continue chatting with the user:\n\nCopied\n\n```\ninputs = tokenizer.apply_chat_template(messages, tools=tools, add_generation_prompt=True, return_dict=True, return_tensors=\"pt\") inputs = {k: v.to(model.device) for k, v in inputs.items()} out = model.generate(**inputs, max_new_tokens=128) print(tokenizer.decode(out[0][len(inputs[\"input_ids\"][0]):]))\n```\n\nAnd we get:\n\nCopied\n\n```\nThe current temperature in Paris, France is 22.0 ¬∞ Celsius.<|im_end|>\n```\n\nAlthough this was a simple demo with dummy tools and a single call, the same technique works with multiple real tools and longer conversations. This can be a powerful way to extend the capabilities of conversational agents with real-time information, computational tools like calculators, or access to large databases.\n\n### [](#understanding-tool-schemas) Understanding tool schemas\n\nEach function you pass to the `tools` argument of `apply_chat_template` is converted into a [JSON schema](https://json-schema.org/learn/getting-started-step-by-step). These schemas are then passed to the model chat template. In other words, tool-use models do not see your functions directly, and they never see the actual code inside them. What they care about is the function **definitions** and the **arguments** they need to pass to them - they care about what the tools do and how to use them, not how they work! It is up to you to read their outputs, detect if they have requested to use a tool, pass their arguments to the tool function, and return the response in the chat.\n\nGenerating JSON schemas to pass to the template should be automatic and invisible as long as your functions follow the specification above, but if you encounter problems, or you simply want more control over the conversion, you can handle the conversion manually. Here is an example of a manual schema conversion.\n\nCopied\n\n```\nfrom transformers.utils import get_json_schema def multiply(a: float, b: float): \"\"\" A function that multiplies two numbers Args: a: The first number to multiply b: The second number to multiply \"\"\" return a * b schema = get_json_schema(multiply) print(schema)\n```\n\nThis will yield:\n\nCopied\n\n```\n{ \"type\": \"function\", \"function\": { \"name\": \"multiply\", \"description\": \"A function that multiplies two numbers\", \"parameters\": { \"type\": \"object\", \"properties\": { \"a\": { \"type\": \"number\", \"description\": \"The first number to multiply\" }, \"b\": { \"type\": \"number\", \"description\": \"The second number to multiply\" } }, \"required\": [\"a\", \"b\"] } } }\n```\n\nIf you wish, you can edit these schemas, or even write them from scratch yourself without using `get_json_schema` at all. JSON schemas can be passed directly to the `tools` argument of `apply_chat_template` - this gives you a lot of power to define precise schemas for more complex functions. Be careful, though - the more complex your schemas, the more likely the model is to get confused when dealing with them! We recommend simple function signatures where possible, keeping arguments (and especially complex, nested arguments) to a minimum.\n\nHere is an example of defining schemas by hand, and passing them directly to `apply_chat_template`:\n\nCopied\n\n```\n# A simple function that takes no arguments current_time = { \"type\": \"function\", \"function\": { \"name\": \"current_time\", \"description\": \"Get the current local time as a string.\", \"parameters\": { 'type': 'object', 'properties': {} } } } # A more complete function that takes two numerical arguments multiply = { 'type': 'function', 'function': { 'name': 'multiply', 'description': 'A function that multiplies two numbers', 'parameters': { 'type': 'object', 'properties': { 'a': { 'type': 'number', 'description': 'The first number to multiply' }, 'b': { 'type': 'number', 'description': 'The second number to multiply' } }, 'required': ['a', 'b'] } } } model_input = tokenizer.apply_chat_template( messages, tools = [current_time, multiply] )\n```\n\n## [](#advanced-retrieval-augmented-generation) Advanced: Retrieval-augmented generation\n\n‚ÄúRetrieval-augmented generation‚Äù or ‚ÄúRAG‚Äù LLMs can search a corpus of documents for information before responding to a query. This allows models to vastly expand their knowledge base beyond their limited context size. Our recommendation for RAG models is that their template should accept a `documents` argument. This should be a list of documents, where each ‚Äúdocument‚Äù is a single dict with `title` and `contents` keys, both of which are strings. Because this format is much simpler than the JSON schemas used for tools, no helper functions are necessary.\n\nHere‚Äôs an example of a RAG template in action:\n\nCopied\n\n```\nfrom transformers import AutoTokenizer, AutoModelForCausalLM # Load the model and tokenizer model_id = \"CohereForAI/c4ai-command-r-v01-4bit\" tokenizer = AutoTokenizer.from_pretrained(model_id) model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\") device = model.device # Get the device the model is loaded on # Define conversation input conversation = [ {\"role\": \"user\", \"content\": \"What has Man always dreamed of?\"} ] # Define documents for retrieval-based generation documents = [ { \"title\": \"The Moon: Our Age-Old Foe\", \"text\": \"Man has always dreamed of destroying the moon. In this essay, I shall...\" }, { \"title\": \"The Sun: Our Age-Old Friend\", \"text\": \"Although often underappreciated, the sun provides several notable benefits...\" } ] # Tokenize conversation and documents using a RAG template, returning PyTorch tensors. input_ids = tokenizer.apply_chat_template( conversation=conversation, documents=documents, chat_template=\"rag\", tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(device) # Generate a response  gen_tokens = model.generate( input_ids, max_new_tokens=100, do_sample=True, temperature=0.3, ) # Decode and print the generated text along with generation prompt gen_text = tokenizer.decode(gen_tokens[0]) print(gen_text)\n```\n\nThe `documents` input for retrieval-augmented generation is not widely supported, and many models have chat templates which simply ignore this input.\n\nTo verify if a model supports the `documents` input, you can read its model card, or `print(tokenizer.chat_template)` to see if the `documents` key is used anywhere.\n\nOne model class that does support it, though, is Cohere‚Äôs [Command-R](https://huggingface.co/CohereForAI/c4ai-command-r-08-2024) and [Command-R+](https://huggingface.co/CohereForAI/c4ai-command-r-plus-08-2024), through their `rag` chat template. You can see additional examples of grounded generation using this feature in their model cards.\n\n## [](#advanced-how-do-chat-templates-work) Advanced: How do chat templates work?\n\nThe chat template for a model is stored on the `tokenizer.chat_template` attribute. If no chat template is set, the default template for that model class is used instead. Let‚Äôs take a look at a `Zephyr` chat template, though note this one is a little simplified from the actual one!\n\nCopied\n\n```\n{%- for message in messages %} {{- '<|' + message['role'] + '|>\\n' }}{{- message['content'] + eos_token }} {%- endfor %} {%- if add_generation_prompt %} {{- '<|assistant|>\\n' }} {%- endif %}\n```\n\nIf you‚Äôve never seen one of these before, this is a [Jinja template](https://jinja.palletsprojects.com/en/3.1.x/templates/). Jinja is a templating language that allows you to write simple code that generates text. In many ways, the code and syntax resembles Python. In pure Python, this template would look something like this:\n\nCopied\n\n```\nfor message in messages: print(f'<|{message[\"role\"]}|>') print(message['content'] + eos_token) if add_generation_prompt: print('<|assistant|>')\n```\n\nEffectively, the template does three things:\n\n  1. For each message, print the role enclosed in `<|` and `|>`, like `<|user|>` or `<|assistant|>`.\n  2. Next, print the content of the message, followed by the end-of-sequence token.\n  3. Finally, if `add_generation_prompt` is set, print the assistant token, so that the model knows to start generating an assistant response.\n\n\n\nThis is a pretty simple template but Jinja gives you a lot of flexibility to do more complex things! Let‚Äôs see a Jinja template that can format inputs similarly to the way LLaMA formats them (note that the real LLaMA template includes handling for default system messages and slightly different system message handling in general - don‚Äôt use this one in your actual code!)\n\nCopied\n\n```\n{%- for message in messages %} {%- if message['role'] == 'user' %} {{- bos_token + '[INST] ' + message['content'] + ' [/INST]' }} {%- elif message['role'] == 'system' %} {{- '<<SYS>>\\\\\\n' + message['content'] + '\\\\\\n<</SYS>>\\\\\\n\\\\\\n' }} {%- elif message['role'] == 'assistant' %} {{- ' ' + message['content'] + ' ' + eos_token }} {%- endif %} {%- endfor %}\n```\n\nHopefully if you stare at this for a little bit you can see what this template is doing - it adds specific tokens like `[INST]` and `[/INST]` based on the role of each message. User, assistant and system messages are clearly distinguishable to the model because of the tokens they‚Äôre wrapped in.\n\n## [](#advanced-adding-and-editing-chat-templates) Advanced: Adding and editing chat templates\n\n### [](#how-do-i-create-a-chat-template) How do I create a chat template?\n\nSimple, just write a jinja template and set `tokenizer.chat_template`. You may find it easier to start with an existing template from another model and simply edit it for your needs! For example, we could take the LLaMA template above and add ‚Äù[ASST]‚Äù and ‚Äù[/ASST]‚Äù to assistant messages:\n\nCopied\n\n```\n{%- for message in messages %} {%- if message['role'] == 'user' %} {{- bos_token + '[INST] ' + message['content'].strip() + ' [/INST]' }} {%- elif message['role'] == 'system' %} {{- '<<SYS>>\\\\\\n' + message['content'].strip() + '\\\\\\n<</SYS>>\\\\\\n\\\\\\n' }} {%- elif message['role'] == 'assistant' %} {{- '[ASST] ' + message['content'] + ' [/ASST]' + eos_token }} {%- endif %} {%- endfor %}\n```\n\nNow, simply set the `tokenizer.chat_template` attribute. Next time you use [apply_chat_template()](/docs/transformers/v4.48.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template), it will use your new template! This attribute will be saved in the `tokenizer_config.json` file, so you can use [push_to_hub()](/docs/transformers/v4.48.0/en/main_classes/model#transformers.utils.PushToHubMixin.push_to_hub) to upload your new template to the Hub and make sure everyone‚Äôs using the right template for your model!\n\nCopied\n\n```\ntemplate = tokenizer.chat_template template = template.replace(\"SYS\", \"SYSTEM\") # Change the system token tokenizer.chat_template = template # Set the new template tokenizer.push_to_hub(\"model_name\") # Upload your new template to the Hub!\n```\n\nThe method [apply_chat_template()](/docs/transformers/v4.48.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template) which uses your chat template is called by the [TextGenerationPipeline](/docs/transformers/v4.48.0/en/main_classes/pipelines#transformers.TextGenerationPipeline) class, so once you set the correct chat template, your model will automatically become compatible with [TextGenerationPipeline](/docs/transformers/v4.48.0/en/main_classes/pipelines#transformers.TextGenerationPipeline).\n\nIf you're fine-tuning a model for chat, in addition to setting a chat template, you should probably add any new chat control tokens as special tokens in the tokenizer. Special tokens are never split, ensuring that your control tokens are always handled as single tokens rather than being tokenized in pieces. You should also set the tokenizer's `eos_token` attribute to the token that marks the end of assistant generations in your template. This will ensure that text generation tools can correctly figure out when to stop generating text.\n\n### [](#why-do-some-models-have-multiple-templates) Why do some models have multiple templates?\n\nSome models use different templates for different use cases. For example, they might use one template for normal chat and another for tool-use, or retrieval-augmented generation. In these cases, `tokenizer.chat_template` is a dictionary. This can cause some confusion, and where possible, we recommend using a single template for all use-cases. You can use Jinja statements like `if tools is defined` and `{% macro %}` definitions to easily wrap multiple code paths in a single template.\n\nWhen a tokenizer has multiple templates, `tokenizer.chat_template` will be a `dict`, where each key is the name of a template. The `apply_chat_template` method has special handling for certain template names: Specifically, it will look for a template named `default` in most cases, and will raise an error if it can‚Äôt find one. However, if a template named `tool_use` exists when the user has passed a `tools` argument, it will use that instead. To access templates with other names, pass the name of the template you want to the `chat_template` argument of `apply_chat_template()`.\n\nWe find that this can be a bit confusing for users, though - so if you‚Äôre writing a template yourself, we recommend trying to put it all in a single template where possible!\n\n### [](#what-template-should-i-use) What template should I use?\n\nWhen setting the template for a model that‚Äôs already been trained for chat, you should ensure that the template exactly matches the message formatting that the model saw during training, or else you will probably experience performance degradation. This is true even if you‚Äôre training the model further - you will probably get the best performance if you keep the chat tokens constant. This is very analogous to tokenization - you generally get the best performance for inference or fine-tuning when you precisely match the tokenization used during training.\n\nIf you‚Äôre training a model from scratch, or fine-tuning a base language model for chat, on the other hand, you have a lot of freedom to choose an appropriate template! LLMs are smart enough to learn to handle lots of different input formats. One popular choice is the `ChatML` format, and this is a good, flexible choice for many use-cases. It looks like this:\n\nCopied\n\n```\n{%- for message in messages %} {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n' }} {%- endfor %}\n```\n\nIf you like this one, here it is in one-liner form, ready to copy into your code. The one-liner also includes handy support for [generation prompts](#what-are-generation-prompts), but note that it doesn‚Äôt add BOS or EOS tokens! If your model expects those, they won‚Äôt be added automatically by `apply_chat_template` - in other words, the text will be tokenized with `add_special_tokens=False`. This is to avoid potential conflicts between the template and the `add_special_tokens` logic. If your model expects special tokens, make sure to add them to the template!\n\nCopied\n\n```\ntokenizer.chat_template = \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"\n```\n\nThis template wraps each message in `<|im_start|>` and `<|im_end|>` tokens, and simply writes the role as a string, which allows for flexibility in the roles you train with. The output looks like this:\n\nCopied\n\n```\n<|im_start|>system You are a helpful chatbot that will do its best not to say anything so stupid that people tweet about it.<|im_end|> <|im_start|>user How are you?<|im_end|> <|im_start|>assistant I'm doing great!<|im_end|>\n```\n\nThe ‚Äúuser‚Äù, ‚Äúsystem‚Äù and ‚Äúassistant‚Äù roles are the standard for chat, and we recommend using them when it makes sense, particularly if you want your model to operate well with [TextGenerationPipeline](/docs/transformers/v4.48.0/en/main_classes/pipelines#transformers.TextGenerationPipeline). However, you are not limited to these roles - templating is extremely flexible, and any string can be a role.\n\n### [](#i-want-to-add-some-chat-templates-how-should-i-get-started) I want to add some chat templates! How should I get started?\n\nIf you have any chat models, you should set their `tokenizer.chat_template` attribute and test it using [apply_chat_template()](/docs/transformers/v4.48.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template), then push the updated tokenizer to the Hub. This applies even if you‚Äôre not the model owner - if you‚Äôre using a model with an empty chat template, or one that‚Äôs still using the default class template, please open a [pull request](https://huggingface.co/docs/hub/repositories-pull-requests-discussions) to the model repository so that this attribute can be set properly!\n\nOnce the attribute is set, that‚Äôs it, you‚Äôre done! `tokenizer.apply_chat_template` will now work correctly for that model, which means it is also automatically supported in places like `TextGenerationPipeline`!\n\nBy ensuring that models have this attribute, we can make sure that the whole community gets to use the full power of open-source models. Formatting mismatches have been haunting the field and silently harming performance for too long - it‚Äôs time to put an end to them!\n\n## [](#advanced-template-writing-tips) Advanced: Template writing tips\n\nThe easiest way to get started with writing Jinja templates is to take a look at some existing ones. You can use `print(tokenizer.chat_template)` for any chat model to see what template it‚Äôs using. In general, models that support tool use have much more complex templates than other models - so when you‚Äôre just getting started, they‚Äôre probably a bad example to learn from! You can also take a look at the [Jinja documentation](https://jinja.palletsprojects.com/en/3.1.x/templates/#synopsis) for details of general Jinja formatting and syntax.\n\nJinja templates in `transformers` are identical to Jinja templates elsewhere. The main thing to know is that the conversation history will be accessible inside your template as a variable called `messages`. You will be able to access `messages` in your template just like you can in Python, which means you can loop over it with `{% for message in messages %}` or access individual messages with `{{ messages[0] }}`, for example.\n\nYou can also use the following tips to write clean, efficient Jinja templates:\n\n### [](#trimming-whitespace) Trimming whitespace\n\nBy default, Jinja will print any whitespace that comes before or after a block. This can be a problem for chat templates, which generally want to be very precise with whitespace! To avoid this, we strongly recommend writing your templates like this:\n\nCopied\n\n```\n{%- for message in messages %} {{- message['role'] + message['content'] }} {%- endfor %}\n```\n\nrather than like this:\n\nCopied\n\n```\n{% for message in messages %}{{ message['role'] + message['content'] }}{% endfor %}\n```\n\nAdding `-` will strip any whitespace that comes before the block. The second example looks innocent, but the newline and indentation may end up being included in the output, which is probably not what you want!\n\n### [](#special-variables) Special variables\n\nInside your template, you will have access several special variables. The most important of these is `messages`, which contains the chat history as a list of message dicts. However, there are several others. Not every variable will be used in every template. The most common other variables are:\n\n  * `tools` contains a list of tools in JSON schema format. Will be `None` or undefined if no tools are passed.\n  * `documents` contains a list of documents in the format `{\"title\": \"Title\", \"contents\": \"Contents\"}`, used for retrieval-augmented generation. Will be `None` or undefined if no documents are passed.\n  * `add_generation_prompt` is a bool that is `True` if the user has requested a generation prompt, and `False` otherwise. If this is set, your template should add the header for an assistant message to the end of the conversation. If your model doesn‚Äôt have a specific header for assistant messages, you can ignore this flag.\n  * **Special tokens** like `bos_token` and `eos_token`. These are extracted from `tokenizer.special_tokens_map`. The exact tokens available inside each template will differ depending on the parent tokenizer.\n\n\n\nYou can actually pass any `kwarg` to `apply_chat_template`, and it will be accessible inside the template as a variable. In general, we recommend trying to stick to the core variables above, as it will make your model harder to use if users have to write custom code to pass model-specific `kwargs`. However, we‚Äôre aware that this field moves quickly, so if you have a new use-case that doesn‚Äôt fit in the core API, feel free to use a new `kwarg` for it! If a new `kwarg` becomes common we may promote it into the core API and create a standard, documented format for it.\n\n### [](#callable-functions) Callable functions\n\nThere is also a short list of callable functions available to you inside your templates. These are:\n\n  * `raise_exception(msg)`: Raises a `TemplateException`. This is useful for debugging, and for telling users when they‚Äôre doing something that your template doesn‚Äôt support.\n  * `strftime_now(format_str)`: Equivalent to `datetime.now().strftime(format_str)` in Python. This is used for getting the current date/time in a specific format, which is sometimes included in system messages.\n\n\n\n### [](#compatibility-with-non-python-jinja) Compatibility with non-Python Jinja\n\nThere are multiple implementations of Jinja in various languages. They generally have the same syntax, but a key difference is that when you‚Äôre writing a template in Python you can use Python methods, such as `.lower()` on strings or `.items()` on dicts. This will break if someone tries to use your template on a non-Python implementation of Jinja. Non-Python implementations are particularly common in deployment environments, where JS and Rust are very popular.\n\nDon‚Äôt panic, though! There are a few easy changes you can make to your templates to ensure they‚Äôre compatible across all implementations of Jinja:\n\n  * Replace Python methods with Jinja filters. These usually have the same name, for example `string.lower()` becomes `string|lower`, and `dict.items()` becomes `dict|items`. One notable change is that `string.strip()` becomes `string|trim`. See the [list of built-in filters](https://jinja.palletsprojects.com/en/3.1.x/templates/#builtin-filters) in the Jinja documentation for more.\n  * Replace `True`, `False` and `None`, which are Python-specific, with `true`, `false` and `none`.\n  * Directly rendering a dict or list may give different results in other implementations (for example, string entries might change from single-quoted to double-quoted). Adding the `tojson` filter can help to ensure consistency here.\n\n\n\n### [](#writing-generation-prompts) Writing generation prompts\n\nWe mentioned above that `add_generation_prompt` is a special variable that will be accessible inside your template, and is controlled by the user setting the `add_generation_prompt` flag. If your model expects a header for assistant messages, then your template must support adding the header when `add_generation_prompt` is set.\n\nHere is an example of a template that formats messages ChatML-style, with generation prompt support:\n\nCopied\n\n```\n{{- bos_token }} {%- for message in messages %} {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n' }} {%- endfor %} {%- if add_generation_prompt %} {{- '<|im_start|>assistant\\n' }} {%- endif %}\n```\n\nThe exact content of the assistant header will depend on your specific model, but it should always be **the string that represents the start of an assistant message** , so that if the user applies your template with `add_generation_prompt=True` and then generates text, the model will write an assistant response. Also note that some models do not need a generation prompt, because assistant messages always begin immediately after user messages. This is particularly common for LLaMA and Mistral models, where assistant messages begin immediately after the `[/INST]` token that ends user messages. In these cases, the template can ignore the `add_generation_prompt` flag.\n\nGeneration prompts are important! If your model requires a generation prompt but it is not set in the template, then model generations will likely be severely degraded, or the model may display unusual behaviour like continuing the final user message!\n\n### [](#writing-and-debugging-larger-templates) Writing and debugging larger templates\n\nWhen this feature was introduced, most templates were quite small, the Jinja equivalent of a ‚Äúone-liner‚Äù script. However, with new models and features like tool-use and RAG, some templates can be 100 lines long or more. When writing templates like these, it‚Äôs a good idea to write them in a separate file, using a text editor. You can easily extract a chat template to a file:\n\nCopied\n\n```\nopen(\"template.jinja\", \"w\").write(tokenizer.chat_template)\n```\n\nOr load the edited template back into the tokenizer:\n\nCopied\n\n```\ntokenizer.chat_template = open(\"template.jinja\").read()\n```\n\nAs an added bonus, when you write a long, multi-line template in a separate file, line numbers in that file will exactly correspond to line numbers in template parsing or execution errors. This will make it much easier to identify the source of issues.\n\n### [](#writing-templates-for-tools) Writing templates for tools\n\nAlthough chat templates do not enforce a specific API for tools (or for anything, really), we recommend template authors try to stick to a standard API where possible. The whole point of chat templates is to allow code to be transferable across models, so deviating from the standard tools API means users will have to write custom code to use tools with your model. Sometimes it‚Äôs unavoidable, but often with clever templating you can make the standard API work!\n\nBelow, we‚Äôll list the elements of the standard API, and give tips on writing templates that will work well with it.\n\n#### [](#tool-definitions) Tool definitions\n\nYour template should expect that the variable `tools` will either be null (if no tools are passed), or is a list of JSON schema dicts. Our chat template methods allow users to pass tools as either JSON schema or Python functions, but when functions are passed, we automatically generate JSON schema and pass that to your template. As a result, the `tools` variable that your template receives will always be a list of JSON schema. Here is a sample tool JSON schema:\n\nCopied\n\n```\n{ \"type\": \"function\", \"function\": { \"name\": \"multiply\", \"description\": \"A function that multiplies two numbers\", \"parameters\": { \"type\": \"object\", \"properties\": { \"a\": { \"type\": \"number\", \"description\": \"The first number to multiply\" }, \"b\": { \"type\": \"number\", \"description\": \"The second number to multiply\" } }, \"required\": [\"a\", \"b\"] } } }\n```\n\nAnd here is some example code for handling tools in your chat template. Remember, this is just an example for a specific format - your model will probably need different formatting!\n\nCopied\n\n```\n{%- if tools %} {%- for tool in tools %} {{- '<tool>' + tool['function']['name'] + '\\n' }} {%- for argument in tool['function']['parameters']['properties'] %} {{- argument + ': ' + tool['function']['parameters']['properties'][argument]['description'] + '\\n' }} {%- endfor %} {{- '\\n</tool>' }} {%- endif %} {%- endif %}\n```\n\nThe specific tokens and tool descriptions your template renders should of course be chosen to match the ones your model was trained with. There is no requirement that your **model** understands JSON schema input, only that your template can translate JSON schema into your model‚Äôs format. For example, [Command-R](https://huggingface.co/CohereForAI/c4ai-command-r-plus-08-2024) was trained with tools defined using Python function headers, but the Command-R tool template accepts JSON schema, converts types internally and renders the input tools as Python headers. You can do a lot with templates!\n\n#### [](#tool-calls) Tool calls\n\nTool calls, if present, will be a list attached to a message with the ‚Äúassistant‚Äù role. Note that `tool_calls` is always a list, even though most tool-calling models only support single tool calls at a time, which means the list will usually only have a single element. Here is a sample message dict containing a tool call:\n\nCopied\n\n```\n{ \"role\": \"assistant\", \"tool_calls\": [ { \"type\": \"function\", \"function\": { \"name\": \"multiply\", \"arguments\": { \"a\": 5, \"b\": 6 } } } ] }\n```\n\nAnd a common pattern for handling them would be something like this:\n\nCopied\n\n```\n{%- if message['role'] == 'assistant' and 'tool_calls' in message %} {%- for tool_call in message['tool_calls'] %} {{- '<tool_call>' + tool_call['function']['name'] + '\\n' + tool_call['function']['arguments']|tojson + '\\n</tool_call>' }} {%- endif %} {%- endfor %} {%- endif %}\n```\n\nAgain, you should render the tool call with the formatting and special tokens that your model expects.\n\n#### [](#tool-responses) Tool responses\n\nTool responses have a simple format: They are a message dict with the ‚Äútool‚Äù role, a ‚Äúname‚Äù key giving the name of the called function, and a ‚Äúcontent‚Äù key containing the result of the tool call. Here is a sample tool response:\n\nCopied\n\n```\n{ \"role\": \"tool\", \"name\": \"multiply\", \"content\": \"30\" }\n```\n\nYou don‚Äôt need to use all of the keys in the tool response. For example, if your model doesn‚Äôt expect the function name to be included in the tool response, then rendering it can be as simple as:\n\nCopied\n\n```\n{%- if message['role'] == 'tool' %} {{- \"<tool_result>\" + message['content'] + \"</tool_result>\" }} {%- endif %}\n```\n\nAgain, remember that the actual formatting and special tokens are model-specific - you should take a lot of care to ensure that tokens, whitespace and everything else exactly match the format your model was trained with!\n\n[< > Update on GitHub](https://github.com/huggingface/transformers/blob/main/docs/source/en/chat_templating.md)\n\n[‚ÜêShare a custom model](/docs/transformers/en/custom_models) [Trainer‚Üí](/docs/transformers/en/trainer)\n\n[Chat Templates](#chat-templates) [Introduction](#introduction) [How do I use chat templates?](#how-do-i-use-chat-templates) [Is there an automated pipeline for chat?](#is-there-an-automated-pipeline-for-chat) [What are ‚Äúgeneration prompts‚Äù?](#what-are-generation-prompts) [What does ‚Äúcontinue_final_message‚Äù do?](#what-does-continuefinalmessage-do) [Can I use chat templates in training?](#can-i-use-chat-templates-in-training) [Advanced: Extra inputs to chat templates](#advanced-extra-inputs-to-chat-templates) [Advanced: Tool use / function calling](#advanced-tool-use--function-calling) [Passing tool results to the model](#passing-tool-results-to-the-model) [A complete tool use example](#a-complete-tool-use-example) [Understanding tool schemas](#understanding-tool-schemas) [Advanced: Retrieval-augmented generation](#advanced-retrieval-augmented-generation) [Advanced: How do chat templates work?](#advanced-how-do-chat-templates-work) [Advanced: Adding and editing chat templates](#advanced-adding-and-editing-chat-templates) [How do I create a chat template?](#how-do-i-create-a-chat-template) [Why do some models have multiple templates?](#why-do-some-models-have-multiple-templates) [What template should I use?](#what-template-should-i-use) [I want to add some chat templates! How should I get started?](#i-want-to-add-some-chat-templates-how-should-i-get-started) [Advanced: Template writing tips](#advanced-template-writing-tips) [Trimming whitespace](#trimming-whitespace) [Special variables](#special-variables) [Callable functions](#callable-functions) [Compatibility with non-Python Jinja](#compatibility-with-non-python-jinja) [Writing generation prompts](#writing-generation-prompts) [Writing and debugging larger templates](#writing-and-debugging-larger-templates) [Writing templates for tools](#writing-templates-for-tools) [Tool definitions](#tool-definitions)[Tool calls](#tool-calls)[Tool responses](#tool-responses)\n",
    "content_quality_score": 0.9,
    "summary": null,
    "child_urls": [
        "https://huggingface.co/",
        "https://huggingface.co/models",
        "https://huggingface.co/datasets",
        "https://huggingface.co/spaces",
        "https://huggingface.co/posts",
        "https://huggingface.co/docs",
        "https://huggingface.co/enterprise",
        "https://huggingface.co/pricing",
        "https://huggingface.co/login",
        "https://huggingface.co/join",
        "https://huggingface.co/docs/transformers/en/index",
        "https://huggingface.co/docs/transformers/en/quicktour",
        "https://huggingface.co/docs/transformers/en/installation",
        "https://huggingface.co/docs/transformers/en/add_new_model",
        "https://huggingface.co/docs/transformers/en/pipeline_tutorial",
        "https://huggingface.co/docs/transformers/en/autoclass_tutorial",
        "https://huggingface.co/docs/transformers/en/preprocessing",
        "https://huggingface.co/docs/transformers/en/training",
        "https://huggingface.co/docs/transformers/en/run_scripts",
        "https://huggingface.co/docs/transformers/en/accelerate",
        "https://huggingface.co/docs/transformers/en/peft",
        "https://huggingface.co/docs/transformers/en/model_sharing",
        "https://huggingface.co/docs/transformers/en/agents",
        "https://huggingface.co/docs/transformers/en/agents_advanced",
        "https://huggingface.co/docs/transformers/en/llm_tutorial",
        "https://huggingface.co/docs/transformers/en/conversations",
        "https://huggingface.co/docs/transformers/en/fast_tokenizers",
        "https://huggingface.co/docs/transformers/en/multilingual",
        "https://huggingface.co/docs/transformers/en/create_a_model",
        "https://huggingface.co/docs/transformers/en/custom_models",
        "https://huggingface.co/docs/transformers/en/chat_templating",
        "https://huggingface.co/docs/transformers/en/trainer",
        "https://huggingface.co/docs/transformers/en/sagemaker",
        "https://huggingface.co/docs/transformers/en/serialization",
        "https://huggingface.co/docs/transformers/en/tflite",
        "https://huggingface.co/docs/transformers/en/torchscript",
        "https://huggingface.co/docs/transformers/en/benchmarks",
        "https://huggingface.co/docs/transformers/en/notebooks",
        "https://huggingface.co/docs/transformers/en/community",
        "https://huggingface.co/docs/transformers/en/troubleshooting",
        "https://huggingface.co/docs/transformers/en/gguf",
        "https://huggingface.co/docs/transformers/en/tiktoken",
        "https://huggingface.co/docs/transformers/en/modular_transformers",
        "https://huggingface.co/docs/transformers/en/how_to_hack_models",
        "https://huggingface.co/docs/transformers/en/quantization/overview",
        "https://huggingface.co/docs/transformers/en/quantization/bitsandbytes",
        "https://huggingface.co/docs/transformers/en/quantization/gptq",
        "https://huggingface.co/docs/transformers/en/quantization/awq",
        "https://huggingface.co/docs/transformers/en/quantization/aqlm",
        "https://huggingface.co/docs/transformers/en/quantization/vptq",
        "https://huggingface.co/docs/transformers/en/quantization/quanto",
        "https://huggingface.co/docs/transformers/en/quantization/eetq",
        "https://huggingface.co/docs/transformers/en/quantization/higgs",
        "https://huggingface.co/docs/transformers/en/quantization/hqq",
        "https://huggingface.co/docs/transformers/en/quantization/fbgemm_fp8",
        "https://huggingface.co/docs/transformers/en/quantization/optimum",
        "https://huggingface.co/docs/transformers/en/quantization/torchao",
        "https://huggingface.co/docs/transformers/en/quantization/bitnet",
        "https://huggingface.co/docs/transformers/en/quantization/compressed_tensors",
        "https://huggingface.co/docs/transformers/en/quantization/contribute",
        "https://huggingface.co/docs/transformers/en/performance",
        "https://huggingface.co/docs/transformers/en/llm_optims",
        "https://huggingface.co/docs/transformers/en/perf_train_gpu_one",
        "https://huggingface.co/docs/transformers/en/perf_train_gpu_many",
        "https://huggingface.co/docs/transformers/en/fsdp",
        "https://huggingface.co/docs/transformers/en/deepspeed",
        "https://huggingface.co/docs/transformers/en/perf_train_cpu",
        "https://huggingface.co/docs/transformers/en/perf_train_cpu_many",
        "https://huggingface.co/docs/transformers/en/perf_train_tpu_tf",
        "https://huggingface.co/docs/transformers/en/perf_train_special",
        "https://huggingface.co/docs/transformers/en/perf_hardware",
        "https://huggingface.co/docs/transformers/en/hpo_train",
        "https://huggingface.co/docs/transformers/en/perf_infer_cpu",
        "https://huggingface.co/docs/transformers/en/perf_infer_gpu_one",
        "https://huggingface.co/docs/transformers/en/perf_infer_gpu_multi",
        "https://huggingface.co/docs/transformers/en/big_models",
        "https://huggingface.co/docs/transformers/en/debugging",
        "https://huggingface.co/docs/transformers/en/tf_xla",
        "https://huggingface.co/docs/transformers/en/perf_torch_compile",
        "https://huggingface.co/docs/transformers/en/contributing",
        "https://huggingface.co/docs/transformers/en/add_new_pipeline",
        "https://huggingface.co/docs/transformers/en/testing",
        "https://huggingface.co/docs/transformers/en/pr_checks",
        "https://huggingface.co/docs/transformers/en/philosophy",
        "https://huggingface.co/docs/transformers/en/glossary",
        "https://huggingface.co/docs/transformers/en/task_summary",
        "https://huggingface.co/docs/transformers/en/tasks_explained",
        "https://huggingface.co/docs/transformers/en/model_summary",
        "https://huggingface.co/docs/transformers/en/tokenizer_summary",
        "https://huggingface.co/docs/transformers/en/attention",
        "https://huggingface.co/docs/transformers/en/pad_truncation",
        "https://huggingface.co/docs/transformers/en/bertology",
        "https://huggingface.co/docs/transformers/en/perplexity",
        "https://huggingface.co/docs/transformers/en/pipeline_webserver",
        "https://huggingface.co/docs/transformers/en/model_memory_anatomy",
        "https://huggingface.co/docs/transformers/en/llm_tutorial_optimization",
        "https://huggingface.co/docs/transformers/en/main_classes/agent",
        "https://huggingface.co/docs/transformers/en/model_doc/auto",
        "https://huggingface.co/docs/transformers/en/main_classes/backbones",
        "https://huggingface.co/docs/transformers/en/main_classes/callback",
        "https://huggingface.co/docs/transformers/en/main_classes/configuration",
        "https://huggingface.co/docs/transformers/en/main_classes/data_collator",
        "https://huggingface.co/docs/transformers/en/main_classes/keras_callbacks",
        "https://huggingface.co/docs/transformers/en/main_classes/logging",
        "https://huggingface.co/docs/transformers/en/main_classes/model",
        "https://huggingface.co/docs/transformers/en/main_classes/text_generation",
        "https://huggingface.co/docs/transformers/en/main_classes/onnx",
        "https://huggingface.co/docs/transformers/en/main_classes/optimizer_schedules",
        "https://huggingface.co/docs/transformers/en/main_classes/output",
        "https://huggingface.co/docs/transformers/en/main_classes/pipelines",
        "https://huggingface.co/docs/transformers/en/main_classes/processors",
        "https://huggingface.co/docs/transformers/en/main_classes/quantization",
        "https://huggingface.co/docs/transformers/en/main_classes/tokenizer",
        "https://huggingface.co/docs/transformers/en/main_classes/trainer",
        "https://huggingface.co/docs/transformers/en/main_classes/deepspeed",
        "https://huggingface.co/docs/transformers/en/main_classes/executorch",
        "https://huggingface.co/docs/transformers/en/main_classes/feature_extractor",
        "https://huggingface.co/docs/transformers/en/main_classes/image_processor",
        "https://huggingface.co/docs/transformers/en/internal/modeling_utils",
        "https://huggingface.co/docs/transformers/en/internal/pipelines_utils",
        "https://huggingface.co/docs/transformers/en/internal/tokenization_utils",
        "https://huggingface.co/docs/transformers/en/internal/trainer_utils",
        "https://huggingface.co/docs/transformers/en/internal/generation_utils",
        "https://huggingface.co/docs/transformers/en/internal/image_processing_utils",
        "https://huggingface.co/docs/transformers/en/internal/audio_utils",
        "https://huggingface.co/docs/transformers/en/internal/file_utils",
        "https://huggingface.co/docs/transformers/en/internal/time_series_utils",
        "https://huggingface.co/docs/transformers/en/chat_templating#chat-templates",
        "https://huggingface.co/docs/transformers/en/chat_templating#introduction",
        "https://huggingface.co/docs/transformers/en/chat_templating#how-do-i-use-chat-templates",
        "https://huggingface.co/docs/transformers/v4.48.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template",
        "https://huggingface.co/docs/transformers/en/chat_templating#what-are-generation-prompts",
        "https://huggingface.co/docs/transformers/en/chat_templating#is-there-an-automated-pipeline-for-chat",
        "https://huggingface.co/docs/transformers/v4.48.0/en/main_classes/pipelines#transformers.TextGenerationPipeline",
        "https://huggingface.co/docs/transformers/en/chat_templating#what-does-continuefinalmessage-do",
        "https://huggingface.co/docs/transformers/en/chat_templating#can-i-use-chat-templates-in-training",
        "https://huggingface.co/docs/transformers/en/chat_templating#advanced-extra-inputs-to-chat-templates",
        "https://huggingface.co/docs/transformers/en/chat_templating#advanced-tool-use--function-calling",
        "https://huggingface.co/docs/transformers/en/chat_templating#passing-tool-results-to-the-model",
        "https://huggingface.co/docs/transformers/en/chat_templating#a-complete-tool-use-example",
        "https://huggingface.co/CohereForAI/c4ai-command-r-v01",
        "https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1",
        "https://huggingface.co/docs/transformers/en/chat_templating#understanding-tool-schemas",
        "https://huggingface.co/docs/transformers/en/chat_templating#advanced-retrieval-augmented-generation",
        "https://huggingface.co/CohereForAI/c4ai-command-r-08-2024",
        "https://huggingface.co/CohereForAI/c4ai-command-r-plus-08-2024",
        "https://huggingface.co/docs/transformers/en/chat_templating#advanced-how-do-chat-templates-work",
        "https://huggingface.co/docs/transformers/en/chat_templating#advanced-adding-and-editing-chat-templates",
        "https://huggingface.co/docs/transformers/en/chat_templating#how-do-i-create-a-chat-template",
        "https://huggingface.co/docs/transformers/v4.48.0/en/main_classes/model#transformers.utils.PushToHubMixin.push_to_hub",
        "https://huggingface.co/docs/transformers/en/chat_templating#why-do-some-models-have-multiple-templates",
        "https://huggingface.co/docs/transformers/en/chat_templating#what-template-should-i-use",
        "https://huggingface.co/docs/transformers/en/chat_templating#i-want-to-add-some-chat-templates-how-should-i-get-started",
        "https://huggingface.co/docs/hub/repositories-pull-requests-discussions",
        "https://huggingface.co/docs/transformers/en/chat_templating#advanced-template-writing-tips",
        "https://huggingface.co/docs/transformers/en/chat_templating#trimming-whitespace",
        "https://huggingface.co/docs/transformers/en/chat_templating#special-variables",
        "https://huggingface.co/docs/transformers/en/chat_templating#callable-functions",
        "https://huggingface.co/docs/transformers/en/chat_templating#compatibility-with-non-python-jinja",
        "https://huggingface.co/docs/transformers/en/chat_templating#writing-generation-prompts",
        "https://huggingface.co/docs/transformers/en/chat_templating#writing-and-debugging-larger-templates",
        "https://huggingface.co/docs/transformers/en/chat_templating#writing-templates-for-tools",
        "https://huggingface.co/docs/transformers/en/chat_templating#tool-definitions",
        "https://huggingface.co/docs/transformers/en/chat_templating#tool-calls",
        "https://huggingface.co/docs/transformers/en/chat_templating#tool-responses",
        "https://github.com/huggingface/transformers",
        "https://json-schema.org/learn/getting-started-step-by-step",
        "https://jinja.palletsprojects.com/en/3.1.x/templates/",
        "https://jinja.palletsprojects.com/en/3.1.x/templates/#synopsis",
        "https://jinja.palletsprojects.com/en/3.1.x/templates/#builtin-filters",
        "https://github.com/huggingface/transformers/blob/main/docs/source/en/chat_templating.md"
    ]
}
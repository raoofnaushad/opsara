{
    "id": "6cfc9e6c570d3acedfbab299f84c0ea4",
    "metadata": {
        "id": "6cfc9e6c570d3acedfbab299f84c0ea4",
        "url": "https://huggingface.co/docs/transformers/tokenizer_summary/",
        "title": "Summary of the tokenizers",
        "properties": {
            "description": "We‚Äôre on a journey to advance and democratize artificial intelligence through open source and open science.",
            "keywords": null,
            "author": null,
            "og:title": "Summary of the tokenizers",
            "og:type": "website",
            "og:url": "https://huggingface.co/docs/transformers/tokenizer_summary",
            "og:image": "https://huggingface.co/front/thumbnails/docs/transformers.png",
            "twitter:card": "summary_large_image",
            "twitter:site": "@huggingface",
            "twitter:image": "https://huggingface.co/front/thumbnails/docs/transformers.png"
        }
    },
    "parent_metadata": {
        "id": "6ebceb16d6f238b8b60c8dbfed6bcd9b",
        "url": "https://www.notion.so/LLMs-6ebceb16d6f238b8b60c8dbfed6bcd9b",
        "title": "LLMs",
        "properties": {
            "Type": "Leaf"
        }
    },
    "content": "[![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg) Hugging Face](/)\n\n  * [ Models](/models)\n  * [ Datasets](/datasets)\n  * [ Spaces](/spaces)\n  * [ Posts](/posts)\n  * [ Docs](/docs)\n  * [ Enterprise](/enterprise)\n  * [Pricing](/pricing)\n  * [Log In](/login)\n  * [Sign Up](/join)\n\n\n\nTransformers documentation\n\nSummary of the tokenizers\n\n# Transformers\n\nüè° View all docsAWS Trainium & InferentiaAccelerateAmazon SageMakerArgillaAutoTrainBitsandbytesChat UICompetitionsDataset viewerDatasetsDiffusersDistilabelEvaluateGoogle CloudGoogle TPUsGradioHubHub Python LibraryHugging Face Generative AI Services (HUGS)Huggingface.jsInference API (serverless)Inference Endpoints (dedicated)LeaderboardsLightevalOptimumPEFTSafetensorsSentence TransformersTRLTasksText Embeddings InferenceText Generation InferenceTokenizersTransformersTransformers.jssmolagentstimm\n\nSearch documentation\n\n`‚åòK`\n\nmainv4.48.0v4.47.1v4.46.3v4.45.2v4.44.2v4.43.4v4.42.4v4.41.2v4.40.2v4.39.3v4.38.2v4.37.2v4.36.1v4.35.2v4.34.1v4.33.3v4.32.1v4.31.0v4.30.0v4.29.1v4.28.1v4.27.2v4.26.1v4.25.1v4.24.0v4.23.1v4.22.2v4.21.3v4.20.1v4.19.4v4.18.0v4.17.0v4.16.2v4.15.0v4.14.1v4.13.0v4.12.5v4.11.3v4.10.1v4.9.2v4.8.2v4.7.0v4.6.0v4.5.1v4.4.2v4.3.3v4.2.2v4.1.1v4.0.1v3.5.1v3.4.0v3.3.1v3.2.0v3.1.0v3.0.2v2.11.0v2.10.0v2.9.1v2.8.0v2.7.0v2.6.0v2.5.1v2.4.1v2.3.0v2.2.2v2.1.1v2.0.0v1.2.0v1.1.0v1.0.0doc-builder-html ARDEENESFRHIITJAKOPTTETRZH [ 137,864](https://github.com/huggingface/transformers)\n\nGet started\n\n[ü§ó Transformers ](/docs/transformers/index)[Quick tour ](/docs/transformers/quicktour)[Installation ](/docs/transformers/installation)[Adding a new model to `transformers` ](/docs/transformers/add_new_model)\n\nTutorials\n\n[Run inference with pipelines ](/docs/transformers/pipeline_tutorial)[Write portable code with AutoClass ](/docs/transformers/autoclass_tutorial)[Preprocess data ](/docs/transformers/preprocessing)[Fine-tune a pretrained model ](/docs/transformers/training)[Train with a script ](/docs/transformers/run_scripts)[Set up distributed training with ü§ó Accelerate ](/docs/transformers/accelerate)[Load and train adapters with ü§ó PEFT ](/docs/transformers/peft)[Share your model ](/docs/transformers/model_sharing)[Agents 101 ](/docs/transformers/agents)[Agents, supercharged - Multi-agents, External tools, and more ](/docs/transformers/agents_advanced)[Generation with LLMs ](/docs/transformers/llm_tutorial)[Chatting with Transformers ](/docs/transformers/conversations)\n\nTask Guides\n\nNatural Language Processing\n\nAudio\n\nComputer Vision\n\nMultimodal\n\nGeneration\n\nPrompting\n\nDeveloper guides\n\n[Use fast tokenizers from ü§ó Tokenizers ](/docs/transformers/fast_tokenizers)[Run inference with multilingual models ](/docs/transformers/multilingual)[Use model-specific APIs ](/docs/transformers/create_a_model)[Share a custom model ](/docs/transformers/custom_models)[Chat templates ](/docs/transformers/chat_templating)[Trainer ](/docs/transformers/trainer)[Run training on Amazon SageMaker ](/docs/transformers/sagemaker)[Export to ONNX ](/docs/transformers/serialization)[Export to TFLite ](/docs/transformers/tflite)[Export to TorchScript ](/docs/transformers/torchscript)[Benchmarks ](/docs/transformers/benchmarks)[Notebooks with examples ](/docs/transformers/notebooks)[Community resources ](/docs/transformers/community)[Troubleshoot ](/docs/transformers/troubleshooting)[Interoperability with GGUF files ](/docs/transformers/gguf)[Interoperability with TikToken files ](/docs/transformers/tiktoken)[Modularity in `transformers` ](/docs/transformers/modular_transformers)[Model Hacking (overwriting a class to your usage) ](/docs/transformers/how_to_hack_models)\n\nQuantization Methods\n\n[Getting started ](/docs/transformers/quantization/overview)[bitsandbytes ](/docs/transformers/quantization/bitsandbytes)[GPTQ ](/docs/transformers/quantization/gptq)[AWQ ](/docs/transformers/quantization/awq)[AQLM ](/docs/transformers/quantization/aqlm)[VPTQ ](/docs/transformers/quantization/vptq)[Quanto ](/docs/transformers/quantization/quanto)[EETQ ](/docs/transformers/quantization/eetq)[HIGGS ](/docs/transformers/quantization/higgs)[HQQ ](/docs/transformers/quantization/hqq)[FBGEMM_FP8 ](/docs/transformers/quantization/fbgemm_fp8)[Optimum ](/docs/transformers/quantization/optimum)[TorchAO ](/docs/transformers/quantization/torchao)[BitNet ](/docs/transformers/quantization/bitnet)[compressed-tensors ](/docs/transformers/quantization/compressed_tensors)[Contribute new quantization method ](/docs/transformers/quantization/contribute)\n\nPerformance and scalability\n\n[Overview ](/docs/transformers/performance)[LLM inference optimization ](/docs/transformers/llm_optims)\n\nEfficient training techniques\n\n[Methods and tools for efficient training on a single GPU ](/docs/transformers/perf_train_gpu_one)[Multiple GPUs and parallelism ](/docs/transformers/perf_train_gpu_many)[Fully Sharded Data Parallel ](/docs/transformers/fsdp)[DeepSpeed ](/docs/transformers/deepspeed)[Efficient training on CPU ](/docs/transformers/perf_train_cpu)[Distributed CPU training ](/docs/transformers/perf_train_cpu_many)[Training on TPU with TensorFlow ](/docs/transformers/perf_train_tpu_tf)[PyTorch training on Apple silicon ](/docs/transformers/perf_train_special)[Custom hardware for training ](/docs/transformers/perf_hardware)[Hyperparameter Search using Trainer API ](/docs/transformers/hpo_train)\n\nOptimizing inference\n\n[CPU inference ](/docs/transformers/perf_infer_cpu)[GPU inference ](/docs/transformers/perf_infer_gpu_one)[Multi-GPU inference ](/docs/transformers/perf_infer_gpu_multi)\n\n[Instantiate a big model ](/docs/transformers/big_models)[Debugging ](/docs/transformers/debugging)[XLA Integration for TensorFlow Models ](/docs/transformers/tf_xla)[Optimize inference using `torch.compile()` ](/docs/transformers/perf_torch_compile)\n\nContribute\n\n[How to contribute to ü§ó Transformers? ](/docs/transformers/contributing)[How to add a model to ü§ó Transformers? ](/docs/transformers/add_new_model)[How to add a pipeline to ü§ó Transformers? ](/docs/transformers/add_new_pipeline)[Testing ](/docs/transformers/testing)[Checks on a Pull Request ](/docs/transformers/pr_checks)\n\nConceptual guides\n\n[Philosophy ](/docs/transformers/philosophy)[Glossary ](/docs/transformers/glossary)[What ü§ó Transformers can do ](/docs/transformers/task_summary)[How ü§ó Transformers solve tasks ](/docs/transformers/tasks_explained)[The Transformer model family ](/docs/transformers/model_summary)[Summary of the tokenizers ](/docs/transformers/tokenizer_summary)[Attention mechanisms ](/docs/transformers/attention)[Padding and truncation ](/docs/transformers/pad_truncation)[BERTology ](/docs/transformers/bertology)[Perplexity of fixed-length models ](/docs/transformers/perplexity)[Pipelines for webserver inference ](/docs/transformers/pipeline_webserver)[Model training anatomy ](/docs/transformers/model_memory_anatomy)[Getting the most out of LLMs ](/docs/transformers/llm_tutorial_optimization)\n\nAPI\n\nMain Classes\n\n[Agents and Tools ](/docs/transformers/main_classes/agent)[Auto Classes ](/docs/transformers/model_doc/auto)[Backbones ](/docs/transformers/main_classes/backbones)[Callbacks ](/docs/transformers/main_classes/callback)[Configuration ](/docs/transformers/main_classes/configuration)[Data Collator ](/docs/transformers/main_classes/data_collator)[Keras callbacks ](/docs/transformers/main_classes/keras_callbacks)[Logging ](/docs/transformers/main_classes/logging)[Models ](/docs/transformers/main_classes/model)[Text Generation ](/docs/transformers/main_classes/text_generation)[ONNX ](/docs/transformers/main_classes/onnx)[Optimization ](/docs/transformers/main_classes/optimizer_schedules)[Model outputs ](/docs/transformers/main_classes/output)[Pipelines ](/docs/transformers/main_classes/pipelines)[Processors ](/docs/transformers/main_classes/processors)[Quantization ](/docs/transformers/main_classes/quantization)[Tokenizer ](/docs/transformers/main_classes/tokenizer)[Trainer ](/docs/transformers/main_classes/trainer)[DeepSpeed ](/docs/transformers/main_classes/deepspeed)[ExecuTorch ](/docs/transformers/main_classes/executorch)[Feature Extractor ](/docs/transformers/main_classes/feature_extractor)[Image Processor ](/docs/transformers/main_classes/image_processor)\n\nModels\n\nText models\n\nVision models\n\nAudio models\n\nVideo models\n\nMultimodal models\n\nReinforcement learning models\n\nTime series models\n\nGraph models\n\nInternal Helpers\n\n[Custom Layers and Utilities ](/docs/transformers/internal/modeling_utils)[Utilities for pipelines ](/docs/transformers/internal/pipelines_utils)[Utilities for Tokenizers ](/docs/transformers/internal/tokenization_utils)[Utilities for Trainer ](/docs/transformers/internal/trainer_utils)[Utilities for Generation ](/docs/transformers/internal/generation_utils)[Utilities for Image Processors ](/docs/transformers/internal/image_processing_utils)[Utilities for Audio processing ](/docs/transformers/internal/audio_utils)[General Utilities ](/docs/transformers/internal/file_utils)[Utilities for Time Series ](/docs/transformers/internal/time_series_utils)\n\n![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg)\n\nJoin the Hugging Face community\n\nand get access to the augmented documentation experience \n\nCollaborate on models, datasets and Spaces \n\nFaster examples with accelerated inference \n\nSwitch between documentation themes \n\n[Sign Up](/join)\n\nto get started\n\n# [](#summary-of-the-tokenizers) Summary of the tokenizers\n\n![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)\n\n![Open In Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)\n\nOn this page, we will have a closer look at tokenization.\n\nAs we saw in [the preprocessing tutorial](preprocessing), tokenizing a text is splitting it into words or subwords, which then are converted to ids through a look-up table. Converting words or subwords to ids is straightforward, so in this summary, we will focus on splitting a text into words or subwords (i.e. tokenizing a text). More specifically, we will look at the three main types of tokenizers used in ü§ó Transformers: [Byte-Pair Encoding (BPE)](#byte-pair-encoding), [WordPiece](#wordpiece), and [SentencePiece](#sentencepiece), and show examples of which tokenizer type is used by which model.\n\nNote that on each model page, you can look at the documentation of the associated tokenizer to know which tokenizer type was used by the pretrained model. For instance, if we look at [BertTokenizer](/docs/transformers/v4.48.0/en/model_doc/bert#transformers.BertTokenizer), we can see that the model uses [WordPiece](#wordpiece).\n\n## [](#introduction) Introduction\n\nSplitting a text into smaller chunks is a task that is harder than it looks, and there are multiple ways of doing so. For instance, let‚Äôs look at the sentence `\"Don't you love ü§ó Transformers? We sure do.\"`\n\nA simple way of tokenizing this text is to split it by spaces, which would give:\n\nCopied\n\n```\n[\"Don't\", \"you\", \"love\", \"ü§ó\", \"Transformers?\", \"We\", \"sure\", \"do.\"]\n```\n\nThis is a sensible first step, but if we look at the tokens `\"Transformers?\"` and `\"do.\"`, we notice that the punctuation is attached to the words `\"Transformer\"` and `\"do\"`, which is suboptimal. We should take the punctuation into account so that a model does not have to learn a different representation of a word and every possible punctuation symbol that could follow it, which would explode the number of representations the model has to learn. Taking punctuation into account, tokenizing our exemplary text would give:\n\nCopied\n\n```\n[\"Don\", \"'\", \"t\", \"you\", \"love\", \"ü§ó\", \"Transformers\", \"?\", \"We\", \"sure\", \"do\", \".\"]\n```\n\nBetter. However, it is disadvantageous, how the tokenization dealt with the word `\"Don't\"`. `\"Don't\"` stands for `\"do not\"`, so it would be better tokenized as `[\"Do\", \"n't\"]`. This is where things start getting complicated, and part of the reason each model has its own tokenizer type. Depending on the rules we apply for tokenizing a text, a different tokenized output is generated for the same text. A pretrained model only performs properly if you feed it an input that was tokenized with the same rules that were used to tokenize its training data.\n\n[spaCy](https://spacy.io/) and [Moses](http://www.statmt.org/moses/?n=Development.GetStarted) are two popular rule-based tokenizers. Applying them on our example, _spaCy_ and _Moses_ would output something like:\n\nCopied\n\n```\n[\"Do\", \"n't\", \"you\", \"love\", \"ü§ó\", \"Transformers\", \"?\", \"We\", \"sure\", \"do\", \".\"]\n```\n\nAs can be seen space and punctuation tokenization, as well as rule-based tokenization, is used here. Space and punctuation tokenization and rule-based tokenization are both examples of word tokenization, which is loosely defined as splitting sentences into words. While it‚Äôs the most intuitive way to split texts into smaller chunks, this tokenization method can lead to problems for massive text corpora. In this case, space and punctuation tokenization usually generates a very big vocabulary (the set of all unique words and tokens used). _E.g._ , [Transformer XL](model_doc/transfo-xl) uses space and punctuation tokenization, resulting in a vocabulary size of 267,735!\n\nSuch a big vocabulary size forces the model to have an enormous embedding matrix as the input and output layer, which causes both an increased memory and time complexity. In general, transformers models rarely have a vocabulary size greater than 50,000, especially if they are pretrained only on a single language.\n\nSo if simple space and punctuation tokenization is unsatisfactory, why not simply tokenize on characters?\n\nWhile character tokenization is very simple and would greatly reduce memory and time complexity it makes it much harder for the model to learn meaningful input representations. _E.g._ learning a meaningful context-independent representation for the letter `\"t\"` is much harder than learning a context-independent representation for the word `\"today\"`. Therefore, character tokenization is often accompanied by a loss of performance. So to get the best of both worlds, transformers models use a hybrid between word-level and character-level tokenization called **subword** tokenization.\n\n## [](#subword-tokenization) Subword tokenization\n\nSubword tokenization algorithms rely on the principle that frequently used words should not be split into smaller subwords, but rare words should be decomposed into meaningful subwords. For instance `\"annoyingly\"` might be considered a rare word and could be decomposed into `\"annoying\"` and `\"ly\"`. Both `\"annoying\"` and `\"ly\"` as stand-alone subwords would appear more frequently while at the same time the meaning of `\"annoyingly\"` is kept by the composite meaning of `\"annoying\"` and `\"ly\"`. This is especially useful in agglutinative languages such as Turkish, where you can form (almost) arbitrarily long complex words by stringing together subwords.\n\nSubword tokenization allows the model to have a reasonable vocabulary size while being able to learn meaningful context-independent representations. In addition, subword tokenization enables the model to process words it has never seen before, by decomposing them into known subwords. For instance, the [BertTokenizer](/docs/transformers/v4.48.0/en/model_doc/bert#transformers.BertTokenizer) tokenizes `\"I have a new GPU!\"` as follows:\n\nCopied\n\n```\n>>> from transformers import BertTokenizer >>> tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\") >>> tokenizer.tokenize(\"I have a new GPU!\") [\"i\", \"have\", \"a\", \"new\", \"gp\", \"##u\", \"!\"]\n```\n\nBecause we are considering the uncased model, the sentence was lowercased first. We can see that the words `[\"i\", \"have\", \"a\", \"new\"]` are present in the tokenizer‚Äôs vocabulary, but the word `\"gpu\"` is not. Consequently, the tokenizer splits `\"gpu\"` into known subwords: `[\"gp\" and \"##u\"]`. `\"##\"` means that the rest of the token should be attached to the previous one, without space (for decoding or reversal of the tokenization).\n\nAs another example, [XLNetTokenizer](/docs/transformers/v4.48.0/en/model_doc/xlnet#transformers.XLNetTokenizer) tokenizes our previously exemplary text as follows:\n\nCopied\n\n```\n>>> from transformers import XLNetTokenizer >>> tokenizer = XLNetTokenizer.from_pretrained(\"xlnet/xlnet-base-cased\") >>> tokenizer.tokenize(\"Don't you love ü§ó Transformers? We sure do.\") [\"‚ñÅDon\", \"'\", \"t\", \"‚ñÅyou\", \"‚ñÅlove\", \"‚ñÅ\", \"ü§ó\", \"‚ñÅ\", \"Transform\", \"ers\", \"?\", \"‚ñÅWe\", \"‚ñÅsure\", \"‚ñÅdo\", \".\"]\n```\n\nWe‚Äôll get back to the meaning of those `\"‚ñÅ\"` when we look at [SentencePiece](#sentencepiece). As one can see, the rare word `\"Transformers\"` has been split into the more frequent subwords `\"Transform\"` and `\"ers\"`.\n\nLet‚Äôs now look at how the different subword tokenization algorithms work. Note that all of those tokenization algorithms rely on some form of training which is usually done on the corpus the corresponding model will be trained on.\n\n### [](#byte-pair-encoding-bpe) Byte-Pair Encoding (BPE)\n\nByte-Pair Encoding (BPE) was introduced in [Neural Machine Translation of Rare Words with Subword Units (Sennrich et al., 2015)](https://arxiv.org/abs/1508.07909). BPE relies on a pre-tokenizer that splits the training data into words. Pretokenization can be as simple as space tokenization, e.g. [GPT-2](model_doc/gpt2), [RoBERTa](model_doc/roberta). More advanced pre-tokenization include rule-based tokenization, e.g. [XLM](model_doc/xlm), [FlauBERT](model_doc/flaubert) which uses Moses for most languages, or [GPT](model_doc/openai-gpt) which uses spaCy and ftfy, to count the frequency of each word in the training corpus.\n\nAfter pre-tokenization, a set of unique words has been created and the frequency with which each word occurred in the training data has been determined. Next, BPE creates a base vocabulary consisting of all symbols that occur in the set of unique words and learns merge rules to form a new symbol from two symbols of the base vocabulary. It does so until the vocabulary has attained the desired vocabulary size. Note that the desired vocabulary size is a hyperparameter to define before training the tokenizer.\n\nAs an example, let‚Äôs assume that after pre-tokenization, the following set of words including their frequency has been determined:\n\nCopied\n\n```\n(\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)\n```\n\nConsequently, the base vocabulary is `[\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\"]`. Splitting all words into symbols of the base vocabulary, we obtain:\n\nCopied\n\n```\n(\"h\" \"u\" \"g\", 10), (\"p\" \"u\" \"g\", 5), (\"p\" \"u\" \"n\", 12), (\"b\" \"u\" \"n\", 4), (\"h\" \"u\" \"g\" \"s\", 5)\n```\n\nBPE then counts the frequency of each possible symbol pair and picks the symbol pair that occurs most frequently. In the example above `\"h\"` followed by `\"u\"` is present _10 + 5 = 15_ times (10 times in the 10 occurrences of `\"hug\"`, 5 times in the 5 occurrences of `\"hugs\"`). However, the most frequent symbol pair is `\"u\"` followed by `\"g\"`, occurring _10 + 5 + 5 = 20_ times in total. Thus, the first merge rule the tokenizer learns is to group all `\"u\"` symbols followed by a `\"g\"` symbol together. Next, `\"ug\"` is added to the vocabulary. The set of words then becomes\n\nCopied\n\n```\n(\"h\" \"ug\", 10), (\"p\" \"ug\", 5), (\"p\" \"u\" \"n\", 12), (\"b\" \"u\" \"n\", 4), (\"h\" \"ug\" \"s\", 5)\n```\n\nBPE then identifies the next most common symbol pair. It‚Äôs `\"u\"` followed by `\"n\"`, which occurs 16 times. `\"u\"`, `\"n\"` is merged to `\"un\"` and added to the vocabulary. The next most frequent symbol pair is `\"h\"` followed by `\"ug\"`, occurring 15 times. Again the pair is merged and `\"hug\"` can be added to the vocabulary.\n\nAt this stage, the vocabulary is `[\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\", \"hug\"]` and our set of unique words is represented as\n\nCopied\n\n```\n(\"hug\", 10), (\"p\" \"ug\", 5), (\"p\" \"un\", 12), (\"b\" \"un\", 4), (\"hug\" \"s\", 5)\n```\n\nAssuming, that the Byte-Pair Encoding training would stop at this point, the learned merge rules would then be applied to new words (as long as those new words do not include symbols that were not in the base vocabulary). For instance, the word `\"bug\"` would be tokenized to `[\"b\", \"ug\"]` but `\"mug\"` would be tokenized as `[\"<unk>\", \"ug\"]` since the symbol `\"m\"` is not in the base vocabulary. In general, single letters such as `\"m\"` are not replaced by the `\"<unk>\"` symbol because the training data usually includes at least one occurrence of each letter, but it is likely to happen for very special characters like emojis.\n\nAs mentioned earlier, the vocabulary size, _i.e._ the base vocabulary size + the number of merges, is a hyperparameter to choose. For instance [GPT](model_doc/openai-gpt) has a vocabulary size of 40,478 since they have 478 base characters and chose to stop training after 40,000 merges.\n\n#### [](#byte-level-bpe) Byte-level BPE\n\nA base vocabulary that includes all possible base characters can be quite large if _e.g._ all unicode characters are considered as base characters. To have a better base vocabulary, [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) uses bytes as the base vocabulary, which is a clever trick to force the base vocabulary to be of size 256 while ensuring that every base character is included in the vocabulary. With some additional rules to deal with punctuation, the GPT2‚Äôs tokenizer can tokenize every text without the need for the <unk> symbol. [GPT-2](model_doc/gpt) has a vocabulary size of 50,257, which corresponds to the 256 bytes base tokens, a special end-of-text token and the symbols learned with 50,000 merges.\n\n### [](#wordpiece) WordPiece\n\nWordPiece is the subword tokenization algorithm used for [BERT](model_doc/bert), [DistilBERT](model_doc/distilbert), and [Electra](model_doc/electra). The algorithm was outlined in [Japanese and Korean Voice Search (Schuster et al., 2012)](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf) and is very similar to BPE. WordPiece first initializes the vocabulary to include every character present in the training data and progressively learns a given number of merge rules. In contrast to BPE, WordPiece does not choose the most frequent symbol pair, but the one that maximizes the likelihood of the training data once added to the vocabulary.\n\nSo what does this mean exactly? Referring to the previous example, maximizing the likelihood of the training data is equivalent to finding the symbol pair, whose probability divided by the probabilities of its first symbol followed by its second symbol is the greatest among all symbol pairs. _E.g._ `\"u\"`, followed by `\"g\"` would have only been merged if the probability of `\"ug\"` divided by `\"u\"`, `\"g\"` would have been greater than for any other symbol pair. Intuitively, WordPiece is slightly different to BPE in that it evaluates what it _loses_ by merging two symbols to ensure it‚Äôs _worth it_.\n\n### [](#unigram) Unigram\n\nUnigram is a subword tokenization algorithm introduced in [Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates (Kudo, 2018)](https://arxiv.org/pdf/1804.10959.pdf). In contrast to BPE or WordPiece, Unigram initializes its base vocabulary to a large number of symbols and progressively trims down each symbol to obtain a smaller vocabulary. The base vocabulary could for instance correspond to all pre-tokenized words and the most common substrings. Unigram is not used directly for any of the models in the transformers, but it‚Äôs used in conjunction with [SentencePiece](#sentencepiece).\n\nAt each training step, the Unigram algorithm defines a loss (often defined as the log-likelihood) over the training data given the current vocabulary and a unigram language model. Then, for each symbol in the vocabulary, the algorithm computes how much the overall loss would increase if the symbol was to be removed from the vocabulary. Unigram then removes p (with p usually being 10% or 20%) percent of the symbols whose loss increase is the lowest, _i.e._ those symbols that least affect the overall loss over the training data. This process is repeated until the vocabulary has reached the desired size. The Unigram algorithm always keeps the base characters so that any word can be tokenized.\n\nBecause Unigram is not based on merge rules (in contrast to BPE and WordPiece), the algorithm has several ways of tokenizing new text after training. As an example, if a trained Unigram tokenizer exhibits the vocabulary:\n\nCopied\n\n```\n[\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\", \"hug\"],\n```\n\n`\"hugs\"` could be tokenized both as `[\"hug\", \"s\"]`, `[\"h\", \"ug\", \"s\"]` or `[\"h\", \"u\", \"g\", \"s\"]`. So which one to choose? Unigram saves the probability of each token in the training corpus on top of saving the vocabulary so that the probability of each possible tokenization can be computed after training. The algorithm simply picks the most likely tokenization in practice, but also offers the possibility to sample a possible tokenization according to their probabilities.\n\nThose probabilities are defined by the loss the tokenizer is trained on. Assuming that the training data consists of the wordsx1,‚Ä¶,xNx_{1}, \\dots, x_{N}x1‚Äã,‚Ä¶,xN‚Äã and that the set of all possible tokenizations for a wordxix_{i}xi‚Äã is defined asS(xi)S(x_{i})S(xi‚Äã), then the overall loss is defined as L=‚àí‚àëi=1Nlog‚Å°(‚àëx‚ààS(xi)p(x))\\mathcal{L} = -\\sum_{i=1}^{N} \\log \\left ( \\sum_{x \\in S(x_{i})} p(x) \\right )L=‚àíi=1‚àëN‚Äãlog‚Äãx‚ààS(xi‚Äã)‚àë‚Äãp(x)‚Äã\n\n### [](#sentencepiece) SentencePiece\n\nAll tokenization algorithms described so far have the same problem: It is assumed that the input text uses spaces to separate words. However, not all languages use spaces to separate words. One possible solution is to use language specific pre-tokenizers, _e.g._ [XLM](model_doc/xlm) uses a specific Chinese, Japanese, and Thai pre-tokenizer. To solve this problem more generally, [SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing (Kudo et al., 2018)](https://arxiv.org/pdf/1808.06226.pdf) treats the input as a raw input stream, thus including the space in the set of characters to use. It then uses the BPE or unigram algorithm to construct the appropriate vocabulary.\n\nThe [XLNetTokenizer](/docs/transformers/v4.48.0/en/model_doc/xlnet#transformers.XLNetTokenizer) uses SentencePiece for example, which is also why in the example earlier the `\"‚ñÅ\"` character was included in the vocabulary. Decoding with SentencePiece is very easy since all tokens can just be concatenated and `\"‚ñÅ\"` is replaced by a space.\n\nAll transformers models in the library that use SentencePiece use it in combination with unigram. Examples of models using SentencePiece are [ALBERT](model_doc/albert), [XLNet](model_doc/xlnet), [Marian](model_doc/marian), and [T5](model_doc/t5).\n\n[< > Update on GitHub](https://github.com/huggingface/transformers/blob/main/docs/source/en/tokenizer_summary.md)\n\n[‚ÜêThe Transformer model family](/docs/transformers/model_summary) [Attention mechanisms‚Üí](/docs/transformers/attention)\n\n[Summary of the tokenizers](#summary-of-the-tokenizers) [Introduction](#introduction) [Subword tokenization](#subword-tokenization) [Byte-Pair Encoding (BPE)](#byte-pair-encoding-bpe) [Byte-level BPE](#byte-level-bpe)[WordPiece](#wordpiece) [Unigram](#unigram) [SentencePiece](#sentencepiece)\n",
    "content_quality_score": 1.0,
    "summary": null,
    "child_urls": [
        "https://huggingface.co/",
        "https://huggingface.co/models",
        "https://huggingface.co/datasets",
        "https://huggingface.co/spaces",
        "https://huggingface.co/posts",
        "https://huggingface.co/docs",
        "https://huggingface.co/enterprise",
        "https://huggingface.co/pricing",
        "https://huggingface.co/login",
        "https://huggingface.co/join",
        "https://huggingface.co/docs/transformers/index",
        "https://huggingface.co/docs/transformers/quicktour",
        "https://huggingface.co/docs/transformers/installation",
        "https://huggingface.co/docs/transformers/add_new_model",
        "https://huggingface.co/docs/transformers/pipeline_tutorial",
        "https://huggingface.co/docs/transformers/autoclass_tutorial",
        "https://huggingface.co/docs/transformers/preprocessing",
        "https://huggingface.co/docs/transformers/training",
        "https://huggingface.co/docs/transformers/run_scripts",
        "https://huggingface.co/docs/transformers/accelerate",
        "https://huggingface.co/docs/transformers/peft",
        "https://huggingface.co/docs/transformers/model_sharing",
        "https://huggingface.co/docs/transformers/agents",
        "https://huggingface.co/docs/transformers/agents_advanced",
        "https://huggingface.co/docs/transformers/llm_tutorial",
        "https://huggingface.co/docs/transformers/conversations",
        "https://huggingface.co/docs/transformers/fast_tokenizers",
        "https://huggingface.co/docs/transformers/multilingual",
        "https://huggingface.co/docs/transformers/create_a_model",
        "https://huggingface.co/docs/transformers/custom_models",
        "https://huggingface.co/docs/transformers/chat_templating",
        "https://huggingface.co/docs/transformers/trainer",
        "https://huggingface.co/docs/transformers/sagemaker",
        "https://huggingface.co/docs/transformers/serialization",
        "https://huggingface.co/docs/transformers/tflite",
        "https://huggingface.co/docs/transformers/torchscript",
        "https://huggingface.co/docs/transformers/benchmarks",
        "https://huggingface.co/docs/transformers/notebooks",
        "https://huggingface.co/docs/transformers/community",
        "https://huggingface.co/docs/transformers/troubleshooting",
        "https://huggingface.co/docs/transformers/gguf",
        "https://huggingface.co/docs/transformers/tiktoken",
        "https://huggingface.co/docs/transformers/modular_transformers",
        "https://huggingface.co/docs/transformers/how_to_hack_models",
        "https://huggingface.co/docs/transformers/quantization/overview",
        "https://huggingface.co/docs/transformers/quantization/bitsandbytes",
        "https://huggingface.co/docs/transformers/quantization/gptq",
        "https://huggingface.co/docs/transformers/quantization/awq",
        "https://huggingface.co/docs/transformers/quantization/aqlm",
        "https://huggingface.co/docs/transformers/quantization/vptq",
        "https://huggingface.co/docs/transformers/quantization/quanto",
        "https://huggingface.co/docs/transformers/quantization/eetq",
        "https://huggingface.co/docs/transformers/quantization/higgs",
        "https://huggingface.co/docs/transformers/quantization/hqq",
        "https://huggingface.co/docs/transformers/quantization/fbgemm_fp8",
        "https://huggingface.co/docs/transformers/quantization/optimum",
        "https://huggingface.co/docs/transformers/quantization/torchao",
        "https://huggingface.co/docs/transformers/quantization/bitnet",
        "https://huggingface.co/docs/transformers/quantization/compressed_tensors",
        "https://huggingface.co/docs/transformers/quantization/contribute",
        "https://huggingface.co/docs/transformers/performance",
        "https://huggingface.co/docs/transformers/llm_optims",
        "https://huggingface.co/docs/transformers/perf_train_gpu_one",
        "https://huggingface.co/docs/transformers/perf_train_gpu_many",
        "https://huggingface.co/docs/transformers/fsdp",
        "https://huggingface.co/docs/transformers/deepspeed",
        "https://huggingface.co/docs/transformers/perf_train_cpu",
        "https://huggingface.co/docs/transformers/perf_train_cpu_many",
        "https://huggingface.co/docs/transformers/perf_train_tpu_tf",
        "https://huggingface.co/docs/transformers/perf_train_special",
        "https://huggingface.co/docs/transformers/perf_hardware",
        "https://huggingface.co/docs/transformers/hpo_train",
        "https://huggingface.co/docs/transformers/perf_infer_cpu",
        "https://huggingface.co/docs/transformers/perf_infer_gpu_one",
        "https://huggingface.co/docs/transformers/perf_infer_gpu_multi",
        "https://huggingface.co/docs/transformers/big_models",
        "https://huggingface.co/docs/transformers/debugging",
        "https://huggingface.co/docs/transformers/tf_xla",
        "https://huggingface.co/docs/transformers/perf_torch_compile",
        "https://huggingface.co/docs/transformers/contributing",
        "https://huggingface.co/docs/transformers/add_new_pipeline",
        "https://huggingface.co/docs/transformers/testing",
        "https://huggingface.co/docs/transformers/pr_checks",
        "https://huggingface.co/docs/transformers/philosophy",
        "https://huggingface.co/docs/transformers/glossary",
        "https://huggingface.co/docs/transformers/task_summary",
        "https://huggingface.co/docs/transformers/tasks_explained",
        "https://huggingface.co/docs/transformers/model_summary",
        "https://huggingface.co/docs/transformers/tokenizer_summary",
        "https://huggingface.co/docs/transformers/attention",
        "https://huggingface.co/docs/transformers/pad_truncation",
        "https://huggingface.co/docs/transformers/bertology",
        "https://huggingface.co/docs/transformers/perplexity",
        "https://huggingface.co/docs/transformers/pipeline_webserver",
        "https://huggingface.co/docs/transformers/model_memory_anatomy",
        "https://huggingface.co/docs/transformers/llm_tutorial_optimization",
        "https://huggingface.co/docs/transformers/main_classes/agent",
        "https://huggingface.co/docs/transformers/model_doc/auto",
        "https://huggingface.co/docs/transformers/main_classes/backbones",
        "https://huggingface.co/docs/transformers/main_classes/callback",
        "https://huggingface.co/docs/transformers/main_classes/configuration",
        "https://huggingface.co/docs/transformers/main_classes/data_collator",
        "https://huggingface.co/docs/transformers/main_classes/keras_callbacks",
        "https://huggingface.co/docs/transformers/main_classes/logging",
        "https://huggingface.co/docs/transformers/main_classes/model",
        "https://huggingface.co/docs/transformers/main_classes/text_generation",
        "https://huggingface.co/docs/transformers/main_classes/onnx",
        "https://huggingface.co/docs/transformers/main_classes/optimizer_schedules",
        "https://huggingface.co/docs/transformers/main_classes/output",
        "https://huggingface.co/docs/transformers/main_classes/pipelines",
        "https://huggingface.co/docs/transformers/main_classes/processors",
        "https://huggingface.co/docs/transformers/main_classes/quantization",
        "https://huggingface.co/docs/transformers/main_classes/tokenizer",
        "https://huggingface.co/docs/transformers/main_classes/trainer",
        "https://huggingface.co/docs/transformers/main_classes/deepspeed",
        "https://huggingface.co/docs/transformers/main_classes/executorch",
        "https://huggingface.co/docs/transformers/main_classes/feature_extractor",
        "https://huggingface.co/docs/transformers/main_classes/image_processor",
        "https://huggingface.co/docs/transformers/internal/modeling_utils",
        "https://huggingface.co/docs/transformers/internal/pipelines_utils",
        "https://huggingface.co/docs/transformers/internal/tokenization_utils",
        "https://huggingface.co/docs/transformers/internal/trainer_utils",
        "https://huggingface.co/docs/transformers/internal/generation_utils",
        "https://huggingface.co/docs/transformers/internal/image_processing_utils",
        "https://huggingface.co/docs/transformers/internal/audio_utils",
        "https://huggingface.co/docs/transformers/internal/file_utils",
        "https://huggingface.co/docs/transformers/internal/time_series_utils",
        "https://huggingface.co/docs/transformers/tokenizer_summary/#summary-of-the-tokenizers",
        "https://huggingface.co/docs/transformers/tokenizer_summary/preprocessing",
        "https://huggingface.co/docs/transformers/tokenizer_summary/#byte-pair-encoding",
        "https://huggingface.co/docs/transformers/tokenizer_summary/#wordpiece",
        "https://huggingface.co/docs/transformers/tokenizer_summary/#sentencepiece",
        "https://huggingface.co/docs/transformers/v4.48.0/en/model_doc/bert#transformers.BertTokenizer",
        "https://huggingface.co/docs/transformers/tokenizer_summary/#introduction",
        "https://huggingface.co/docs/transformers/tokenizer_summary/model_doc/transfo-xl",
        "https://huggingface.co/docs/transformers/tokenizer_summary/#subword-tokenization",
        "https://huggingface.co/docs/transformers/v4.48.0/en/model_doc/xlnet#transformers.XLNetTokenizer",
        "https://huggingface.co/docs/transformers/tokenizer_summary/#byte-pair-encoding-bpe",
        "https://huggingface.co/docs/transformers/tokenizer_summary/model_doc/gpt2",
        "https://huggingface.co/docs/transformers/tokenizer_summary/model_doc/roberta",
        "https://huggingface.co/docs/transformers/tokenizer_summary/model_doc/xlm",
        "https://huggingface.co/docs/transformers/tokenizer_summary/model_doc/flaubert",
        "https://huggingface.co/docs/transformers/tokenizer_summary/model_doc/openai-gpt",
        "https://huggingface.co/docs/transformers/tokenizer_summary/#byte-level-bpe",
        "https://huggingface.co/docs/transformers/tokenizer_summary/model_doc/gpt",
        "https://huggingface.co/docs/transformers/tokenizer_summary/model_doc/bert",
        "https://huggingface.co/docs/transformers/tokenizer_summary/model_doc/distilbert",
        "https://huggingface.co/docs/transformers/tokenizer_summary/model_doc/electra",
        "https://huggingface.co/docs/transformers/tokenizer_summary/#unigram",
        "https://huggingface.co/docs/transformers/tokenizer_summary/model_doc/albert",
        "https://huggingface.co/docs/transformers/tokenizer_summary/model_doc/xlnet",
        "https://huggingface.co/docs/transformers/tokenizer_summary/model_doc/marian",
        "https://huggingface.co/docs/transformers/tokenizer_summary/model_doc/t5",
        "https://github.com/huggingface/transformers",
        "https://spacy.io/",
        "http://www.statmt.org/moses/?n=Development.GetStarted",
        "https://arxiv.org/abs/1508.07909",
        "https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf",
        "https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf",
        "https://arxiv.org/pdf/1804.10959.pdf",
        "https://arxiv.org/pdf/1808.06226.pdf",
        "https://github.com/huggingface/transformers/blob/main/docs/source/en/tokenizer_summary.md"
    ]
}
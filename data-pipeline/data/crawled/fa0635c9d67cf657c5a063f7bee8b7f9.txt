•

NaN / NaN

•

NaN / NaN

Play (k) 

Back  [ ](/ "YouTube Home") RO 

Skip navigation

Search 

Search 

[Sign in](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3Dhttps%253A%252F%252Fwww.youtube.com%252Fwatch%253Fapp%253Ddesktop%2526v%253DkCc8FmEb1nY%25252F&hl=en-GB&ec=65620)

[ ](/ "YouTube Home") RO 

Let's build GPT: from scratch, in code, spelled out.

Search

Watch Later

Share

Copy link

Info

Shopping

Tap to unmute

2x

If playback doesn't begin shortly, try restarting your device.

•

Up next

LiveUpcoming

CancelPlay now

You're signed out

Videos that you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.

CancelConfirm

Share

Include playlist

An error occurred while retrieving sharing information. Please try again later.

0:00

[](https://www.youtube.com/watch?v=zjkBMFhNj_g "Next \(SHIFT+n\)")

0:00 / 1:56:19•Watch full videoLive

•

intro: ChatGPT, Transformers, nanoGPT, Shakespeare

•

Scroll for details

#  Let's build GPT: from scratch, in code, spelled out.

[![](https://yt3.ggpht.com/ytc/AIdro_nDvyq2NoPL626bk1IbxQ94SfQsD-B0qgZchghtQNkLWoEz=s48-c-k-c0x00ffffff-no-rj)](/@AndrejKarpathy)

[Andrej Karpathy](/@AndrejKarpathy)

Andrej Karpathy 

606K subscribers

<__slot-el>

Subscribe

<__slot-el>

Subscribed

117K

Share

Download

Download 

Save

5M views2 years ago

5,098,877 views • 17 Jan 2023 

Show less 

We build a Generatively Pretrained Transformer (GPT), following the paper "Attention is All You Need" and OpenAI's GPT-2 / GPT-3. We talk about connections to ChatGPT, which has taken the world by storm. We watch GitHub Copilot, itself a GPT, help us write a GPT (meta :D!) . I recommend people watch the earlier makemore videos to get comfortable with the autoregressive language modeling framework and basics of tensors and PyTorch nn, which we take for granted in this video.…...more 

...more 

## 

Chapters

View all

#### [intro: ChatGPT, Transformers, nanoGPT, Shakespeare intro: ChatGPT, Transformers, nanoGPT, Shakespeare 0:00 ](/watch?v=kCc8FmEb1nY&t=0s)

#### [intro: ChatGPT, Transformers, nanoGPT, Shakespeare ](/watch?v=kCc8FmEb1nY&t=0s)

0:00

#### [reading and exploring the data reading and exploring the data 7:52 ](/watch?v=kCc8FmEb1nY&t=472s)

#### [reading and exploring the data ](/watch?v=kCc8FmEb1nY&t=472s)

7:52

#### [tokenization, train/val split tokenization, train/val split 9:28 ](/watch?v=kCc8FmEb1nY&t=568s)

#### [tokenization, train/val split ](/watch?v=kCc8FmEb1nY&t=568s)

9:28

#### [data loader: batches of chunks of data data loader: batches of chunks of data 14:27 ](/watch?v=kCc8FmEb1nY&t=867s)

#### [data loader: batches of chunks of data ](/watch?v=kCc8FmEb1nY&t=867s)

14:27

#### [simplest baseline: bigram language model, loss, generation simplest baseline: bigram language model, loss, generation 22:11 ](/watch?v=kCc8FmEb1nY&t=1331s)

#### [simplest baseline: bigram language model, loss, generation ](/watch?v=kCc8FmEb1nY&t=1331s)

22:11

#### [training the bigram model training the bigram model 34:53 ](/watch?v=kCc8FmEb1nY&t=2093s)

#### [training the bigram model ](/watch?v=kCc8FmEb1nY&t=2093s)

34:53

#### [port our code to a script port our code to a script 38:00 ](/watch?v=kCc8FmEb1nY&t=2280s)

#### [port our code to a script ](/watch?v=kCc8FmEb1nY&t=2280s)

38:00

#### [version 1: averaging past context with for loops, the weakest form of aggregation version 1: averaging past context with for loops, the weakest form of aggregation 42:13 ](/watch?v=kCc8FmEb1nY&t=2533s)

#### [version 1: averaging past context with for loops, the weakest form of aggregation ](/watch?v=kCc8FmEb1nY&t=2533s)

42:13

Corrections

View all

[ 57:00 ](/watch?v=kCc8FmEb1nY&t=3420s)

Oops "tokens from the _future_ cannot communicate", not "past". Sorry! :)

Transcript

Follow along using the transcript.

Show transcript

### [Andrej Karpathy 606K subscribers  ](/@AndrejKarpathy)

[Videos](/channel/UCXUPKJO5MZQN11PqgIvyuvQ/videos)

[About](/channel/UCXUPKJO5MZQN11PqgIvyuvQ/about)

[Videos](/channel/UCXUPKJO5MZQN11PqgIvyuvQ/videos)[About](/channel/UCXUPKJO5MZQN11PqgIvyuvQ/about)[Eureka X](https://www.youtube.com/redirect?event=Watch_SD_EP&redir_token=QUFFLUhqbmdiMkgwcU54U1I0VjlFT1h5SG9ubzVNd3NUUXxBQ3Jtc0ttcWFodGlpU1B6SEl2ZXBJSzV4a2RwUklUZU95OEp6THQyMG5zbzV1SVltMVZBeEdsRmJScWVlYi1HRWlRWUZCbXkxbnU4dXdzVnJ6M04xQlNwbS1wdDZFbU1QR1pOSFdSRVN0MXQ3TDlTZXZnWGxhdw&q=https%3A%2F%2Ftwitter.com%2FEurekaLabsAI)[Andrej X](https://www.youtube.com/redirect?event=Watch_SD_EP&redir_token=QUFFLUhqa29nSjhuZUdYS2Ffd3hBWk5GZDU1b3F3cl8yZ3xBQ3Jtc0ttdDJzamxnQU5QYVBwUTcxM3NEbkRUcGdZbjdicUhYcG9kN3RqUmpBQk9pU0ZKNy1UVlVpY2NUZV9ERGpVNDl0RTFBazJTaHJRU3ZZazNuSFA0dTNwTVJQZlRCRXYyY2E3dXRsZEVFbmNHdHJkQ2NZYw&q=https%3A%2F%2Ftwitter.com%2Fkarpathy)[Discord](https://www.youtube.com/redirect?event=Watch_SD_EP&redir_token=QUFFLUhqbi1oWWE1djNvOFVWVUo5ODBoVVFkSGdwUVRnd3xBQ3Jtc0tuWm1oR0NuNTQzaXRTdnRqLVlocGlNR1NLNDBEMUJ6LUV3RVlPa3FuS0lQVVRFdzBWaTNjcGl1c0VzdkFHTnJJVEQwY3loeWlaMjVQSmdjMVNiNEpTcnU0X01vcWlRaVJFVnN3WVJkLURxRkpRdlBMWQ&q=https%3A%2F%2Fdiscord.gg%2F3zy8kqD9Cp)

Show less 

# Let's build GPT: from scratch, in code, spelled out.

5,098,877 views5M views

17 Jan 2023

117K

Share

Download

Download 

Save

##  2,693 Comments

Sort comments 

Sort by

Top comments  Newest first 

![Default profile photo](https://yt3.ggpht.com/a/default-user=s48-c-k-c0x00ffffff-no-rj)

Add a comment…

[ ![](https://yt3.ggpht.com/lJsyJ-9zgUQq2fX-bmcXf4E2dlMiGl18c2MdMGa0Bz-OBsJrhIpmt94D0zyEjwn2LK4LAsA1=s88-c-k-c0x00ffffff-no-rj) ](/@fgfanta)

###  [ @fgfanta  ](/@fgfanta)

[ 1 year ago (edited) ](/watch?v=kCc8FmEb1nY&lc=Ugy8TR07lac0eXRIr8B4AaABAg)

Imagine being between your job at Tesla and your job at OpenAI, being a tad bored and, just for fun, dropping on YouTube the best introduction to deep-learning and NLP from scratch so far, for free. Amazing people do amazing things even for a hobby.

Show less Read more

[](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

Like 

7.5K  [](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

Dislike 

[Reply](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

82 replies

82 replies

[ ![](https://yt3.ggpht.com/ytc/AIdro_mw-ZL7o4bvdjmWzzmBE5rVpyGVwkZqgZUWzjcu68M=s88-c-k-c0x00ffffff-no-rj) ](/@8LFrank)

###  [ @8LFrank  ](/@8LFrank)

[ 2 years ago ](/watch?v=kCc8FmEb1nY&lc=UgzFAhNL0eja9GyoOLZ4AaABAg)

Living in a world where a world-class top guy posts a 2-hour video for free on how to make such cutting-edge stuff. I barely started this tutorial but at first I just wanted to say thank you mate!

Show less Read more

[](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

Like 

4.8K  [](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

Dislike 

![](https://yt3.ggpht.com/ytc/AIdro_nDvyq2NoPL626bk1IbxQ94SfQsD-B0qgZchghtQNkLWoEz=s88-c-k-c0x00ffffff-no-rj)

❤ by Andrej Karpathy 

[Reply](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

59 replies

59 replies

[ ![](https://yt3.ggpht.com/5vseDJPShYlJpIKUiliee5fav7e3A3vzOq3z87JlvPMwLBMG6QmrlGizEsqT5wdGYin7ulq0GQ=s88-c-k-c0x00ffffff-no-rj) ](/@softwaredevelopmentwiththo9648)

###  [ @softwaredevelopmentwiththo9648  ](/@softwaredevelopmentwiththo9648)

[ 2 years ago ](/watch?v=kCc8FmEb1nY&lc=UgwU9oC5Qew1UTZws_J4AaABAg)

Thank you for taking the time to create these lectures. I am sure it takes a lot of time and effort to record and cut these. Your effort to level up the the community is greatly appreciated. Thanks Andrej.

Show less Read more

[](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

Like 

2K  [](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

Dislike 

❤ by Andrej Karpathy 

[Reply](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

10 replies

10 replies

[ ](/@BAIR68)

###  [ @BAIR68  ](/@BAIR68)

[ 1 year ago ](/watch?v=kCc8FmEb1nY&lc=Ugye8tHpLxNzr0d94tp4AaABAg)

I am a college professor and learning GPT from Andrej. Every time I watch this video, I not only I learn the contents, also how to deliver any topic effectively. I would vote him as the "Best AI teacher in YouTube”. Salute to Andrej for his outstanding lectures.

Show less Read more

[](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

Like 

382  [](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

Dislike 

[Reply](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

4 replies

4 replies

[ ](/@ShihgianLee)

###  [ @ShihgianLee  ](/@ShihgianLee)

[ 1 year ago ](/watch?v=kCc8FmEb1nY&lc=Ugxi3FFgVH9N0dq97NJ4AaABAg)

This lecture answers ALL my questions from the 2017 Attention Is All You Need paper. I am alway curious about the code behind Transformer. This lecture quenched my curiosity with a colab to tinker with. Thank you so much for your effort and time in creating the lecture to spread the knowledge!

Show less Read more

[](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

Like 

54  [](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

Dislike 

❤ by Andrej Karpathy 

[Reply](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

[ ](/@jamesfraser7394)

###  [ @jamesfraser7394  ](/@jamesfraser7394)

[ 1 year ago ](/watch?v=kCc8FmEb1nY&lc=Ugx0yflCzgtyvIWIRU94AaABAg)

Wow! I knew nothing and now I am enlightened! I actually understand how this AI/ML model works now. As a near 70 year old that just started playing with Python, I am a living example of how effective this lecture is. My humble thanks to Andrej Karpathy for allowing to see into and understand this emerging new world.

Show less Read more

[](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

Like 

874  [](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

Dislike 

[Reply](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

12 replies

12 replies

[ ](/@JainPuneet)

###  [ @JainPuneet  ](/@JainPuneet)

[ 1 year ago ](/watch?v=kCc8FmEb1nY&lc=UgwsuI3l71W0SgC3Rkl4AaABAg)

Andrej, I cannot comprehend how much effort you have put in making these videos. Humanity is thankful to you for making these publically available and educating us with your wisdom. One thing is to know the stuff and apply it in corp setting and another thing is to use that instead to educate millions for free. This is one of the best kind of charity a CS major can do. Kudos to you and thank you so much for doing this.

Show less Read more

[](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

Like 

853  [](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

Dislike 

[Reply](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

12 replies

12 replies

[ ](/@gokublack4832)

###  [ @gokublack4832  ](/@gokublack4832)

[ 2 years ago ](/watch?v=kCc8FmEb1nY&lc=Ugxkp9qRnbucvaYnY2d4AaABAg)

Wow! Having the ex-lead of ML at Tesla make tutorials on ML is amazing. Thank you for producing these resources!

Show less Read more

[](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

Like 

282  [](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

Dislike 

[Reply](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

6 replies

6 replies

[ ](/@DavidAttwater)

###  [ @DavidAttwater  ](/@DavidAttwater)

[ 1 year ago ](/watch?v=kCc8FmEb1nY&lc=Ugz8ZmrfqwlyV3AGYTx4AaABAg)

I cannot thank you enough for this material. I've been a spoken language technologist for 20 years and this plus your micro-grad and make more videos has given me a graduate level update in less than 10 hours. Astonishingly well-prepared and presented material. Thank you.

Show less Read more

[](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

Like 

40  [](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

Dislike 

[Reply](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

[ ](/@RemKim)

###  [ @RemKim  ](/@RemKim)

[ 1 year ago ](/watch?v=kCc8FmEb1nY&lc=UgzQC4qCwPOhXfmikGB4AaABAg)

I suggest watching this video multiple times in order to understand how transformers work. This is by far the best hands on explanation + example.

Show less Read more

[](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

Like 

4  [](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

Dislike 

[Reply](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

[ ](/@fslurrehman)

###  [ @fslurrehman  ](/@fslurrehman)

[ 1 year ago ](/watch?v=kCc8FmEb1nY&lc=UgxcLECqm25VbQ-t2YZ4AaABAg)

I knew only python, math and definitions of NN, GA, ML and DNN. In 2 hours, this lecture has not only given me the understanding of GPT model, but also taught me how to read AI papers and turn them into code, how to use pytoch, and tons of AI definitions. This is the best lecture and practical application on AI. Because it not only gives you an idea of DNN, but also give you code directly from research papers and a final product. Looking forward to more lectures like these. Thanks Andrej Karpathy.

Show less Read more

[](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

Like 

173  [](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

Dislike 

[Reply](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

[ ](/@antopolskiy)

###  [ @antopolskiy  ](/@antopolskiy)

[ 2 years ago ](/watch?v=kCc8FmEb1nY&lc=Ugy33U7PtH5xStvp8Cl4AaABAg)

It is difficult to comprehend how lucky we are to have you teaching us. Thank you, Andrej.

Show less Read more

[](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

Like 

179  [](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

Dislike 

[Reply](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

[ ](/@NicholasRenotte)

###  [ @NicholasRenotte  ](/@NicholasRenotte)

[ @NicholasRenotte @NicholasRenotte  ](/@NicholasRenotte) [ 2 years ago ](/watch?v=kCc8FmEb1nY&lc=UgxQxOTSl0D7j9vdEGV4AaABAg)

This is AMAZING! You're an absolute legend for sharing your knowledge so freely like this Andrej! I'm finally getting some time to get into transformer architectures this is a brilliant deep dive, going to spend the weekend walking through it!! Thank you

Show less Read more

[](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

Like 

108  [](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

Dislike 

[Reply](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

5 replies

5 replies

[ ](/@aojiao3662)

###  [ @aojiao3662  ](/@aojiao3662)

[ 1 year ago ](/watch?v=kCc8FmEb1nY&lc=Ugx2ADHeh_y6sN5gge14AaABAg)

Most clear and intuitive and well explained transformer video I've ever seen. Watched it as if it were a tv show and that's how down-to-earth this video is. Shoutout to the man of legend.

Show less Read more

[](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

Like 

27  [](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

Dislike 

[Reply](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

[ ](/@ajitkirpekar4251)

###  [ @ajitkirpekar4251  ](/@ajitkirpekar4251)

[ 3 months ago (edited) ](/watch?v=kCc8FmEb1nY&lc=Ugzbv7tslw_G-yAvW894AaABAg)

I have read this paper and its variants so many times over and yet this is BY FAR the best most comprehensive tutorial on it I have ever experienced. I applaud Andrej for really nailing all of the different components in a very structured way such that it doesn't overwhelm like it did/does for most people who pound their head at it. I will be recommending this video to anyone and everyone - not just practitioners of NLP, ML, or data science.

Show less Read more

[](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

Like 

1  [](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

Dislike 

[Reply](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

[ ](/@JoseLopez-ox7sq)

###  [ @JoseLopez-ox7sq  ](/@JoseLopez-ox7sq)

[ 2 years ago ](/watch?v=kCc8FmEb1nY&lc=UgxBpgbG45X7kZgiOJl4AaABAg)

This is simply fantastic. I think it would be beneficial for people learning to see the actual process of training, the graphs in W&B and how they can try to train something like this.

Show less Read more

[](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

Like 

186  [](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

Dislike 

[Reply](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

·

5 replies

·

5 replies

[ ](/@rafaelsouza4575)

###  [ @rafaelsouza4575  ](/@rafaelsouza4575)

[ 1 year ago ](/watch?v=kCc8FmEb1nY&lc=UgxmPqfRj719cT9Z_mh4AaABAg)

I was always scared of Transformer's diagram. Honestly, I never understood how such schema could make sense until this day when Andrej enlightened us with his super teaching power. Thank you so much! Andrej, please save the day again by doing one more class about Stable Diffusion!! Please, you are the best!

Show less Read more

[](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

Like 

225  [](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

Dislike 

[Reply](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

[ ](/@meghanaiitb)

###  [ @meghanaiitb  ](/@meghanaiitb)

[ 1 year ago ](/watch?v=kCc8FmEb1nY&lc=UgzMslmekVjoOb5mGtR4AaABAg)

What a feeling ! Just finished sitting on this for the weekend, building along and finally understanding Transformers. More than anything, a sense of fulfilment. Thanks Andrej.

Show less Read more

[](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

Like 

71  [](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

Dislike 

[Reply](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

[ ](/@riochuong105)

###  [ @riochuong105  ](/@riochuong105)

[ 7 months ago ](/watch?v=kCc8FmEb1nY&lc=Ugx_AJHqTRSC2Vs0ldp4AaABAg)

US$20.00 

thanks again for the great lecture. I am able to follow line by line and train it on labmda lab with light effort. Hope to buy you a coffee for all this hard work. Off to the next 4hr GPT2 repro 

Show less Read more

[](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

Like 

3  [](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

Dislike 

❤ by Andrej Karpathy 

[Reply](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

[ ](/@themenon)

###  [ @themenon  ](/@themenon)

[ 6 months ago ](/watch?v=kCc8FmEb1nY&lc=UgxwTQP06K90kA6Mun54AaABAg)

JP¥1,000 

Thanks for this well explained and wonderful series! Hope you will cover qunatization for people with low power GPU.

Show less Read more

[](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

Like 

13  [](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

Dislike 

❤ by Andrej Karpathy 

[Reply](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3D%252Fwatch%253Fv%253DkCc8FmEb1nY&hl=en-GB)

##  Comments 2.6K

Top comments  Newest first 

##  In this video

Timeline

Chapters

Transcript

##  Chapters

#### [intro: ChatGPT, Transformers, nanoGPT, Shakespeare intro: ChatGPT, Transformers, nanoGPT, Shakespeare 0:00 ](/watch?v=kCc8FmEb1nY&t=0s)

#### [intro: ChatGPT, Transformers, nanoGPT, Shakespeare ](/watch?v=kCc8FmEb1nY&t=0s)

0:00

#### [reading and exploring the data reading and exploring the data 7:52 ](/watch?v=kCc8FmEb1nY&t=472s)

#### [reading and exploring the data ](/watch?v=kCc8FmEb1nY&t=472s)

7:52

#### [tokenization, train/val split tokenization, train/val split 9:28 ](/watch?v=kCc8FmEb1nY&t=568s)

#### [tokenization, train/val split ](/watch?v=kCc8FmEb1nY&t=568s)

9:28

#### [data loader: batches of chunks of data data loader: batches of chunks of data 14:27 ](/watch?v=kCc8FmEb1nY&t=867s)

#### [data loader: batches of chunks of data ](/watch?v=kCc8FmEb1nY&t=867s)

14:27

#### [simplest baseline: bigram language model, loss, generation simplest baseline: bigram language model, loss, generation 22:11 ](/watch?v=kCc8FmEb1nY&t=1331s)

#### [simplest baseline: bigram language model, loss, generation ](/watch?v=kCc8FmEb1nY&t=1331s)

22:11

#### [training the bigram model training the bigram model 34:53 ](/watch?v=kCc8FmEb1nY&t=2093s)

#### [training the bigram model ](/watch?v=kCc8FmEb1nY&t=2093s)

34:53

#### [port our code to a script port our code to a script 38:00 ](/watch?v=kCc8FmEb1nY&t=2280s)

#### [port our code to a script ](/watch?v=kCc8FmEb1nY&t=2280s)

38:00

#### [version 1: averaging past context with for loops, the weakest form of aggregation version 1: averaging past context with for loops, the weakest form of aggregation 42:13 ](/watch?v=kCc8FmEb1nY&t=2533s)

#### [version 1: averaging past context with for loops, the weakest form of aggregation ](/watch?v=kCc8FmEb1nY&t=2533s)

42:13

#### [the trick in self-attention: matrix multiply as weighted aggregation the trick in self-attention: matrix multiply as weighted aggregation 47:11 ](/watch?v=kCc8FmEb1nY&t=2831s)

#### [the trick in self-attention: matrix multiply as weighted aggregation ](/watch?v=kCc8FmEb1nY&t=2831s)

47:11

#### [version 2: using matrix multiply version 2: using matrix multiply 51:54 ](/watch?v=kCc8FmEb1nY&t=3114s)

#### [version 2: using matrix multiply ](/watch?v=kCc8FmEb1nY&t=3114s)

51:54

#### [version 3: adding softmax version 3: adding softmax 54:42 ](/watch?v=kCc8FmEb1nY&t=3282s)

#### [version 3: adding softmax ](/watch?v=kCc8FmEb1nY&t=3282s)

54:42

#### [minor code cleanup minor code cleanup 58:26 ](/watch?v=kCc8FmEb1nY&t=3506s)

#### [minor code cleanup ](/watch?v=kCc8FmEb1nY&t=3506s)

58:26

#### [positional encoding positional encoding 1:00:18 ](/watch?v=kCc8FmEb1nY&t=3618s)

#### [positional encoding ](/watch?v=kCc8FmEb1nY&t=3618s)

1:00:18

#### [THE CRUX OF THE VIDEO: version 4: self-attention THE CRUX OF THE VIDEO: version 4: self-attention 1:02:00 ](/watch?v=kCc8FmEb1nY&t=3720s)

#### [THE CRUX OF THE VIDEO: version 4: self-attention ](/watch?v=kCc8FmEb1nY&t=3720s)

1:02:00

#### [note 1: attention as communication note 1: attention as communication 1:11:38 ](/watch?v=kCc8FmEb1nY&t=4298s)

#### [note 1: attention as communication ](/watch?v=kCc8FmEb1nY&t=4298s)

1:11:38

#### [note 2: attention has no notion of space, operates over sets note 2: attention has no notion of space, operates over sets 1:12:46 ](/watch?v=kCc8FmEb1nY&t=4366s)

#### [note 2: attention has no notion of space, operates over sets ](/watch?v=kCc8FmEb1nY&t=4366s)

1:12:46

#### [note 3: there is no communication across batch dimension note 3: there is no communication across batch dimension 1:13:40 ](/watch?v=kCc8FmEb1nY&t=4420s)

#### [note 3: there is no communication across batch dimension ](/watch?v=kCc8FmEb1nY&t=4420s)

1:13:40

#### [note 4: encoder blocks vs. decoder blocks note 4: encoder blocks vs. decoder blocks 1:14:14 ](/watch?v=kCc8FmEb1nY&t=4454s)

#### [note 4: encoder blocks vs. decoder blocks ](/watch?v=kCc8FmEb1nY&t=4454s)

1:14:14

#### [note 5: attention vs. self-attention vs. cross-attention note 5: attention vs. self-attention vs. cross-attention 1:15:39 ](/watch?v=kCc8FmEb1nY&t=4539s)

#### [note 5: attention vs. self-attention vs. cross-attention ](/watch?v=kCc8FmEb1nY&t=4539s)

1:15:39

#### [note 6: "scaled" self-attention. why divide by sqrt(head_size) note 6: "scaled" self-attention. why divide by sqrt(head_size) 1:16:56 ](/watch?v=kCc8FmEb1nY&t=4616s)

#### [note 6: "scaled" self-attention. why divide by sqrt(head_size) ](/watch?v=kCc8FmEb1nY&t=4616s)

1:16:56

#### [inserting a single self-attention block to our network inserting a single self-attention block to our network 1:19:11 ](/watch?v=kCc8FmEb1nY&t=4751s)

#### [inserting a single self-attention block to our network ](/watch?v=kCc8FmEb1nY&t=4751s)

1:19:11

#### [multi-headed self-attention multi-headed self-attention 1:21:59 ](/watch?v=kCc8FmEb1nY&t=4919s)

#### [multi-headed self-attention ](/watch?v=kCc8FmEb1nY&t=4919s)

1:21:59

#### [feedforward layers of transformer block feedforward layers of transformer block 1:24:25 ](/watch?v=kCc8FmEb1nY&t=5065s)

#### [feedforward layers of transformer block ](/watch?v=kCc8FmEb1nY&t=5065s)

1:24:25

#### [residual connections residual connections 1:26:48 ](/watch?v=kCc8FmEb1nY&t=5208s)

#### [residual connections ](/watch?v=kCc8FmEb1nY&t=5208s)

1:26:48

#### [layernorm (and its relationship to our previous batchnorm) layernorm (and its relationship to our previous batchnorm) 1:32:51 ](/watch?v=kCc8FmEb1nY&t=5571s)

#### [layernorm (and its relationship to our previous batchnorm) ](/watch?v=kCc8FmEb1nY&t=5571s)

1:32:51

#### [scaling up the model! creating a few variables. adding dropout scaling up the model! creating a few variables. adding dropout 1:37:49 ](/watch?v=kCc8FmEb1nY&t=5869s)

#### [scaling up the model! creating a few variables. adding dropout ](/watch?v=kCc8FmEb1nY&t=5869s)

1:37:49

#### [encoder vs. decoder vs. both (?) Transformers encoder vs. decoder vs. both (?) Transformers 1:42:39 ](/watch?v=kCc8FmEb1nY&t=6159s)

#### [encoder vs. decoder vs. both (?) Transformers ](/watch?v=kCc8FmEb1nY&t=6159s)

1:42:39

#### [super quick walkthrough of nanoGPT, batched multi-headed self-attention super quick walkthrough of nanoGPT, batched multi-headed self-attention 1:46:22 ](/watch?v=kCc8FmEb1nY&t=6382s)

#### [super quick walkthrough of nanoGPT, batched multi-headed self-attention ](/watch?v=kCc8FmEb1nY&t=6382s)

1:46:22

#### [back to ChatGPT, GPT-3, pretraining vs. finetuning, RLHF back to ChatGPT, GPT-3, pretraining vs. finetuning, RLHF 1:48:53 ](/watch?v=kCc8FmEb1nY&t=6533s)

#### [back to ChatGPT, GPT-3, pretraining vs. finetuning, RLHF ](/watch?v=kCc8FmEb1nY&t=6533s)

1:48:53

#### [conclusions conclusions 1:54:32 ](/watch?v=kCc8FmEb1nY&t=6872s)

#### [conclusions ](/watch?v=kCc8FmEb1nY&t=6872s)

1:54:32

Sync to video time 

Sync to video time

##  Description

Let's build GPT: from scratch, in code, spelled out.

117KLikes

5,098,877Views

202317 Jan

We build a Generatively Pretrained Transformer (GPT), following the paper "Attention is All You Need" and OpenAI's GPT-2 / GPT-3. We talk about connections to ChatGPT, which has taken the world by storm. We watch GitHub Copilot, itself a GPT, help us write a GPT (meta :D!) . I recommend people watch the earlier makemore videos to get comfortable with the autoregressive language modeling framework and basics of tensors and PyTorch nn, which we take for granted in this video. Links: 

  * Google colab for the video: [https://colab.research.google.com/dri...](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqa0RmZVJha0VoX2ptTUNlUUxINThQQ201VVdjd3xBQ3Jtc0tsdW1keUd2bGFKOE5sR0lzUkZLYTZNc2lpVS11eXlPclp0Nm5md25PR3kwNWkxalVNbWt1YzhmQW1qeGg5WE9PZS1FSjl1dzRqMVRBbzBaZEc2cE5kcHp0cmFrck1tQnhBVjEyNEFzRHBxTllqZ19zRQ&q=https%3A%2F%2Fcolab.research.google.com%2Fdrive%2F1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-%3Fusp%3Dsharing&v=kCc8FmEb1nY)
  * GitHub repo for the video: [https://github.com/karpathy/ng-video-...](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbWlRbUR0ekxKNmFUMGFLRWpYNlVWbEg1c0RDQXxBQ3Jtc0tsdFYzckdMRFBXWkZ1WVpyTXcyTlpjdWo0b2FOc3N4UnM4N3NNUGMwRFJTQjJZeVhaN3lhSmE4REpfcTJkVW1YbkNCazU5Ym5kdC04Y3FIOWxXX1EtZnpaMDhLSzBSOFpGVXNTTHJpU29oUFBxRWZ3WQ&q=https%3A%2F%2Fgithub.com%2Fkarpathy%2Fng-video-lecture&v=kCc8FmEb1nY)
  * Playlist of the whole Zero to Hero series so far: [ • The spelled-out intro to neural netwo... ](/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=1&t=0s)
  * nanoGPT repo: [https://github.com/karpathy/nanoGPT](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqblN2QXF3WlpMTXI5a0R1ckJLVFhxdnRVbklfd3xBQ3Jtc0trUjYtcllFNHJUWXB2Q3pNREdZV0p4cFRDSHFSRHhKb2NNV3FicGhIWi15VUItbHFuc05QbGtLOTBMRk8wVGhUUmRxY0swTTJuT2ZjZFpaT2RxbTZVMTBQSmtZWHpmbU9UdXBuVlUzdkRVSnNkd2Q0VQ&q=https%3A%2F%2Fgithub.com%2Fkarpathy%2FnanoGPT&v=kCc8FmEb1nY)
  * my website: [https://karpathy.ai](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbUladnBDdXVSdXFxQnptMnR4OEZRZ2VhYUQwd3xBQ3Jtc0ttSkhPbno1c0xOdnRIX21PbFp0alJXZnlfekpuZzRWWXVpbWtrWFZfV0VGLU9nZU96VXlCWGs1WkxDTHNVNjE1ZVVfeUIwTUZiaFRFU3FLSFJRbTFHY2QyRkxWeXNFdHRXYUtGUzdSMXh4dXNZWE1kcw&q=https%3A%2F%2Fkarpathy.ai%2F&v=kCc8FmEb1nY)
  * my twitter: [ / karpathy ](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbHRGajJ0T2JMMEJyTnZBVGFJYjdWSXc2OTJjQXxBQ3Jtc0tsLWgyUXhyaFdWRHp3WncxNVJaa21oT0h5UW5RUHdzSXNhTzQyUDdPcm9Ud251WWVla0RsWDdiQi1Sb0lRQTM2MDJtenNveUdSclJWVjJ2eUtKY0RzMWZJNlpNZXpwUXFLS3pkZ3F6cS0wTUplVmd6bw&q=https%3A%2F%2Ftwitter.com%2Fkarpathy&v=kCc8FmEb1nY)
  * our Discord channel: [ / discord ](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbk9NWUtPcjRhMnZMbnNsVnp3WU5pMERieFZmd3xBQ3Jtc0tuVW0tSVN3R3d5Tmk0QWxnZ2ZiLW1BdTN5QktZaEdqSEpJMndGZkhOY19nc2dsb3otT0ZpcFpDMlJ1X0tyZnc1d0FhcUctcFFnaEU1Y2dhdzRvTExTcEpzeDhoSkI2dTVhVWdSek9LV0xDaTl2U2FJSQ&q=https%3A%2F%2Fdiscord.gg%2F3zy8kqD9Cp&v=kCc8FmEb1nY)

Supplementary links: 

  * Attention is All You Need paper: [https://arxiv.org/abs/1706.03762](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbFdfbGRudmFrTmlTWHNRQ2RuRk9kQ2ZOT3FsQXxBQ3Jtc0tuSEp0ZEkwd1BrTV9BcDNtdWs3NVh6QkNZY3psV01CLXM4X1RUcEMxOGFkT2RFekV5Sl9zaVBDT2swYndwWi1SYXRxdkplMksydE9lTTVjNzdMX1Rfbk1LQkVRLWJGREtabkVLYUhjcjJ6ejBmQzEyNA&q=https%3A%2F%2Farxiv.org%2Fabs%2F1706.03762&v=kCc8FmEb1nY)
  * OpenAI GPT-3 paper: [https://arxiv.org/abs/2005.14165](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqa2hQQXcxME11NkNoWmlHdm10UC1rREFLSzVKZ3xBQ3Jtc0tsLWxGVVJMTTlYTGZRWGZmSmg2X2d1UWZmUVN6dTRkY3QwV2Q5b2NlRVJQTTZ5ZXJybldReWhIeFdFLXladkdoVWM3ckRrempDTnpaVGJMcWJHTGlpaExrcE9UNHNJa1JhVkFuUEYyUU9kbW9XVkNEZw&q=https%3A%2F%2Farxiv.org%2Fabs%2F2005.14165&v=kCc8FmEb1nY)
  * OpenAI ChatGPT blog post: [https://openai.com/blog/chatgpt/](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbXRDRWRIZmtUQ2d3TzExUVluYVhYSVE1akwtUXxBQ3Jtc0trT0VnWVJ2SGNDMnNYU1A2TmpidXNsUlVtcXRHRjZ3ZzdIUlVBTHdZWElIVm5nMkR2SEdwcmFteHRHUWFXb3U0ZmdJSjJrNVBJb3RsaENhNHJzWmFsaUUwZlZ4bkhkV3BySFpQYWtXdGtDYVlaYjZwWQ&q=https%3A%2F%2Fopenai.com%2Fblog%2Fchatgpt%2F&v=kCc8FmEb1nY)
  * The GPU I'm training the model on is from Lambda GPU Cloud, I think the best and easiest way to spin up an on-demand GPU instance in the cloud that you can ssh to: [https://lambdalabs.com](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbFdXS1dlMmJ3aC12ZzRyb0FybTJlbEtpcHB4Z3xBQ3Jtc0trSl9vb1JTYzdkTHR0TUdES2hhWEkzLUxvZmpXV1RudkhNQ1hBVVRjVUo4T3JSWmJKY09UMHZXSy1qZkJhTExOV1BfRkM0ZHZpaHhnc3RNRzdyb1NvUXdRTkdNRnFQcEpZMnhkUHNaZGFMamJXdVBLbw&q=https%3A%2F%2Flambdalabs.com%2F&v=kCc8FmEb1nY) . If you prefer to work in notebooks, I think the easiest path today is Google Colab. 

Suggested exercises: 

  * EX1: The n-dimensional tensor mastery challenge: Combine the `Head` and `MultiHeadAttention` into one class that processes all the heads in parallel, treating the heads as another batch dimension (answer is in nanoGPT). 
  * EX2: Train the GPT on your own dataset of choice! What other data could be fun to blabber on about? (A fun advanced suggestion if you like: train a GPT to do addition of two numbers, i.e. a+b=c. You may find it helpful to predict the digits of c in reverse order, as the typical addition algorithm (that you're hoping it learns) would proceed right to left too. You may want to modify the data loader to simply serve random problems and skip the generation of train.bin, val.bin. You may want to mask out the loss at the input positions of a+b that just specify the problem using y=-1 in the targets (see CrossEntropyLoss ignore_index). Does your Transformer learn to add? Once you have this, swole doge project: build a calculator clone in GPT, for all of +-*/. Not an easy problem. You may need Chain of Thought traces.) 
  * EX3: Find a dataset that is very large, so large that you can't see a gap between train and val loss. Pretrain the transformer on this data, then initialize with that model and finetune it on tiny shakespeare with a smaller number of steps and lower learning rate. Can you obtain a lower validation loss by the use of pretraining? 
  * EX4: Read some transformer papers and implement one additional feature or change that people seem to use. Does it improve the performance of your GPT? 

Chapters: [00:00:00](/watch?v=kCc8FmEb1nY&t=0s) intro: ChatGPT, Transformers, nanoGPT, Shakespeare baseline language modeling, code setup [00:07:52](/watch?v=kCc8FmEb1nY&t=472s) reading and exploring the data [00:09:28](/watch?v=kCc8FmEb1nY&t=568s) tokenization, train/val split [00:14:27](/watch?v=kCc8FmEb1nY&t=867s) data loader: batches of chunks of data [00:22:11](/watch?v=kCc8FmEb1nY&t=1331s) simplest baseline: bigram language model, loss, generation [00:34:53](/watch?v=kCc8FmEb1nY&t=2093s) training the bigram model [00:38:00](/watch?v=kCc8FmEb1nY&t=2280s) port our code to a script Building the "self-attention" [00:42:13](/watch?v=kCc8FmEb1nY&t=2533s) version 1: averaging past context with for loops, the weakest form of aggregation [00:47:11](/watch?v=kCc8FmEb1nY&t=2831s) the trick in self-attention: matrix multiply as weighted aggregation [00:51:54](/watch?v=kCc8FmEb1nY&t=3114s) version 2: using matrix multiply [00:54:42](/watch?v=kCc8FmEb1nY&t=3282s) version 3: adding softmax [00:58:26](/watch?v=kCc8FmEb1nY&t=3506s) minor code cleanup [01:00:18](/watch?v=kCc8FmEb1nY&t=3618s) positional encoding [01:02:00](/watch?v=kCc8FmEb1nY&t=3720s) THE CRUX OF THE VIDEO: version 4: self-attention [01:11:38](/watch?v=kCc8FmEb1nY&t=4298s) note 1: attention as communication [01:12:46](/watch?v=kCc8FmEb1nY&t=4366s) note 2: attention has no notion of space, operates over sets [01:13:40](/watch?v=kCc8FmEb1nY&t=4420s) note 3: there is no communication across batch dimension [01:14:14](/watch?v=kCc8FmEb1nY&t=4454s) note 4: encoder blocks vs. decoder blocks [01:15:39](/watch?v=kCc8FmEb1nY&t=4539s) note 5: attention vs. self-attention vs. cross-attention [01:16:56](/watch?v=kCc8FmEb1nY&t=4616s) note 6: "scaled" self-attention. why divide by sqrt(head_size) Building the Transformer [01:19:11](/watch?v=kCc8FmEb1nY&t=4751s) inserting a single self-attention block to our network [01:21:59](/watch?v=kCc8FmEb1nY&t=4919s) multi-headed self-attention [01:24:25](/watch?v=kCc8FmEb1nY&t=5065s) feedforward layers of transformer block [01:26:48](/watch?v=kCc8FmEb1nY&t=5208s) residual connections [01:32:51](/watch?v=kCc8FmEb1nY&t=5571s) layernorm (and its relationship to our previous batchnorm) [01:37:49](/watch?v=kCc8FmEb1nY&t=5869s) scaling up the model! creating a few variables. adding dropout Notes on Transformer [01:42:39](/watch?v=kCc8FmEb1nY&t=6159s) encoder vs. decoder vs. both (?) Transformers [01:46:22](/watch?v=kCc8FmEb1nY&t=6382s) super quick walkthrough of nanoGPT, batched multi-headed self-attention [01:48:53](/watch?v=kCc8FmEb1nY&t=6533s) back to ChatGPT, GPT-3, pretraining vs. finetuning, RLHF [01:54:32](/watch?v=kCc8FmEb1nY&t=6872s) conclusions Corrections: [00:57:00](/watch?v=kCc8FmEb1nY&t=3420s) Oops "tokens from the future cannot communicate", not "past". Sorry! :) [01:20:05](/watch?v=kCc8FmEb1nY&t=4805s) Oops I should be using the head_size for the normalization, not C

Show less ...more

We build a Generatively Pretrained Transformer (GPT), following the paper "Attention is All You Need" and OpenAI's GPT-2 / GPT-3. We talk about connections to ChatGPT, which has taken the world by storm. We watch GitHub Copilot, itself a GPT, help us write a GPT (meta :D!) . I recommend people watch the earlier makemore videos to get comfortable with the autoregressive language modeling framework and basics of tensors and PyTorch nn, which we take for granted in this video. Links: 

  * Google colab for the video: [https://colab.research.google.com/dri...](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqa0RmZVJha0VoX2ptTUNlUUxINThQQ201VVdjd3xBQ3Jtc0tsdW1keUd2bGFKOE5sR0lzUkZLYTZNc2lpVS11eXlPclp0Nm5md25PR3kwNWkxalVNbWt1YzhmQW1qeGg5WE9PZS1FSjl1dzRqMVRBbzBaZEc2cE5kcHp0cmFrck1tQnhBVjEyNEFzRHBxTllqZ19zRQ&q=https%3A%2F%2Fcolab.research.google.com%2Fdrive%2F1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-%3Fusp%3Dsharing&v=kCc8FmEb1nY)
  * GitHub repo for the video: [https://github.com/karpathy/ng-video-...](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbWlRbUR0ekxKNmFUMGFLRWpYNlVWbEg1c0RDQXxBQ3Jtc0tsdFYzckdMRFBXWkZ1WVpyTXcyTlpjdWo0b2FOc3N4UnM4N3NNUGMwRFJTQjJZeVhaN3lhSmE4REpfcTJkVW1YbkNCazU5Ym5kdC04Y3FIOWxXX1EtZnpaMDhLSzBSOFpGVXNTTHJpU29oUFBxRWZ3WQ&q=https%3A%2F%2Fgithub.com%2Fkarpathy%2Fng-video-lecture&v=kCc8FmEb1nY)
  * Playlist of the whole Zero to Hero series so far: [ • The spelled-out intro to neural netwo... ](/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=1&t=0s)
  * nanoGPT repo: [https://github.com/karpathy/nanoGPT](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqblN2QXF3WlpMTXI5a0R1ckJLVFhxdnRVbklfd3xBQ3Jtc0trUjYtcllFNHJUWXB2Q3pNREdZV0p4cFRDSHFSRHhKb2NNV3FicGhIWi15VUItbHFuc05QbGtLOTBMRk8wVGhUUmRxY0swTTJuT2ZjZFpaT2RxbTZVMTBQSmtZWHpmbU9UdXBuVlUzdkRVSnNkd2Q0VQ&q=https%3A%2F%2Fgithub.com%2Fkarpathy%2FnanoGPT&v=kCc8FmEb1nY)
  * my website: [https://karpathy.ai](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbUladnBDdXVSdXFxQnptMnR4OEZRZ2VhYUQwd3xBQ3Jtc0ttSkhPbno1c0xOdnRIX21PbFp0alJXZnlfekpuZzRWWXVpbWtrWFZfV0VGLU9nZU96VXlCWGs1WkxDTHNVNjE1ZVVfeUIwTUZiaFRFU3FLSFJRbTFHY2QyRkxWeXNFdHRXYUtGUzdSMXh4dXNZWE1kcw&q=https%3A%2F%2Fkarpathy.ai%2F&v=kCc8FmEb1nY)
  * my twitter: [ / karpathy ](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbHRGajJ0T2JMMEJyTnZBVGFJYjdWSXc2OTJjQXxBQ3Jtc0tsLWgyUXhyaFdWRHp3WncxNVJaa21oT0h5UW5RUHdzSXNhTzQyUDdPcm9Ud251WWVla0RsWDdiQi1Sb0lRQTM2MDJtenNveUdSclJWVjJ2eUtKY0RzMWZJNlpNZXpwUXFLS3pkZ3F6cS0wTUplVmd6bw&q=https%3A%2F%2Ftwitter.com%2Fkarpathy&v=kCc8FmEb1nY)
  * our Discord channel: [ / discord ](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbk9NWUtPcjRhMnZMbnNsVnp3WU5pMERieFZmd3xBQ3Jtc0tuVW0tSVN3R3d5Tmk0QWxnZ2ZiLW1BdTN5QktZaEdqSEpJMndGZkhOY19nc2dsb3otT0ZpcFpDMlJ1X0tyZnc1d0FhcUctcFFnaEU1Y2dhdzRvTExTcEpzeDhoSkI2dTVhVWdSek9LV0xDaTl2U2FJSQ&q=https%3A%2F%2Fdiscord.gg%2F3zy8kqD9Cp&v=kCc8FmEb1nY)

Supplementary links: 

  * Attention is All You Need paper: [https://arxiv.org/abs/1706.03762](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbFdfbGRudmFrTmlTWHNRQ2RuRk9kQ2ZOT3FsQXxBQ3Jtc0tuSEp0ZEkwd1BrTV9BcDNtdWs3NVh6QkNZY3psV01CLXM4X1RUcEMxOGFkT2RFekV5Sl9zaVBDT2swYndwWi1SYXRxdkplMksydE9lTTVjNzdMX1Rfbk1LQkVRLWJGREtabkVLYUhjcjJ6ejBmQzEyNA&q=https%3A%2F%2Farxiv.org%2Fabs%2F1706.03762&v=kCc8FmEb1nY)
  * OpenAI GPT-3 paper: [https://arxiv.org/abs/2005.14165](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqa2hQQXcxME11NkNoWmlHdm10UC1rREFLSzVKZ3xBQ3Jtc0tsLWxGVVJMTTlYTGZRWGZmSmg2X2d1UWZmUVN6dTRkY3QwV2Q5b2NlRVJQTTZ5ZXJybldReWhIeFdFLXladkdoVWM3ckRrempDTnpaVGJMcWJHTGlpaExrcE9UNHNJa1JhVkFuUEYyUU9kbW9XVkNEZw&q=https%3A%2F%2Farxiv.org%2Fabs%2F2005.14165&v=kCc8FmEb1nY)
  * OpenAI ChatGPT blog post: [https://openai.com/blog/chatgpt/](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbXRDRWRIZmtUQ2d3TzExUVluYVhYSVE1akwtUXxBQ3Jtc0trT0VnWVJ2SGNDMnNYU1A2TmpidXNsUlVtcXRHRjZ3ZzdIUlVBTHdZWElIVm5nMkR2SEdwcmFteHRHUWFXb3U0ZmdJSjJrNVBJb3RsaENhNHJzWmFsaUUwZlZ4bkhkV3BySFpQYWtXdGtDYVlaYjZwWQ&q=https%3A%2F%2Fopenai.com%2Fblog%2Fchatgpt%2F&v=kCc8FmEb1nY)
  * The GPU I'm training the model on is from Lambda GPU Cloud, I think the best and easiest way to spin up an on-demand GPU instance in the cloud that you can ssh to: [https://lambdalabs.com](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbFdXS1dlMmJ3aC12ZzRyb0FybTJlbEtpcHB4Z3xBQ3Jtc0trSl9vb1JTYzdkTHR0TUdES2hhWEkzLUxvZmpXV1RudkhNQ1hBVVRjVUo4T3JSWmJKY09UMHZXSy1qZkJhTExOV1BfRkM0ZHZpaHhnc3RNRzdyb1NvUXdRTkdNRnFQcEpZMnhkUHNaZGFMamJXdVBLbw&q=https%3A%2F%2Flambdalabs.com%2F&v=kCc8FmEb1nY) . If you prefer to work in notebooks, I think the easiest path today is Google Colab. 

Suggested exercises: 

  * EX1: The n-dimensional tensor mastery challenge: Combine the `Head` and `MultiHeadAttention` into one class that processes all the heads in parallel, treating the heads as another batch dimension (answer is in nanoGPT). 
  * EX2: Train the GPT on your own dataset of choice! What other data could be fun to blabber on about? (A fun advanced suggestion if you like: train a GPT to do addition of two numbers, i.e. a+b=c. You may find it helpful to predict the digits of c in reverse order, as the typical addition algorithm (that you're hoping it learns) would proceed right to left too. You may want to modify the data loader to simply serve random problems and skip the generation of train.bin, val.bin. You may want to mask out the loss at the input positions of a+b that just specify the problem using y=-1 in the targets (see CrossEntropyLoss ignore_index). Does your Transformer learn to add? Once you have this, swole doge project: build a calculator clone in GPT, for all of +-*/. Not an easy problem. You may need Chain of Thought traces.) 
  * EX3: Find a dataset that is very large, so large that you can't see a gap between train and val loss. Pretrain the transformer on this data, then initialize with that model and finetune it on tiny shakespeare with a smaller number of steps and lower learning rate. Can you obtain a lower validation loss by the use of pretraining? 
  * EX4: Read some transformer papers and implement one additional feature or change that people seem to use. Does it improve the performance of your GPT? 

Chapters: [00:00:00](/watch?v=kCc8FmEb1nY&t=0s) intro: ChatGPT, Transformers, nanoGPT, Shakespeare baseline language modeling, code setup [00:07:52](/watch?v=kCc8FmEb1nY&t=472s) reading and exploring the data [00:09:28](/watch?v=kCc8FmEb1nY&t=568s) tokenization, train/val split [00:14:27](/watch?v=kCc8FmEb1nY&t=867s) data loader: batches of chunks of data [00:22:11](/watch?v=kCc8FmEb1nY&t=1331s) simplest baseline: bigram language model, loss, generation [00:34:53](/watch?v=kCc8FmEb1nY&t=2093s) training the bigram model [00:38:00](/watch?v=kCc8FmEb1nY&t=2280s) port our code to a script Building the "self-attention" [00:42:13](/watch?v=kCc8FmEb1nY&t=2533s) version 1: averaging past context with for loops, the weakest form of aggregation [00:47:11](/watch?v=kCc8FmEb1nY&t=2831s) the trick in self-attention: matrix multiply as weighted aggregation [00:51:54](/watch?v=kCc8FmEb1nY&t=3114s) version 2: using matrix multiply [00:54:42](/watch?v=kCc8FmEb1nY&t=3282s) version 3: adding softmax [00:58:26](/watch?v=kCc8FmEb1nY&t=3506s) minor code cleanup [01:00:18](/watch?v=kCc8FmEb1nY&t=3618s) positional encoding [01:02:00](/watch?v=kCc8FmEb1nY&t=3720s) THE CRUX OF THE VIDEO: version 4: self-attention [01:11:38](/watch?v=kCc8FmEb1nY&t=4298s) note 1: attention as communication [01:12:46](/watch?v=kCc8FmEb1nY&t=4366s) note 2: attention has no notion of space, operates over sets [01:13:40](/watch?v=kCc8FmEb1nY&t=4420s) note 3: there is no communication across batch dimension [01:14:14](/watch?v=kCc8FmEb1nY&t=4454s) note 4: encoder blocks vs. decoder blocks [01:15:39](/watch?v=kCc8FmEb1nY&t=4539s) note 5: attention vs. self-attention vs. cross-attention [01:16:56](/watch?v=kCc8FmEb1nY&t=4616s) note 6: "scaled" self-attention. why divide by sqrt(head_size) Building the Transformer [01:19:11](/watch?v=kCc8FmEb1nY&t=4751s) inserting a single self-attention block to our network [01:21:59](/watch?v=kCc8FmEb1nY&t=4919s) multi-headed self-attention [01:24:25](/watch?v=kCc8FmEb1nY&t=5065s) feedforward layers of transformer block [01:26:48](/watch?v=kCc8FmEb1nY&t=5208s) residual connections [01:32:51](/watch?v=kCc8FmEb1nY&t=5571s) layernorm (and its relationship to our previous batchnorm) [01:37:49](/watch?v=kCc8FmEb1nY&t=5869s) scaling up the model! creating a few variables. adding dropout Notes on Transformer [01:42:39](/watch?v=kCc8FmEb1nY&t=6159s) encoder vs. decoder vs. both (?) Transformers [01:46:22](/watch?v=kCc8FmEb1nY&t=6382s) super quick walkthrough of nanoGPT, batched multi-headed self-attention [01:48:53](/watch?v=kCc8FmEb1nY&t=6533s) back to ChatGPT, GPT-3, pretraining vs. finetuning, RLHF [01:54:32](/watch?v=kCc8FmEb1nY&t=6872s) conclusions Corrections: [00:57:00](/watch?v=kCc8FmEb1nY&t=3420s) Oops "tokens from the future cannot communicate", not "past". Sorry! :) [01:20:05](/watch?v=kCc8FmEb1nY&t=4805s) Oops I should be using the head_size for the normalization, not C…...more 

...more Show less 

## 

Chapters

View all

#### [intro: ChatGPT, Transformers, nanoGPT, Shakespeare intro: ChatGPT, Transformers, nanoGPT, Shakespeare 0:00 ](/watch?v=kCc8FmEb1nY&t=0s)

#### [intro: ChatGPT, Transformers, nanoGPT, Shakespeare ](/watch?v=kCc8FmEb1nY&t=0s)

0:00

#### [reading and exploring the data reading and exploring the data 7:52 ](/watch?v=kCc8FmEb1nY&t=472s)

#### [reading and exploring the data ](/watch?v=kCc8FmEb1nY&t=472s)

7:52

#### [tokenization, train/val split tokenization, train/val split 9:28 ](/watch?v=kCc8FmEb1nY&t=568s)

#### [tokenization, train/val split ](/watch?v=kCc8FmEb1nY&t=568s)

9:28

#### [data loader: batches of chunks of data data loader: batches of chunks of data 14:27 ](/watch?v=kCc8FmEb1nY&t=867s)

#### [data loader: batches of chunks of data ](/watch?v=kCc8FmEb1nY&t=867s)

14:27

Corrections

View all

[ 57:00 ](/watch?v=kCc8FmEb1nY&t=3420s)

Oops "tokens from the _future_ cannot communicate", not "past". Sorry! :)

Transcript

Follow along using the transcript.

Show transcript

### [Andrej Karpathy 606K subscribers  ](/@AndrejKarpathy)

[Videos](/channel/UCXUPKJO5MZQN11PqgIvyuvQ/videos)

[About](/channel/UCXUPKJO5MZQN11PqgIvyuvQ/about)

[Videos](/channel/UCXUPKJO5MZQN11PqgIvyuvQ/videos)[About](/channel/UCXUPKJO5MZQN11PqgIvyuvQ/about)[Eureka X](https://www.youtube.com/redirect?event=Watch_SD_EP&redir_token=QUFFLUhqbmdiMkgwcU54U1I0VjlFT1h5SG9ubzVNd3NUUXxBQ3Jtc0ttcWFodGlpU1B6SEl2ZXBJSzV4a2RwUklUZU95OEp6THQyMG5zbzV1SVltMVZBeEdsRmJScWVlYi1HRWlRWUZCbXkxbnU4dXdzVnJ6M04xQlNwbS1wdDZFbU1QR1pOSFdSRVN0MXQ3TDlTZXZnWGxhdw&q=https%3A%2F%2Ftwitter.com%2FEurekaLabsAI)[Andrej X](https://www.youtube.com/redirect?event=Watch_SD_EP&redir_token=QUFFLUhqa29nSjhuZUdYS2Ffd3hBWk5GZDU1b3F3cl8yZ3xBQ3Jtc0ttdDJzamxnQU5QYVBwUTcxM3NEbkRUcGdZbjdicUhYcG9kN3RqUmpBQk9pU0ZKNy1UVlVpY2NUZV9ERGpVNDl0RTFBazJTaHJRU3ZZazNuSFA0dTNwTVJQZlRCRXYyY2E3dXRsZEVFbmNHdHJkQ2NZYw&q=https%3A%2F%2Ftwitter.com%2Fkarpathy)[Discord](https://www.youtube.com/redirect?event=Watch_SD_EP&redir_token=QUFFLUhqbi1oWWE1djNvOFVWVUo5ODBoVVFkSGdwUVRnd3xBQ3Jtc0tuWm1oR0NuNTQzaXRTdnRqLVlocGlNR1NLNDBEMUJ6LUV3RVlPa3FuS0lQVVRFdzBWaTNjcGl1c0VzdkFHTnJJVEQwY3loeWlaMjVQSmdjMVNiNEpTcnU0X01vcWlRaVJFVnN3WVJkLURxRkpRdlBMWQ&q=https%3A%2F%2Fdiscord.gg%2F3zy8kqD9Cp)

##  Corrections

[ 57:00 ](/watch?v=kCc8FmEb1nY&t=3420s)

Oops "tokens from the _future_ cannot communicate", not "past". Sorry! :)

[ 1:20:05 ](/watch?v=kCc8FmEb1nY&t=4805s)

Oops I should be using the head_size for the normalization, not C

##  Transcript

NaN / NaN

[ ![](https://i.ytimg.com/vi/zduSFxRajkE/hqdefault.jpg?sqp=-oaymwEmCKgBEF5IWvKriqkDGQgBFQAAiEIYAdgBAeIBCggYEAIYBjgBQAE=&rs=AOn4CLB21U29jW1WqoHBynSKXC6yq968Pw) 2:13:35 2:13:35 Now playing ](/watch?v=zduSFxRajkE)

### [ Let's build the GPT Tokenizer  Andrej Karpathy Andrej Karpathy  • • 679K views 11 months ago ](/watch?v=zduSFxRajkE)

[ ![](https://i.ytimg.com/vi/wjZofJX0v4M/hqdefault.jpg?sqp=-oaymwEmCKgBEF5IWvKriqkDGQgBFQAAiEIYAdgBAeIBCggYEAIYBjgBQAE=&rs=AOn4CLDnN43ABWZWmhimtbs0_Evpxgj0IQ) 27:14 27:14 Now playing ](/watch?v=wjZofJX0v4M)

### [ Transformers (how LLMs work) explained visually | DL5  3Blue1Brown 3Blue1Brown  Verified  • • 4.4M views 9 months ago ](/watch?v=wjZofJX0v4M)

[ ![](https://i.ytimg.com/vi/jfKfPfyJRdk/hqdefault.jpg?v=665dffb9&sqp=-oaymwEmCKgBEF5IWvKriqkDGQgBFQAAiEIYAdgBAeIBCggYEAIYBjgBQAE=&rs=AOn4CLAPQlls0D-iIDsUxXGtm2b3tP1KLg) Now playing ](/watch?v=jfKfPfyJRdk)

### [ lofi hip hop radio 📚 beats to relax/study to  Lofi Girl Lofi Girl  Verified  • • 24K watching LIVE ](/watch?v=jfKfPfyJRdk)

[ ![](https://i.ytimg.com/vi/tx5OapbK-8A/hqdefault.jpg?sqp=-oaymwEmCKgBEF5IWvKriqkDGQgBFQAAiEIYAdgBAeIBCggYEAIYBjgBQAE=&rs=AOn4CLBOp2Kn6riFBoz0VqGKAPTuWnhYlg) 24:27 24:27 Now playing ](/watch?v=tx5OapbK-8A)

### [ How to Build Effective AI Agents (without the hype)  Dave Ebbelaar Dave Ebbelaar  Verified  • • 21K views 21 hours ago New ](/watch?v=tx5OapbK-8A)

[ ![](https://i.ytimg.com/vi/KJtZARuO3JY/hqdefault.jpg?sqp=-oaymwEmCKgBEF5IWvKriqkDGQgBFQAAiEIYAdgBAeIBCggYEAIYBjgBQAE=&rs=AOn4CLDnqdN8UlSGGs_v6hoJXerC7v9atQ) 57:45 57:45 Now playing ](/watch?v=KJtZARuO3JY)

### [ Visualizing transformers and attention | Talk for TNG Big Tech Day '24  Grant Sanderson Grant Sanderson  • • 327K views 2 months ago ](/watch?v=KJtZARuO3JY)

[ ![](https://i.ytimg.com/vi/oirRMykdd5E/hqdefault.jpg?v=65eea272&sqp=-oaymwEmCKgBEF5IWvKriqkDGQgBFQAAiEIYAdgBAeIBCggYEAIYBjgBQAE=&rs=AOn4CLAiWnly9Wxs8D0dFxjtKxPx-XfsSQ) Now playing ](/watch?v=oirRMykdd5E)

### [ 40 Hz Brain Activation Binaural Beats: Activate 100% of Your Brain, Gamma Waves  Good Vibes - Binaural Beats Good Vibes - Binaural Beats  Verified  • • 579 watching LIVE ](/watch?v=oirRMykdd5E)

[ ![](https://i.ytimg.com/vi/KrRD7r7y7NY/hqdefault.jpg?sqp=-oaymwEmCKgBEF5IWvKriqkDGQgBFQAAiEIYAdgBAeIBCggYEAIYBjgBQAE=&rs=AOn4CLDBjwBx0lz_o-lQxJWnouGfXi1s8Q) 26:52 26:52 Now playing ](/watch?v=KrRD7r7y7NY)

### [ Andrew Ng Explores The Rise Of AI Agents And Agentic Reasoning | BUILD 2024 Keynote  Snowflake Inc. Snowflake Inc.  • • 468K views 2 months ago ](/watch?v=KrRD7r7y7NY)

[ ![](https://i.ytimg.com/vi/9UMxZofMNbA/hqdefault.jpg?v=60c5e7c1&sqp=-oaymwEmCKgBEF5IWvKriqkDGQgBFQAAiEIYAdgBAeIBCggYEAIYBjgBQAE=&rs=AOn4CLDz945mCZzYjEVboGmf8K52qtG0kA) Now playing ](/watch?v=9UMxZofMNbA)

### [ Chillout Lounge - Calm & Relaxing Background Music | Study, Work, Sleep, Meditation, Chill  The Good Life Radio x Sensual Musique The Good Life Radio x Sensual Musique  Verified  • • 2.8K watching LIVE ](/watch?v=9UMxZofMNbA)

[ ![](https://i.ytimg.com/vi/zjkBMFhNj_g/hqdefault.jpg?sqp=-oaymwEmCKgBEF5IWvKriqkDGQgBFQAAiEIYAdgBAeIBCggYEAIYBjgBQAE=&rs=AOn4CLB2kNx2uAWCSCyVhYBI7zzGHRGqDg) 59:48 59:48 Now playing ](/watch?v=zjkBMFhNj_g)

### [ [1hr Talk] Intro to Large Language Models  Andrej Karpathy Andrej Karpathy  • • 2.4M views 1 year ago ](/watch?v=zjkBMFhNj_g)

[ ![](https://i.ytimg.com/vi/l8pRSuU81PU/hqdefault.jpg?sqp=-oaymwEmCKgBEF5IWvKriqkDGQgBFQAAiEIYAdgBAeIBCggYEAIYBjgBQAE=&rs=AOn4CLB0wsEVQyVgeM5QKJ-_A8UrgL0lkA) 4:01:26 4:01:26 Now playing ](/watch?v=l8pRSuU81PU)

### [ Let's reproduce GPT-2 (124M)  Andrej Karpathy Andrej Karpathy  • • 682K views 7 months ago ](/watch?v=l8pRSuU81PU)

[ ![](https://i.ytimg.com/vi/zc5NTeJbk-k/hqdefault.jpg?sqp=-oaymwEmCKgBEF5IWvKriqkDGQgBFQAAiEIYAdgBAeIBCggYEAIYBjgBQAE=&rs=AOn4CLD0YsPGVIaP4w--5mbFaiwQ1MRAGQ) 20:18 20:18 Now playing ](/watch?v=zc5NTeJbk-k)

### [ Why Does Diffusion Work Better than Auto-Regression?  Algorithmic Simplicity Algorithmic Simplicity  • • 433K views 11 months ago ](/watch?v=zc5NTeJbk-k)

[ 1:44:31 1:44:31 Now playing ](/watch?v=9vM4p9NN0Ts)

### [ Stanford CS229 I Machine Learning I Building Large Language Models (LLMs)  Stanford Online Stanford Online  Verified  • • 755K views 4 months ago ](/watch?v=9vM4p9NN0Ts)

[ 17:57 17:57 Now playing ](/watch?v=2IK3DFHRFfw)

### [ Generative AI in a Nutshell - how to survive and thrive in the age of AI  Henrik Kniberg Henrik Kniberg  • • 2.6M views 1 year ago ](/watch?v=2IK3DFHRFfw)

[ 22:13 22:13 Now playing ](/watch?v=WxYC9-hBM_g)

### [ Run your own AI (but private)  NetworkChuck NetworkChuck  Verified  • • 1.8M views 10 months ago ](/watch?v=WxYC9-hBM_g)

[ 58:04 58:04 Now playing ](/watch?v=bCz4OMemCcA)

### [ Attention is all you need (Transformer) - Model explanation (including math), Inference and Training  Umar Jamil Umar Jamil  • • 454K views 1 year ago ](/watch?v=bCz4OMemCcA)

[ 42:40 42:40 Now playing ](/watch?v=bZQun8Y4L2A)

### [ State of GPT | BRK216HFS  Microsoft Developer Microsoft Developer  Verified  • • 692K views 1 year ago ](/watch?v=bZQun8Y4L2A)

[ 18:44 18:44 Now playing ](/watch?v=JWfNLF_g_V0)

### [ Turn ANY Website into LLM Knowledge in SECONDS  Cole Medin Cole Medin  • • 112K views 8 days ago ](/watch?v=JWfNLF_g_V0)

[ 2:57:24 2:57:24 Now playing ](/watch?v=GWB9ApTPTv4)

### [ Ollama Course – Build AI Apps Locally  freeCodeCamp.org freeCodeCamp.org  Verified  • • 199K views 1 month ago ](/watch?v=GWB9ApTPTv4)

[ 23:21 23:21 Now playing ](/watch?v=aR6CzM0x-g0)

### [ CUDA Mode Keynote | Andrej Karpathy | Eureka Labs  Accel Accel  • • 21K views 3 months ago ](/watch?v=aR6CzM0x-g0)

[ 1:57:45 1:57:45 Now playing ](/watch?v=PaCmpygFfXo)

### [ The spelled-out intro to language modeling: building makemore  Andrej Karpathy Andrej Karpathy  • • 789K views 2 years ago ](/watch?v=PaCmpygFfXo)

Show more

Saving your choice

An error occurred while saving your choice. Try again.

A Google company

en-GB

Afrikaans Azərbaycan Bahasa Indonesia Bahasa Malaysia Bosanski Català Čeština Dansk Deutsch Eesti English (India) English (UK) English (US) Español (España) Español (Latinoamérica) Español (US) Euskara Filipino Français Français (Canada) Galego Hrvatski IsiZulu Íslenska Italiano Kiswahili Latviešu valoda Lietuvių Magyar Nederlands Norsk O‘zbek Polski Português Português (Brasil) Română Shqip Slovenčina Slovenščina Srpski Suomi Svenska Tiếng Việt Türkçe Беларуская Български Кыргызча Қазақ Тілі Македонски Монгол Русский Српски Українська Ελληνικά Հայերեն עברית اردو العربية فارسی नेपाली मराठी हिन्दी অসমীয়া বাংলা ਪੰਜਾਬੀ ગુજરાતી ଓଡ଼ିଆ தமிழ் తెలుగు ಕನ್ನಡ മലയാളം සිංහල ภาษาไทย ລາວ ဗမာ ქართული አማርኛ ខ្មែរ 中文 (简体) 中文 (繁體) 中文 (香港) 日本語 한국어

[Sign in](https://accounts.google.com/ServiceLogin?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den-GB%26next%3Dhttps%253A%252F%252Fwww.youtube.com%252Fwatch%253Fapp%253Ddesktop%2526v%253DkCc8FmEb1nY%25252F&hl=en-GB)

Sign in 

##  Before you continue to YouTube

We use [cookies](https://policies.google.com/technologies/cookies?hl=en-GB) and data to

  * Deliver and maintain Google services

  * Track outages and protect against spam, fraud and abuse

  * Measure audience engagement and site statistics to understand how our services are used and enhance the quality of those services




If you choose to 'Accept all', we will also use cookies and data to

  * Develop and improve new services

  * Deliver and measure the effectiveness of ads

  * Show personalised content, depending on your settings

  * Show personalised ads, depending on your settings




If you choose to 'Reject all', we will not use cookies for these additional purposes.

Non-personalised content and ads are influenced by things like the content that you’re currently viewing and your location (ad serving is based on general location). Personalised content and ads can also include things like video recommendations, a customised YouTube homepage and tailored ads based on past activity, like the videos that you watch and the things that you search for on YouTube. We also use cookies and data to tailor the experience to be age-appropriate, if relevant.

Select 'More options' to see additional information, including details about managing your privacy settings. You can also visit g.co/privacytools at any time.

Reject all

Accept all

[More options](https://consent.youtube.com/d?continue=https://www.youtube.com/watch%3Fapp%3Ddesktop%26v%3DkCc8FmEb1nY%252F%26cbrd%3D1&gl=RO&m=0&pc=yt&cm=2&hl=en-GB&src=2)

[Privacy Policy](https://policies.google.com/privacy?hl=en-GB) • [Terms of Service](https://policies.google.com/terms?hl=en-GB)

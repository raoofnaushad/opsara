{
    "id": "7d3aa811edfcea2ab4fd0a709dcce8eb",
    "metadata": {
        "id": "7d3aa811edfcea2ab4fd0a709dcce8eb",
        "url": "https://huggingface.co/docs/sagemaker/en/inference/",
        "title": "Deploy models to Amazon SageMaker",
        "properties": {
            "description": "We‚Äôre on a journey to advance and democratize artificial intelligence through open source and open science.",
            "keywords": null,
            "author": null,
            "og:title": "Deploy models to Amazon SageMaker",
            "og:type": "website",
            "og:url": "https://huggingface.co/docs/sagemaker/en/inference",
            "og:image": "https://huggingface.co/front/thumbnails/docs/sagemaker.png",
            "twitter:card": "summary_large_image",
            "twitter:site": "@huggingface",
            "twitter:image": "https://huggingface.co/front/thumbnails/docs/sagemaker.png"
        }
    },
    "parent_metadata": {
        "id": "bbaffb61d956ddd61fac4ad86b32b9e6",
        "url": "https://www.notion.so/SageMaker-bbaffb61d956ddd61fac4ad86b32b9e6",
        "title": "SageMaker",
        "properties": {
            "Type": "Leaf"
        }
    },
    "content": "[![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg) Hugging Face](/)\n\n  * [ Models](/models)\n  * [ Datasets](/datasets)\n  * [ Spaces](/spaces)\n  * [ Posts](/posts)\n  * [ Docs](/docs)\n  * [ Enterprise](/enterprise)\n  * [Pricing](/pricing)\n  * [Log In](/login)\n  * [Sign Up](/join)\n\n\n\nAmazon SageMaker documentation\n\nDeploy models to Amazon SageMaker\n\n# Amazon SageMaker\n\nüè° View all docsAWS Trainium & InferentiaAccelerateAmazon SageMakerArgillaAutoTrainBitsandbytesChat UICompetitionsDataset viewerDatasetsDiffusersDistilabelEvaluateGoogle CloudGoogle TPUsGradioHubHub Python LibraryHugging Face Generative AI Services (HUGS)Huggingface.jsInference API (serverless)Inference Endpoints (dedicated)LeaderboardsLightevalOptimumPEFTSafetensorsSentence TransformersTRLTasksText Embeddings InferenceText Generation InferenceTokenizersTransformersTransformers.jssmolagentstimm\n\nSearch documentation\n\n`‚åòK`\n\nmain EN [ 321](https://github.com/huggingface/hub-docs)\n\n[Hugging Face on Amazon SageMaker ](/docs/sagemaker/en/index)[Get started ](/docs/sagemaker/en/getting-started)[Run training on Amazon SageMaker ](/docs/sagemaker/en/train)[Deploy models to Amazon SageMaker ](/docs/sagemaker/en/inference)[Reference ](/docs/sagemaker/en/reference)\n\n![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg)\n\nJoin the Hugging Face community\n\nand get access to the augmented documentation experience \n\nCollaborate on models, datasets and Spaces \n\nFaster examples with accelerated inference \n\nSwitch between documentation themes \n\n[Sign Up](/join)\n\nto get started\n\n# [](#deploy-models-to-amazon-sagemaker) Deploy models to Amazon SageMaker\n\nDeploying a ü§ó Transformers models in SageMaker for inference is as easy as:\n\nCopied\n\n```\nfrom sagemaker.huggingface import HuggingFaceModel # create Hugging Face Model Class and deploy it as SageMaker endpoint huggingface_model = HuggingFaceModel(...).deploy()\n```\n\nThis guide will show you how to deploy models with zero-code using the [Inference Toolkit](https://github.com/aws/sagemaker-huggingface-inference-toolkit). The Inference Toolkit builds on top of the [`pipeline` feature](https://huggingface.co/docs/transformers/main_classes/pipelines) from ü§ó Transformers. Learn how to:\n\n  * [Install and setup the Inference Toolkit](#installation-and-setup).\n  * [Deploy a ü§ó Transformers model trained in SageMaker](#deploy-a-transformer-model-trained-in-sagemaker).\n  * [Deploy a ü§ó Transformers model from the Hugging Face [model Hub](https://huggingface.co/models)](#deploy-a-model-from-the-hub).\n  * [Run a Batch Transform Job using ü§ó Transformers and Amazon SageMaker](#run-batch-transform-with-transformers-and-sagemaker).\n  * [Create a custom inference module](#user-defined-code-and-modules).\n\n\n\n## [](#installation-and-setup) Installation and setup\n\nBefore deploying a ü§ó Transformers model to SageMaker, you need to sign up for an AWS account. If you don‚Äôt have an AWS account yet, learn more [here](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-set-up.html).\n\nOnce you have an AWS account, get started using one of the following:\n\n  * [SageMaker Studio](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-studio-onboard.html)\n  * [SageMaker notebook instance](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-console.html)\n  * Local environment\n\n\n\nTo start training locally, you need to setup an appropriate [IAM role](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html).\n\nUpgrade to the latest `sagemaker` version.\n\nCopied\n\n```\npip install sagemaker --upgrade\n```\n\n**SageMaker environment**\n\nSetup your SageMaker environment as shown below:\n\nCopied\n\n```\nimport sagemaker sess = sagemaker.Session() role = sagemaker.get_execution_role()\n```\n\n_Note: The execution role is only available when running a notebook within SageMaker. If you run`get_execution_role` in a notebook not on SageMaker, expect a `region` error._\n\n**Local environment**\n\nSetup your local environment as shown below:\n\nCopied\n\n```\nimport sagemaker import boto3 iam_client = boto3.client('iam') role = iam_client.get_role(RoleName='role-name-of-your-iam-role-with-right-permissions')['Role']['Arn'] sess = sagemaker.Session()\n```\n\n## [](#deploy-a--transformers-model-trained-in-sagemaker) Deploy a ü§ó Transformers model trained in SageMaker\n\nThere are two ways to deploy your Hugging Face model trained in SageMaker:\n\n  * Deploy it after your training has finished.\n  * Deploy your saved model at a later time from S3 with the `model_data`.\n\n\n\nüìì Open the [deploy_transformer_model_from_s3.ipynb notebook](https://github.com/huggingface/notebooks/blob/main/sagemaker/10_deploy_model_from_s3/deploy_transformer_model_from_s3.ipynb) for an example of how to deploy a model from S3 to SageMaker for inference.\n\n### [](#deploy-after-training) Deploy after training\n\nTo deploy your model directly after training, ensure all required files are saved in your training script, including the tokenizer and the model.\n\nIf you use the Hugging Face `Trainer`, you can pass your tokenizer as an argument to the `Trainer`. It will be automatically saved when you call `trainer.save_model()`.\n\nCopied\n\n```\nfrom sagemaker.huggingface import HuggingFace ############ pseudo code start ############ # create Hugging Face Estimator for training huggingface_estimator = HuggingFace(....) # start the train job with our uploaded datasets as input huggingface_estimator.fit(...) ############ pseudo code end ############ # deploy model to SageMaker Inference predictor = hf_estimator.deploy(initial_instance_count=1, instance_type=\"ml.m5.xlarge\") # example request: you always need to define \"inputs\" data = { \"inputs\": \"Camera - You are awarded a SiPix Digital Camera! call 09061221066 fromm landline. Delivery within 28 days.\" } # request predictor.predict(data)\n```\n\nAfter you run your request you can delete the endpoint as shown:\n\nCopied\n\n```\n# delete endpoint predictor.delete_endpoint()\n```\n\n### [](#deploy-with-modeldata) Deploy with model_data\n\nIf you‚Äôve already trained your model and want to deploy it at a later time, use the `model_data` argument to specify the location of your tokenizer and model weights.\n\nCopied\n\n```\nfrom sagemaker.huggingface.model import HuggingFaceModel # create Hugging Face Model Class huggingface_model = HuggingFaceModel( model_data=\"s3://models/my-bert-model/model.tar.gz\", # path to your trained SageMaker model role=role, # IAM role with permissions to create an endpoint transformers_version=\"4.26\", # Transformers version used pytorch_version=\"1.13\", # PyTorch version used py_version='py39', # Python version used ) # deploy model to SageMaker Inference predictor = huggingface_model.deploy( initial_instance_count=1, instance_type=\"ml.m5.xlarge\" ) # example request: you always need to define \"inputs\" data = { \"inputs\": \"Camera - You are awarded a SiPix Digital Camera! call 09061221066 fromm landline. Delivery within 28 days.\" } # request predictor.predict(data)\n```\n\nAfter you run our request, you can delete the endpoint again with:\n\nCopied\n\n```\n# delete endpoint predictor.delete_endpoint()\n```\n\n### [](#create-a-model-artifact-for-deployment) Create a model artifact for deployment\n\nFor later deployment, you can create a `model.tar.gz` file that contains all the required files, such as:\n\n  * `pytorch_model.bin`\n  * `tf_model.h5`\n  * `tokenizer.json`\n  * `tokenizer_config.json`\n\n\n\nFor example, your file should look like this:\n\nCopied\n\n```\nmodel.tar.gz/ |- pytorch_model.bin |- vocab.txt |- tokenizer_config.json |- config.json |- special_tokens_map.json\n```\n\nCreate your own `model.tar.gz` from a model from the ü§ó Hub:\n\n  1. Download a model:\n\n\n\nCopied\n\n```\ngit lfs install git clone git@hf.co:{repository}\n```\n\n  1. Create a `tar` file:\n\n\n\nCopied\n\n```\ncd {repository} tar zcvf model.tar.gz *\n```\n\n  1. Upload `model.tar.gz` to S3:\n\n\n\nCopied\n\n```\naws s3 cp model.tar.gz <s3://{my-s3-path}>\n```\n\nNow you can provide the S3 URI to the `model_data` argument to deploy your model later.\n\n## [](#deploy-a-model-from-the--hub) Deploy a model from the ü§ó Hub\n\nTo deploy a model directly from the ü§ó Hub to SageMaker, define two environment variables when you create a `HuggingFaceModel`:\n\n  * `HF_MODEL_ID` defines the model ID which is automatically loaded from [huggingface.co/models](http://huggingface.co/models) when you create a SageMaker endpoint. Access 10,000+ models on he ü§ó Hub through this environment variable.\n  * `HF_TASK` defines the task for the ü§ó Transformers `pipeline`. A complete list of tasks can be found [here](https://huggingface.co/docs/transformers/main_classes/pipelines).\n\n\n\n> ‚ö†Ô∏è ** Pipelines are not optimized for parallelism (multi-threading) and tend to consume a lot of RAM. For example, on a GPU-based instance, the pipeline operates on a single vCPU. When this vCPU becomes saturated with the inference requests preprocessing, it can create a bottleneck, preventing the GPU from being fully utilized for model inference. Learn more [here](https://huggingface.co/docs/transformers/en/pipeline_webserver#using-pipelines-for-a-webserver)\n\nCopied\n\n```\nfrom sagemaker.huggingface.model import HuggingFaceModel # Hub model configuration <https://huggingface.co/models> hub = { 'HF_MODEL_ID':'distilbert-base-uncased-distilled-squad', # model_id from hf.co/models 'HF_TASK':'question-answering' # NLP task you want to use for predictions } # create Hugging Face Model Class huggingface_model = HuggingFaceModel( env=hub, # configuration for loading model from Hub role=role, # IAM role with permissions to create an endpoint transformers_version=\"4.26\", # Transformers version used pytorch_version=\"1.13\", # PyTorch version used py_version='py39', # Python version used ) # deploy model to SageMaker Inference predictor = huggingface_model.deploy( initial_instance_count=1, instance_type=\"ml.m5.xlarge\" ) # example request: you always need to define \"inputs\" data = { \"inputs\": { \"question\": \"What is used for inference?\", \"context\": \"My Name is Philipp and I live in Nuremberg. This model is used with sagemaker for inference.\" } } # request predictor.predict(data)\n```\n\nAfter you run our request, you can delete the endpoint again with:\n\nCopied\n\n```\n# delete endpoint predictor.delete_endpoint()\n```\n\nüìì Open the [deploy_transformer_model_from_hf_hub.ipynb notebook](https://github.com/huggingface/notebooks/blob/main/sagemaker/11_deploy_model_from_hf_hub/deploy_transformer_model_from_hf_hub.ipynb) for an example of how to deploy a model from the ü§ó Hub to SageMaker for inference.\n\n## [](#run-batch-transform-with--transformers-and-sagemaker) Run batch transform with ü§ó Transformers and SageMaker\n\nAfter training a model, you can use [SageMaker batch transform](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-batch.html) to perform inference with the model. Batch transform accepts your inference data as an S3 URI and then SageMaker will take care of downloading the data, running the prediction, and uploading the results to S3. For more details about batch transform, take a look [here](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html).\n\n‚ö†Ô∏è The Hugging Face Inference DLC currently only supports `.jsonl` for batch transform due to the complex structure of textual data.\n\n_Note: Make sure your`inputs` fit the `max_length` of the model during preprocessing._\n\nIf you trained a model using the Hugging Face Estimator, call the `transformer()` method to create a transform job for a model based on the training job (see [here](https://sagemaker.readthedocs.io/en/stable/overview.html#sagemaker-batch-transform) for more details):\n\nCopied\n\n```\nbatch_job = huggingface_estimator.transformer( instance_count=1, instance_type='ml.p3.2xlarge', strategy='SingleRecord') batch_job.transform( data='s3://s3-uri-to-batch-data', content_type='application/json', split_type='Line')\n```\n\nIf you want to run your batch transform job later or with a model from the ü§ó Hub, create a `HuggingFaceModel` instance and then call the `transformer()` method:\n\nCopied\n\n```\nfrom sagemaker.huggingface.model import HuggingFaceModel # Hub model configuration <https://huggingface.co/models> hub = { 'HF_MODEL_ID':'distilbert/distilbert-base-uncased-finetuned-sst-2-english', 'HF_TASK':'text-classification' } # create Hugging Face Model Class huggingface_model = HuggingFaceModel( env=hub, # configuration for loading model from Hub role=role, # IAM role with permissions to create an endpoint transformers_version=\"4.26\", # Transformers version used pytorch_version=\"1.13\", # PyTorch version used py_version='py39', # Python version used ) # create transformer to run a batch job batch_job = huggingface_model.transformer( instance_count=1, instance_type='ml.p3.2xlarge', strategy='SingleRecord' ) # starts batch transform job and uses S3 data as input batch_job.transform( data='s3://sagemaker-s3-demo-test/samples/input.jsonl', content_type='application/json', split_type='Line' )\n```\n\nThe `input.jsonl` looks like this:\n\nCopied\n\n```\n{\"inputs\":\"this movie is terrible\"} {\"inputs\":\"this movie is amazing\"} {\"inputs\":\"SageMaker is pretty cool\"} {\"inputs\":\"SageMaker is pretty cool\"} {\"inputs\":\"this movie is terrible\"} {\"inputs\":\"this movie is amazing\"}\n```\n\nüìì Open the [sagemaker-notebook.ipynb notebook](https://github.com/huggingface/notebooks/blob/main/sagemaker/12_batch_transform_inference/sagemaker-notebook.ipynb) for an example of how to run a batch transform job for inference.\n\n## [](#deploy-an-llm-to-sagemaker-using-tgi) Deploy an LLM to SageMaker using TGI\n\nIf you are interested in using a high-performance serving container for LLMs, you can use the Hugging Face TGI container. This utilizes the [Text Generation Inference](https://github.com/huggingface/text-generation-inference) library. A list of compatible models can be found [here](https://huggingface.co/docs/text-generation-inference/supported_models#supported-models).\n\nFirst, make sure that the latest version of SageMaker SDK is installed:\n\nCopied\n\n```\npip install sagemaker>=2.231.0\n```\n\nThen, we import the SageMaker Python SDK and instantiate a sagemaker_session to find the current region and execution role.\n\nCopied\n\n```\nimport sagemaker from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri import time sagemaker_session = sagemaker.Session() region = sagemaker_session.boto_region_name role = sagemaker.get_execution_role()\n```\n\nNext we retrieve the LLM image URI. We use the helper function get_huggingface_llm_image_uri() to generate the appropriate image URI for the Hugging Face Large Language Model (LLM) inference. The function takes a required parameter backend and several optional parameters. The backend specifies the type of backend to use for the model: ‚Äúhuggingface‚Äù refers to using Hugging Face TGI backend.\n\nCopied\n\n```\nimage_uri = get_huggingface_llm_image_uri( backend=\"huggingface\", region=region )\n```\n\nNow that we have the image uri, the next step is to configure the model object. We specify a unique name, the image_uri for the managed TGI container, and the execution role for the endpoint. Additionally, we specify a number of environment variables including the `HF_MODEL_ID` which corresponds to the model from the HuggingFace Hub that will be deployed, and the `HF_TASK` which configures the inference task to be performed by the model.\n\nYou should also define `SM_NUM_GPUS`, which specifies the tensor parallelism degree of the model. Tensor parallelism can be used to split the model across multiple GPUs, which is necessary when working with LLMs that are too big for a single GPU. To learn more about tensor parallelism with inference, see our previous blog post. Here, you should set `SM_NUM_GPUS` to the number of available GPUs on your selected instance type. For example, in this tutorial, we set `SM_NUM_GPUS` to 4 because our selected instance type ml.g4dn.12xlarge has 4 available GPUs.\n\nNote that you can optionally reduce the memory and computational footprint of the model by setting the `HF_MODEL_QUANTIZE` environment variable to `true`, but this lower weight precision could affect the quality of the output for some models.\n\nCopied\n\n```\nmodel_name = \"llama-3-1-8b-instruct\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime()) hub = { 'HF_MODEL_ID':'meta-llama/Llama-3.1-8B-Instruct', 'SM_NUM_GPUS':'1', 'HUGGING_FACE_HUB_TOKEN': '<REPLACE WITH YOUR TOKEN>', } assert hub['HUGGING_FACE_HUB_TOKEN'] != '<REPLACE WITH YOUR TOKEN>', \"You have to provide a token.\" model = HuggingFaceModel( name=model_name, env=hub, role=role, image_uri=image_uri )\n```\n\nNext, we invoke the deploy method to deploy the model.\n\nCopied\n\n```\npredictor = model.deploy( initial_instance_count=1, instance_type=\"ml.g5.2xlarge\", endpoint_name=model_name )\n```\n\nOnce the model is deployed, we can invoke it to generate text. We pass an input prompt and run the predict method to generate a text response from the LLM running in the TGI container.\n\nCopied\n\n```\ninput_data = { \"inputs\": \"The diamondback terrapin was the first reptile to\", \"parameters\": { \"do_sample\": True, \"max_new_tokens\": 100, \"temperature\": 0.7, \"watermark\": True } } predictor.predict(input_data)\n```\n\nWe receive the following auto-generated text response:\n\nCopied\n\n```\n[{'generated_text': 'The diamondback terrapin was the first reptile to make the list, followed by the American alligator, the American crocodile, and the American box turtle. The polecat, a ferret-like animal, and the skunk rounded out the list, both having gained their slots because they have proven to be particularly dangerous to humans.\\n\\nCalifornians also seemed to appreciate the new list, judging by the comments left after the election.\\n\\n‚ÄúThis is fantastic,‚Äù one commenter declared.\\n\\n‚ÄúCalifornia is a very'}]\n```\n\nOnce we are done experimenting, we delete the endpoint and the model resources.\n\nCopied\n\n```\npredictor.delete_model() predictor.delete_endpoint()\n```\n\n## [](#user-defined-code-and-modules) User defined code and modules\n\nThe Hugging Face Inference Toolkit allows the user to override the default methods of the `HuggingFaceHandlerService`. You will need to create a folder named `code/` with an `inference.py` file in it. See [here](#create-a-model-artifact-for-deployment) for more details on how to archive your model artifacts. For example:\n\nCopied\n\n```\nmodel.tar.gz/ |- pytorch_model.bin |- .... |- code/ |- inference.py |- requirements.txt \n```\n\nThe `inference.py` file contains your custom inference module, and the `requirements.txt` file contains additional dependencies that should be added. The custom module can override the following methods:\n\n  * `model_fn(model_dir)` overrides the default method for loading a model. The return value `model` will be used in `predict` for predictions. `predict` receives argument the `model_dir`, the path to your unzipped `model.tar.gz`.\n  * `transform_fn(model, data, content_type, accept_type)` overrides the default transform function with your custom implementation. You will need to implement your own `preprocess`, `predict` and `postprocess` steps in the `transform_fn`. This method can‚Äôt be combined with `input_fn`, `predict_fn` or `output_fn` mentioned below.\n  * `input_fn(input_data, content_type)` overrides the default method for preprocessing. The return value `data` will be used in `predict` for predictions. The inputs are:\n    * `input_data` is the raw body of your request.\n    * `content_type` is the content type from the request header.\n  * `predict_fn(processed_data, model)` overrides the default method for predictions. The return value `predictions` will be used in `postprocess`. The input is `processed_data`, the result from `preprocess`.\n  * `output_fn(prediction, accept)` overrides the default method for postprocessing. The return value `result` will be the response of your request (e.g.`JSON`). The inputs are:\n    * `predictions` is the result from `predict`.\n    * `accept` is the return accept type from the HTTP Request, e.g. `application/json`.\n\n\n\nHere is an example of a custom inference module with `model_fn`, `input_fn`, `predict_fn`, and `output_fn`:\n\nCopied\n\n```\nfrom sagemaker_huggingface_inference_toolkit import decoder_encoder def model_fn(model_dir): # implement custom code to load the model loaded_model = ... return loaded_model def input_fn(input_data, content_type): # decode the input data (e.g. JSON string -> dict) data = decoder_encoder.decode(input_data, content_type) return data def predict_fn(data, model): # call your custom model with the data outputs = model(data , ... ) return predictions def output_fn(prediction, accept): # convert the model output to the desired output format (e.g. dict -> JSON string) response = decoder_encoder.encode(prediction, accept) return response\n```\n\nCustomize your inference module with only `model_fn` and `transform_fn`:\n\nCopied\n\n```\nfrom sagemaker_huggingface_inference_toolkit import decoder_encoder def model_fn(model_dir): # implement custom code to load the model loaded_model = ... return loaded_model def transform_fn(model, input_data, content_type, accept): # decode the input data (e.g. JSON string -> dict) data = decoder_encoder.decode(input_data, content_type) # call your custom model with the data outputs = model(data , ... ) # convert the model output to the desired output format (e.g. dict -> JSON string) response = decoder_encoder.encode(output, accept) return response\n```\n\n[< > Update on GitHub](https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/inference.md)\n\n[‚ÜêRun training on Amazon SageMaker](/docs/sagemaker/en/train) [Reference‚Üí](/docs/sagemaker/en/reference)\n\n[Deploy models to Amazon SageMaker](#deploy-models-to-amazon-sagemaker) [Installation and setup](#installation-and-setup) [Deploy a ü§ó Transformers model trained in SageMaker](#deploy-a--transformers-model-trained-in-sagemaker) [Deploy after training](#deploy-after-training) [Deploy with model_data](#deploy-with-modeldata) [Create a model artifact for deployment](#create-a-model-artifact-for-deployment) [Deploy a model from the ü§ó Hub](#deploy-a-model-from-the--hub) [Run batch transform with ü§ó Transformers and SageMaker](#run-batch-transform-with--transformers-and-sagemaker) [Deploy an LLM to SageMaker using TGI](#deploy-an-llm-to-sagemaker-using-tgi) [User defined code and modules](#user-defined-code-and-modules)\n",
    "content_quality_score": 0.9,
    "summary": null,
    "child_urls": [
        "https://huggingface.co/",
        "https://huggingface.co/models",
        "https://huggingface.co/datasets",
        "https://huggingface.co/spaces",
        "https://huggingface.co/posts",
        "https://huggingface.co/docs",
        "https://huggingface.co/enterprise",
        "https://huggingface.co/pricing",
        "https://huggingface.co/login",
        "https://huggingface.co/join",
        "https://huggingface.co/docs/sagemaker/en/index",
        "https://huggingface.co/docs/sagemaker/en/getting-started",
        "https://huggingface.co/docs/sagemaker/en/train",
        "https://huggingface.co/docs/sagemaker/en/inference",
        "https://huggingface.co/docs/sagemaker/en/reference",
        "https://huggingface.co/docs/sagemaker/en/inference/#deploy-models-to-amazon-sagemaker",
        "https://huggingface.co/docs/transformers/main_classes/pipelines",
        "https://huggingface.co/docs/sagemaker/en/inference/#installation-and-setup",
        "https://huggingface.co/docs/sagemaker/en/inference/#deploy-a-transformer-model-trained-in-sagemaker",
        "https://huggingface.co/docs/sagemaker/en/inference/#deploy-a-model-from-the-hub",
        "https://huggingface.co/docs/sagemaker/en/inference/#run-batch-transform-with-transformers-and-sagemaker",
        "https://huggingface.co/docs/sagemaker/en/inference/#user-defined-code-and-modules",
        "https://huggingface.co/docs/sagemaker/en/inference/#deploy-a--transformers-model-trained-in-sagemaker",
        "https://huggingface.co/docs/sagemaker/en/inference/#deploy-after-training",
        "https://huggingface.co/docs/sagemaker/en/inference/#deploy-with-modeldata",
        "https://huggingface.co/docs/sagemaker/en/inference/#create-a-model-artifact-for-deployment",
        "https://huggingface.co/docs/sagemaker/en/inference/#deploy-a-model-from-the--hub",
        "http://huggingface.co/models",
        "https://huggingface.co/docs/transformers/en/pipeline_webserver#using-pipelines-for-a-webserver",
        "https://huggingface.co/docs/sagemaker/en/inference/#run-batch-transform-with--transformers-and-sagemaker",
        "https://huggingface.co/docs/sagemaker/en/inference/#deploy-an-llm-to-sagemaker-using-tgi",
        "https://huggingface.co/docs/text-generation-inference/supported_models#supported-models",
        "https://github.com/huggingface/hub-docs",
        "https://github.com/aws/sagemaker-huggingface-inference-toolkit",
        "https://docs.aws.amazon.com/sagemaker/latest/dg/gs-set-up.html",
        "https://docs.aws.amazon.com/sagemaker/latest/dg/gs-studio-onboard.html",
        "https://docs.aws.amazon.com/sagemaker/latest/dg/gs-console.html",
        "https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html",
        "https://github.com/huggingface/notebooks/blob/main/sagemaker/10_deploy_model_from_s3/deploy_transformer_model_from_s3.ipynb",
        "https://github.com/huggingface/notebooks/blob/main/sagemaker/11_deploy_model_from_hf_hub/deploy_transformer_model_from_hf_hub.ipynb",
        "https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-batch.html",
        "https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html",
        "https://sagemaker.readthedocs.io/en/stable/overview.html#sagemaker-batch-transform",
        "https://github.com/huggingface/notebooks/blob/main/sagemaker/12_batch_transform_inference/sagemaker-notebook.ipynb",
        "https://github.com/huggingface/text-generation-inference",
        "https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/inference.md"
    ]
}
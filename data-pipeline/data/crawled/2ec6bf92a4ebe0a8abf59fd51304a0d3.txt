[Skip to main content](#content)

[![Cornell University](/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)

In just 3 minutes help us improve arXiv:

[Annual Global Survey](https://cornell.ca1.qualtrics.com/jfe/form/SV_6m22mbqW9GQ3pQO)

We gratefully acknowledge support from the Simons Foundation, [member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors. [Donate](https://info.arxiv.org/about/donate.html)

[](/IgnoreMe)

[![arxiv logo](/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](/) > [cs](/list/cs/recent) > arXiv:2408.03314 

[Help](https://info.arxiv.org/help) | [Advanced Search](https://arxiv.org/search/advanced)

All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text

Search

[![arXiv logo](/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)

[ ![Cornell University Logo](/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg) ](https://www.cornell.edu/)

open search

GO

open navigation menu

## quick links

  * [Login](https://arxiv.org/login)
  * [Help Pages](https://info.arxiv.org/help)
  * [About](https://info.arxiv.org/about)



# Computer Science > Machine Learning

**arXiv:2408.03314** (cs) 

[Submitted on 6 Aug 2024]

# Title:Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters

Authors:[Charlie Snell](https://arxiv.org/search/cs?searchtype=author&query=Snell,+C), [Jaehoon Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee,+J), [Kelvin Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu,+K), [Aviral Kumar](https://arxiv.org/search/cs?searchtype=author&query=Kumar,+A)

View a PDF of the paper titled Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters, by Charlie Snell and 3 other authors

[View PDF](/pdf/2408.03314) [HTML (experimental)](https://arxiv.org/html/2408.03314v1)

> Abstract:Enabling LLMs to improve their outputs by using more test-time computation is a critical step towards building generally self-improving agents that can operate on open-ended natural language. In this paper, we study the scaling of inference-time computation in LLMs, with a focus on answering the question: if an LLM is allowed to use a fixed but non-trivial amount of inference-time compute, how much can it improve its performance on a challenging prompt? Answering this question has implications not only on the achievable performance of LLMs, but also on the future of LLM pretraining and how one should tradeoff inference-time and pre-training compute. Despite its importance, little research attempted to understand the scaling behaviors of various test-time inference methods. Moreover, current work largely provides negative results for a number of these strategies. In this work, we analyze two primary mechanisms to scale test-time computation: (1) searching against dense, process-based verifier reward models; and (2) updating the model's distribution over a response adaptively, given the prompt at test time. We find that in both cases, the effectiveness of different approaches to scaling test-time compute critically varies depending on the difficulty of the prompt. This observation motivates applying a "compute-optimal" scaling strategy, which acts to most effectively allocate test-time compute adaptively per prompt. Using this compute-optimal strategy, we can improve the efficiency of test-time compute scaling by more than 4x compared to a best-of-N baseline. Additionally, in a FLOPs-matched evaluation, we find that on problems where a smaller base model attains somewhat non-trivial success rates, test-time compute can be used to outperform a 14x larger model. 

Subjects: |  Machine Learning (cs.LG); Computation and Language (cs.CL)  
---|---  
Cite as: | [arXiv:2408.03314](https://arxiv.org/abs/2408.03314) [cs.LG]  
(or  [arXiv:2408.03314v1](https://arxiv.org/abs/2408.03314v1) [cs.LG] for this version)   
<https://doi.org/10.48550/arXiv.2408.03314> Focus to learn more arXiv-issued DOI via DataCite  
  
## Submission history

From: Charlie Snell [[view email](/show-email/dfa16c4c/2408.03314)] **[v1]** Tue, 6 Aug 2024 17:35:05 UTC (4,152 KB) 

Full-text links:

## Access Paper:

View a PDF of the paper titled Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters, by Charlie Snell and 3 other authors

  * [View PDF](/pdf/2408.03314)
  * [HTML (experimental)](https://arxiv.org/html/2408.03314v1)
  * [TeX Source](/src/2408.03314)
  * [Other Formats](/format/2408.03314)



[ ![license icon](https://arxiv.org/icons/licenses/by-4.0.png) view license ](http://creativecommons.org/licenses/by/4.0/ "Rights to this article")

Current browse context: 

cs.LG

[< prev](/prevnext?id=2408.03314&function=prev&context=cs.LG "previous in cs.LG \(accesskey p\)") |  [next >](/prevnext?id=2408.03314&function=next&context=cs.LG "next in cs.LG \(accesskey n\)")

[new](/list/cs.LG/new) |  [recent](/list/cs.LG/recent) | [2024-08](/list/cs.LG/2024-08)

Change to browse by: 

[cs](/abs/2408.03314?context=cs) [cs.CL](/abs/2408.03314?context=cs.CL)

### References & Citations

  * [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2408.03314)
  * [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2408.03314)
  * [Semantic Scholar](https://api.semanticscholar.org/arXiv:2408.03314)



### [ 1 blog link](/tb/2408.03314)

([what is this?](https://info.arxiv.org/help/trackback.html)) 

[a](/static/browse/0.3.4/css/cite.css) export BibTeX citation Loading...

## BibTeX formatted citation

Ã—

loading...

Data provided by: 

### Bookmark

[ ![BibSonomy logo](/static/browse/0.3.4/images/icons/social/bibsonomy.png) ](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2408.03314&description=Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters "Bookmark on BibSonomy") [ ![Reddit logo](/static/browse/0.3.4/images/icons/social/reddit.png) ](https://reddit.com/submit?url=https://arxiv.org/abs/2408.03314&title=Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters "Bookmark on Reddit")

Bibliographic Tools

# Bibliographic and Citation Tools

Bibliographic Explorer Toggle

Bibliographic Explorer _([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_

Connected Papers Toggle

Connected Papers _([What is Connected Papers?](https://www.connectedpapers.com/about))_

Litmaps Toggle

Litmaps _([What is Litmaps?](https://www.litmaps.co/))_

scite.ai Toggle

scite Smart Citations _([What are Smart Citations?](https://www.scite.ai/))_

Code, Data, Media

# Code, Data and Media Associated with this Article

alphaXiv Toggle

alphaXiv _([What is alphaXiv?](https://alphaxiv.org/))_

Links to Code Toggle

CatalyzeX Code Finder for Papers _([What is CatalyzeX?](https://www.catalyzex.com))_

DagsHub Toggle

DagsHub _([What is DagsHub?](https://dagshub.com/))_

GotitPub Toggle

Gotit.pub _([What is GotitPub?](http://gotit.pub/faq))_

Huggingface Toggle

Hugging Face _([What is Huggingface?](https://huggingface.co/huggingface))_

Links to Code Toggle

Papers with Code _([What is Papers with Code?](https://paperswithcode.com/))_

ScienceCast Toggle

ScienceCast _([What is ScienceCast?](https://sciencecast.org/welcome))_

Demos

# Demos

Replicate Toggle

Replicate _([What is Replicate?](https://replicate.com/docs/arxiv/about))_

Spaces Toggle

Hugging Face Spaces _([What is Spaces?](https://huggingface.co/docs/hub/spaces))_

Spaces Toggle

TXYZ.AI _([What is TXYZ.AI?](https://txyz.ai))_

Related Papers

# Recommenders and Search Tools

Link to Influence Flower

Influence Flower _([What are Influence Flowers?](https://influencemap.cmlab.dev/))_

Core recommender toggle

CORE Recommender _([What is CORE?](https://core.ac.uk/services/recommender))_

IArxiv recommender toggle

IArxiv Recommender _([What is IArxiv?](https://iarxiv.org/about))_

  * Author
  * Venue
  * Institution
  * Topic



About arXivLabs 

# arXivLabs: experimental projects with community collaborators

arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.

Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.

Have an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).

[Which authors of this paper are endorsers?](/auth/show-endorsers/2408.03314) | [Disable MathJax](javascript:setMathjaxCookie\(\)) ([What is MathJax?](https://info.arxiv.org/help/mathjax.html)) 

  * [About](https://info.arxiv.org/about)
  * [Help](https://info.arxiv.org/help)



  * contact arXivClick here to contact arXiv [ Contact](https://info.arxiv.org/help/contact.html)
  * subscribe to arXiv mailingsClick here to subscribe [ Subscribe](https://info.arxiv.org/help/subscribe)



  * [Copyright](https://info.arxiv.org/help/license/index.html)
  * [Privacy Policy](https://info.arxiv.org/help/policies/privacy_policy.html)



  * [Web Accessibility Assistance](https://info.arxiv.org/help/web_accessibility.html)
  * [arXiv Operational Status ](https://status.arxiv.org) Get status notifications via [email](https://subscribe.sorryapp.com/24846f03/email/new) or [slack](https://subscribe.sorryapp.com/24846f03/slack/new)




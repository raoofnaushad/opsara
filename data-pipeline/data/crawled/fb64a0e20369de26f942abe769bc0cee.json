{
    "id": "fb64a0e20369de26f942abe769bc0cee",
    "metadata": {
        "id": "fb64a0e20369de26f942abe769bc0cee",
        "url": "https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder/",
        "title": "Understanding Encoder And Decoder LLMs",
        "properties": {
            "description": "Delve into Transformer architectures: from the original encoder-decoder structure, to BERT & RoBERTa encoder-only models, to the GPT series focused on decoding. Explore their evolution, strengths, & applications in NLP tasks.",
            "keywords": null,
            "author": "Sebastian Raschka, PhD",
            "og:url": "https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder",
            "og:type": "article",
            "og:title": "Understanding Encoder And Decoder LLMs",
            "og:description": "Several people asked me to dive a bit deeper into large language model (LLM) jargon and explain some of the more technical terms we nowadays take for granted. This includes references to \"encoder-style\" and \"decoder-style\" LLMs. What do these terms mean?",
            "og:image": "https://substackcdn.com/image/fetch/w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1655aff8-c9bd-4a93-9e72-f1911359a667_1646x1090.jpeg",
            "twitter:card": "summary_large_image",
            "twitter:title": "Understanding Encoder And Decoder LLMs",
            "twitter:description": "Several people asked me to dive a bit deeper into large language model (LLM) jargon and explain some of the more technical terms we nowadays take for granted. This includes references to \"encoder-style\" and \"decoder-style\" LLMs. What do these terms mean?",
            "twitter:image": "https://substackcdn.com/image/fetch/f_auto,q_auto:best,fl_progressive:steep/https%3A%2F%2Fsebastianraschka.substack.com%2Fapi%2Fv1%2Fpost_preview%2F128842439%2Ftwitter.jpg%3Fversion%3D4"
        }
    },
    "parent_metadata": {
        "id": "ddd7ffca1675e0346c9d2d01603b2cfd",
        "url": "https://www.notion.so/LLMs-ddd7ffca1675e0346c9d2d01603b2cfd",
        "title": "LLMs",
        "properties": {
            "Type": "Leaf"
        }
    },
    "content": "[![](https://substackcdn.com/image/fetch/w_96,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4dcbe6f-2617-404f-8368-9bc428272016_1280x1280.png)](/)\n\n# [![Ahead of AI](https://substackcdn.com/image/fetch/e_trim:10:white/e_trim:10:transparent/h_72,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe553b9ca-3e8c-442b-969a-22556b86d1e5_448x212.png)](/)\n\nSubscribeSign in\n\n#### Share this post\n\n[![](https://substackcdn.com/image/fetch/w_520,h_272,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1655aff8-c9bd-4a93-9e72-f1911359a667_1646x1090.jpeg)![Ahead of AI](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4dcbe6f-2617-404f-8368-9bc428272016_1280x1280.png)Ahead of AIUnderstanding Encoder And Decoder LLMs](https://substack.com/home/post/p-128842439?utm_campaign=post&utm_medium=web)\n\nCopy linkFacebookEmailNotesMore\n\n![](https://substackcdn.com/image/fetch/w_64,h_64,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg)\n\n![Ahead of AI](https://substackcdn.com/image/fetch/w_48,h_48,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4dcbe6f-2617-404f-8368-9bc428272016_1280x1280.png)\n\n#### Discover more from Ahead of AI\n\nAhead AI specializes in Machine Learning & AI research and is read by tens of thousands of researchers and practitioners who want to stay ahead in the ever-evolving field.\n\nOver 79,000 subscribers\n\nSubscribe\n\nContinue reading\n\nSign in\n\n# Understanding Encoder And Decoder LLMs\n\n[![](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg)](https://substack.com/@rasbt)\n\n[Sebastian Raschka, PhD](https://substack.com/@rasbt)\n\nJun 17, 2023\n\n155\n\n#### Share this post\n\n[![](https://substackcdn.com/image/fetch/w_520,h_272,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1655aff8-c9bd-4a93-9e72-f1911359a667_1646x1090.jpeg)![Ahead of AI](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4dcbe6f-2617-404f-8368-9bc428272016_1280x1280.png)Ahead of AIUnderstanding Encoder And Decoder LLMs](https://substack.com/home/post/p-128842439?utm_campaign=post&utm_medium=web)\n\nCopy linkFacebookEmailNotesMore\n\n[5](https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder/comments)10\n\n[Share](javascript:void\\(0\\))\n\nSeveral people asked me to dive a bit deeper into large language model (LLM) jargon and explain some of the more technical terms we nowadays take for granted. This includes references to \"encoder-style\" and \"decoder-style\" LLMs. What do these terms mean? \n\nLet's get to it:  _**What are the differences between encoder- and decoder-based language transformers?**_\n\n## **Encoder- And Decoder-Style Transformers**\n\nFundamentally, both encoder- and decoder-style architectures use the same self-attention layers to encode word tokens. However, the main difference is that encoders are designed to learn embeddings that can be used for various predictive modeling tasks such as classification. In contrast, decoders are designed to generate new texts, for example, answering user queries.\n\n**The original transformer**\n\nThe original transformer architecture (_[Attention Is All You Need, 2017](https://arxiv.org/abs/1706.03762)_), which was developed for English-to-French and English-to-German language translation, utilized both an encoder and a decoder, as illustrated in the figure below.\n\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81c2aa73-dd8c-46bf-85b0-90e01145b0ed_1422x1460.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81c2aa73-dd8c-46bf-85b0-90e01145b0ed_1422x1460.png)Illustration of the original transformer architecture proposed in [Attention Is All You Need, 2017](https://arxiv.org/abs/1706.03762)\n\nIn the figure above, the input text (that is, the sentences of the text that is to be translated) is first tokenized into individual word tokens, which are then encoded via an embedding layer before it enters the encoder part. Then, after adding a positional encoding vector to each embedded word, the embeddings go through a multi-head self-attention layer. The multi-head attention layer is followed by an \"Add & normalize\" step, which performs a layer normalization and adds the original embeddings via a skip connection (also known as a residual or shortcut connection). Finally, after entering a \"fully connected layer,\" which is a small multilayer perceptron consisting of two fully connected layers with a nonlinear activation function in between, the outputs are again added and normalized before they are passed to a multi-head self-attention layer of the decoder part.\n\nThe decoder part in the figure above has a similar overall structure as the encoder part. The key difference is that the inputs and outputs are different. The encoder receives the input text that is to be translated, and the decoder generates the translated text.\n\n**Encoders**\n\nThe encoder part in the original transformer, illustrated in the preceding figure, is responsible for understanding and extracting the relevant information from the input text. It then outputs a continuous representation (embedding) of the input text that is passed to the decoder. Finally, the decoder generates the translated text (target language) based on the continuous representation received from the encoder.\n\nOver the years, various encoder-only architectures have been developed based on the encoder module of the original transformer model outlined above. Notable examples include BERT (_[Pre-training of Deep Bidirectional Transformers for Language Understanding, 2018](https://arxiv.org/abs/1810.04805)_) and RoBERTa (_[A Robustly Optimized BERT Pretraining Approach, 2018](https://arxiv.org/abs/1907.11692)_).\n\nBERT (**B** idirectional **E** ncoder **R** epresentations from **T** ransformers) is an encoder-only architecture based on the Transformer's encoder module. The BERT model is pretrained on a large text corpus using masked language modeling (illustrated in the figure below) and next-sentence prediction tasks.\n\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8fc5971-d59b-4d5a-bb2a-d8ac6d2aa23a_2022x1224.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8fc5971-d59b-4d5a-bb2a-d8ac6d2aa23a_2022x1224.png)Illustration of the masked language modeling pretraining objective used in BERT-style transformers.\n\nThe main idea behind masked language modeling is to mask (or replace) random word tokens in the input sequence and then train the model to predict the original masked tokens based on the surrounding context.\n\nNext to the masked language modeling pretraining task illustrated in the figure above, the next-sentence prediction task asks the model to predict whether the original document's sentence order of two randomly shuffled sentences is correct. For example, two sentences, in random order, are separated by the [SEP] token:\n\n  * [CLS] Toast is a simple yet delicious food [SEP] Itâ€™s often served with butter, jam, or honey.\n\n  * [CLS] Itâ€™s often served with butter, jam, or honey. [SEP] Toast is a simple yet delicious food.\n\n\n\n\nThe [CLS] token is a placeholder token for the model, prompting the model to return a _True_ or _False_ label indicating whether the sentences are in the correct order or not.\n\nThe masked language and next-sentence pretraining objectives (which are a form of self-supervised learning, as discussed in Chapter 2) allow BERT to learn rich contextual representations of the input texts, which can then be finetuned for various downstream tasks like sentiment analysis, question-answering, and named entity recognition.\n\nRoBERTa (**R** obustly **o** ptimized **BERT****a** pproach) is an optimized version of BERT. It maintains the same overall architecture as BERT but employs several training and optimization improvements, such as larger batch sizes, more training data, and eliminating the next-sentence prediction task. These changes resulted in RoBERTa achieving better performance on various natural language understanding tasks than BERT.\n\n**Decoders**\n\nComing back to the original transformer architecture outlined at the beginning of this section, the multi-head self-attention mechanism in the decoder is similar to the one in the encoder, but it is masked to prevent the model from attending to future positions, ensuring that the predictions for position _i_ can depend only on the known outputs at positions less than _i_. As illustrated in the figure below, the decoder generates the output word by word.\n\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3c55ad5-cc86-4421-b0bc-bbd967409ca6_1162x1022.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3c55ad5-cc86-4421-b0bc-bbd967409ca6_1162x1022.png)Illustration of the next-sentence prediction task used in the original transformer.\n\nThis masking (shown explicitly in the figure above, although it happens internally in the decoder's multi-head self-attention mechanism) is essential to maintain the autoregressive property of the transformer model during training and inference. The autoregressive property ensures that the model generates output tokens one at a time and uses previously generated tokens as context for generating the next word token.\n\nOver the years, researchers have built upon the original encoder-decoder transformer architecture and developed several decoder-only models that have proven to be highly effective in various natural language processing tasks. The most notable models include the GPT family.\n\nThe GPT (**G** enerative **P** re-trained **T** ransformer) series are decoder-only models pretrained on large-scale unsupervised text data and finetuned for specific tasks such as text classification, sentiment analysis, question-answering, and summarization. The GPT models, including GPT-2, (_[GPT-3 Language Models are Few-Shot Learners, 2020](https://arxiv.org/abs/2005.14165)_), and the more recent GPT-4, have shown remarkable performance in various benchmarks and are currently the most popular architecture for natural language processing.\n\nOne of the most notable aspects of GPT models is their emergent properties. Emergent properties refer to the abilities and skills that a model develops due to its next-word prediction pretraining. Even though these models were only taught to predict the next word, the pretrained models are capable of text summarization, translation, question answering, classification, and more. Furthermore, these models can perform new tasks without updating the model parameters via in-context learning, which is discussed in more detail in Chapter 18.\n\n**Encoder-decoder hybrids**\n\nNext to the traditional encoder and decoder architectures, there have been advancements in the development of new encoder-decoder models that leverage the strengths of both components. These models often incorporate novel techniques, pre-training objectives, or architectural modifications to enhance their performance in various natural language processing tasks. Some notable examples of these new encoder-decoder models include \n\n  * BART (_[Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension, 2019](https://arxiv.org/abs/1910.13461)_) \n\n  * and T5 (_[Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer, 2019](https://arxiv.org/abs/1910.10683)_).\n\n\n\n\nEncoder-decoder models are typically used for natural language processing tasks that involve understanding input sequences and generating output sequences, often with different lengths and structures. They are particularly good at tasks where there is a complex mapping between the input and output sequences and where it is crucial to capture the relationships between the elements in both sequences. Some common use cases for encoder-decoder models include text translation and summarization.\n\n**Terminology and jargon**\n\nAll of these methods, encoder-only, decoder-only, and encoder-decoder models, are sequence-to-sequence models (often abbreviated as _seq2seq_). Note that while we refer to BERT-style methods as encoder-only, the description _encoder-only_ may be misleading since these methods also _decode_ the embeddings into output tokens or text during pretraining.\n\nIn other words, both encoder-only and decoder-only architectures are \"decoding.\" However, the encoder-only architectures, in contrast to decoder-only and encoder-decoder architectures, are not decoding in an autoregressive fashion. Autoregressive decoding refers to generating output sequences one token at a time, conditioning each token on the previously generated tokens. Encoder-only models do not generate coherent output sequences in this manner. Instead, they focus on understanding the input text and producing task-specific outputs, such as labels or token predictions.\n\n**Conclusion**\n\nIn brief, encoder-style models are popular for learning embeddings used in classification tasks, encoder-decoder-style models are used in generative tasks where the output heavily relies on the input (for example, translation and summarization), and decoder-only models are used for other types of generative tasks including Q&A. Since the first transformer architecture emerged, hundreds of encoder-only, decoder-only, and encoder-decoder hybrids have been developed, as summarized in the figure below.\n\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcce3c437-4b9c-4d15-947d-7c177c9518e5_4258x5745.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcce3c437-4b9c-4d15-947d-7c177c9518e5_4258x5745.png)An overview of _**some**_ of the most popular large language transformers organized by architecture type and developers.\n\nWhile encoder-only models gradually lost in popularity, decoder-only models like GPT exploded in popularity thanks to breakthrough in text generation via GPT-3, ChatGPT, and GPT-4. However, encoder-only models are still very useful for training predictive models based on text embeddings versus generating texts.\n\n_This magazine is a personal passion project that does not offer direct compensation. However, for those who wish to support me, please consider purchasing a copy of[one of my books](https://sebastianraschka.com/books). If you find them insightful and beneficial, please feel free to recommend them to your friends and colleagues._\n\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27b6d27d-1052-4a4f-a419-a69cdd36704a_1118x454.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27b6d27d-1052-4a4f-a419-a69cdd36704a_1118x454.png)[Machine Learning with PyTorch and Scikit-Learn](https://www.amazon.com/Machine-Learning-PyTorch-Scikit-Learn-scikit-learn-ebook-dp-B09NW48MR1/dp/B09NW48MR1/), [Machine Learning Q and AI](https://nostarch.com/machine-learning-and-ai-beyond-basics), and [Build a Large Language Model (from Scratch)](http://mng.bz/M96o)\n\n**Your support means a great deal! Thank you!**\n\n### Subscribe to Ahead of AI\n\nBy Sebastian Raschka Â· Launched 2 years ago\n\nAhead AI specializes in Machine Learning & AI research and is read by tens of thousands of researchers and practitioners who want to stay ahead in the ever-evolving field.\n\nSubscribe\n\n[![](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F64769218-5ab1-4442-b194-623564737424_1080x1080.jpeg)](https://substack.com/profile/96251662-kak)\n\n[![](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F55c2183f-7755-4692-8063-59df2751f834_800x800.png)](https://substack.com/profile/85853406-sairam-sundaresan)\n\n[![](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fc59205ed-f7b0-4c31-8810-1097c56a778f_1200x1600.jpeg)](https://substack.com/profile/1134131-sid-kapur)\n\n[![](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc95711cf-0604-4af6-a3e0-47be3b58a2dd_1364x1364.jpeg)](https://substack.com/profile/147444258-lukasz-ostrowski)\n\n[![](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fa635f57b-3633-43d2-bc80-950afcaa42cb_144x144.png)](https://substack.com/profile/108287441-gabriel)\n\n155 Likesâˆ™\n\n[10 Restacks](https://substack.com/note/p-128842439/restacks?utm_source=substack&utm_content=facepile-restacks)\n\n155\n\n#### Share this post\n\n[![](https://substackcdn.com/image/fetch/w_520,h_272,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1655aff8-c9bd-4a93-9e72-f1911359a667_1646x1090.jpeg)![Ahead of AI](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4dcbe6f-2617-404f-8368-9bc428272016_1280x1280.png)Ahead of AIUnderstanding Encoder And Decoder LLMs](https://substack.com/home/post/p-128842439?utm_campaign=post&utm_medium=web)\n\nCopy linkFacebookEmailNotesMore\n\n[5](https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder/comments)10\n\n[Share](javascript:void\\(0\\))\n\n#### Discussion about this post\n\nCommentsRestacks\n\n![](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Favatars%2Fdefault-light.png)\n\n[![](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F4daf1168-7d63-41f1-a70a-972a9c563d33_144x144.png)](https://substack.com/profile/11904497-richard-hackathorn?utm_source=comment)\n\n[Richard Hackathorn](https://substack.com/profile/11904497-richard-hackathorn?utm_source=substack-feed-item)\n\n[31 Dec 2023](https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder/comment/46346150 \"31 Dec 2023, 21:06\")Edited\n\nLiked by Sebastian Raschka, PhD\n\nThis LLM encoder/decoder stuff messes with my mind! There is something fundamental here that I'm not getting. HELP... ðŸ¤” I have been fascinated with autoencoders, which take an example from feature space and ENCODE it into point in latent space and then DECODE it back into a reconstructed example in feature space, thus allowing a reconstruction loss to be calculated. [ref: Python ML 3Ed, Chap 17] \n\n1) Should LLM decoders be called 'generators' like in GANs? \n\n2) That single line that connects LLM encoder to its decoder... Is that the same data that one receives as an embedding from the LLM API? \n\n3) For a decoder-only LLM, is its input always an embedding vector? Or, where do the model weights come from?\n\n4) Is it possible to take an LLM embedding, reconstruct its initial input, and calculate the reconstruction loss? If true, this would enable us to map the fine (manifold) structures in these mysterious LLM latent spaces. Loved your old examples of putting/removing smiles on celebrity faces. Like to find a few hallucinations lurking in LLM latent spaces! ðŸ˜®\n\nExpand full comment\n\n[Like (2)](javascript:void\\(0\\))ReplyShare\n\n[1 reply by Sebastian Raschka, PhD](https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder/comment/46346150)\n\n[![](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce196789-7b5a-436a-bd65-1078805f1ee6_420x420.png)](https://substack.com/profile/185804816-viswa-kumar?utm_source=comment)\n\n[Viswa Kumar](https://substack.com/profile/185804816-viswa-kumar?utm_source=substack-feed-item)\n\n[7 Nov](https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder/comment/76035897 \"7 Nov 2024, 19:48\")\n\nLiked by Sebastian Raschka, PhD\n\nI agree the term encoder / decoder is overloaded since almost all architectures would essentially perform the encoding / decoding as a function. Engineers are not good at naming not only vars after all ðŸ¤£\n\nExpand full comment\n\n[Like (1)](javascript:void\\(0\\))ReplyShare\n\n[3 more comments...](https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder/comments)\n\nTopLatestDiscussions\n\n[Understanding Large Language Models](https://magazine.sebastianraschka.com/p/understanding-large-language-models)\n\n[A Cross-Section of the Most Relevant Literature To Get Up to Speed](https://magazine.sebastianraschka.com/p/understanding-large-language-models)\n\nApr 16, 2023 â€¢\n\n[Sebastian Raschka, PhD](https://substack.com/@rasbt)\n\n867\n\n#### Share this post\n\n[![](https://substackcdn.com/image/fetch/w_520,h_272,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9a0766d-2e52-4af0-96c5-3e07a30d6ecb_1868x1130.png)![Ahead of AI](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4dcbe6f-2617-404f-8368-9bc428272016_1280x1280.png)Ahead of AIUnderstanding Large Language Models](https://substack.com/home/post/p-115060492?utm_campaign=post&utm_medium=web)\n\nCopy linkFacebookEmailNotesMore\n\n[54](https://magazine.sebastianraschka.com/p/understanding-large-language-models/comments)[](javascript:void\\(0\\))\n\n![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9a0766d-2e52-4af0-96c5-3e07a30d6ecb_1868x1130.png)\n\n[Understanding and Coding Self-Attention, Multi-Head Attention, Cross-Attention, and Causal-Attention in LLMs](https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention)\n\n[This article will teach you about self-attention mechanisms used in transformer architectures and large language models (LLMs) such as GPT-4 and Llama.](https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention)\n\nJan 14, 2024\n\n326\n\n#### Share this post\n\n[![](https://substackcdn.com/image/fetch/w_520,h_272,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69bfee26-ea3b-42a6-8a1a-6b8187852082_738x564.png)![Ahead of AI](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4dcbe6f-2617-404f-8368-9bc428272016_1280x1280.png)Ahead of AIUnderstanding and Coding Self-Attention, Multi-Head Attention, Cross-Attention, and Causal-Attention in LLMs](https://substack.com/home/post/p-140464659?utm_campaign=post&utm_medium=web)\n\nCopy linkFacebookEmailNotesMore\n\n[41](https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention/comments)[](javascript:void\\(0\\))\n\n![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69bfee26-ea3b-42a6-8a1a-6b8187852082_738x564.png)\n\n[Building LLMs from the Ground Up: A 3-hour Coding Workshop](https://magazine.sebastianraschka.com/p/building-llms-from-the-ground-up)\n\n[If your weekend plans include catching up on AI developments and understanding Large Language Models (LLMs), I've prepared a 1-hour presentation on theâ€¦](https://magazine.sebastianraschka.com/p/building-llms-from-the-ground-up)\n\nAug 31, 2024 â€¢\n\n[Sebastian Raschka, PhD](https://substack.com/@rasbt)\n\n385\n\n#### Share this post\n\n[![](https://substackcdn.com/image/fetch/w_520,h_272,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F367b547c-9d22-4a3d-b466-1d56ccc6b055_1844x1224.png)![Ahead of AI](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4dcbe6f-2617-404f-8368-9bc428272016_1280x1280.png)Ahead of AIBuilding LLMs from the Ground Up: A 3-hour Coding Workshop](https://substack.com/home/post/p-148329414?utm_campaign=post&utm_medium=web)\n\nCopy linkFacebookEmailNotesMore\n\n[13](https://magazine.sebastianraschka.com/p/building-llms-from-the-ground-up/comments)[](javascript:void\\(0\\))\n\n![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F367b547c-9d22-4a3d-b466-1d56ccc6b055_1844x1224.png)\n\nSee all\n\nReady for more?\n\nSubscribe\n\nÂ© 2025 Sebastian Raschka\n\n[Privacy](https://substack.com/privacy) âˆ™ [Terms](https://substack.com/tos) âˆ™ [Collection notice](https://substack.com/ccpa#personal-data-collected)\n\n[ Start Writing](https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)\n\n[Substack](https://substack.com) is the home for great culture\n\n#### Share\n\n[](https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder?utm_campaign=unknown&utm_medium=web)\n\nCopy linkFacebookEmailNotesMore\n\n## Create your profile\n\nName (Required)HandleBioEmail (Required)\n\nSubscribe to the newsletter\n\nundefined subscriptions will be displayed on your profile (edit)\n\nSkip for now\n\nSave & Post Comment\n\n## Only paid subscribers can comment on this post\n\n[Subscribe](https://magazine.sebastianraschka.com/subscribe?simple=true&next=https%3A%2F%2Fmagazine.sebastianraschka.com%2Fp%2Funderstanding-encoder-and-decoder&utm_source=paywall&utm_medium=web&utm_content=128842439)\n\n[Already a paid subscriber? **Sign in**](https://substack.com/sign-in?redirect=%2Fp%2Funderstanding-encoder-and-decoder&for_pub=sebastianraschka&change_user=false)\n\n#### Check your email\n\nFor your security, we need to re-authenticate you.\n\nClick the link we sent to , or [click here to sign in](https://substack.com/sign-in?redirect=%2Fp%2Funderstanding-encoder-and-decoder&for_pub=sebastianraschka&with_password=true).\n",
    "content_quality_score": 1.0,
    "summary": null,
    "child_urls": [
        "https://magazine.sebastianraschka.com/",
        "https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder/comments",
        "https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder/comment/46346150",
        "https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder/comment/76035897",
        "https://magazine.sebastianraschka.com/p/understanding-large-language-models",
        "https://magazine.sebastianraschka.com/p/understanding-large-language-models/comments",
        "https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention",
        "https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention/comments",
        "https://magazine.sebastianraschka.com/p/building-llms-from-the-ground-up",
        "https://magazine.sebastianraschka.com/p/building-llms-from-the-ground-up/comments",
        "https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder?utm_campaign=unknown&utm_medium=web",
        "https://magazine.sebastianraschka.com/subscribe?simple=true&next=https%3A%2F%2Fmagazine.sebastianraschka.com%2Fp%2Funderstanding-encoder-and-decoder&utm_source=paywall&utm_medium=web&utm_content=128842439",
        "https://substack.com/home/post/p-128842439?utm_campaign=post&utm_medium=web",
        "https://substack.com/@rasbt",
        "javascript:void(0)",
        "https://arxiv.org/abs/1706.03762",
        "https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81c2aa73-dd8c-46bf-85b0-90e01145b0ed_1422x1460.png",
        "https://arxiv.org/abs/1810.04805",
        "https://arxiv.org/abs/1907.11692",
        "https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8fc5971-d59b-4d5a-bb2a-d8ac6d2aa23a_2022x1224.png",
        "https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3c55ad5-cc86-4421-b0bc-bbd967409ca6_1162x1022.png",
        "https://arxiv.org/abs/2005.14165",
        "https://arxiv.org/abs/1910.13461",
        "https://arxiv.org/abs/1910.10683",
        "https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcce3c437-4b9c-4d15-947d-7c177c9518e5_4258x5745.png",
        "https://sebastianraschka.com/books",
        "https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27b6d27d-1052-4a4f-a419-a69cdd36704a_1118x454.png",
        "https://www.amazon.com/Machine-Learning-PyTorch-Scikit-Learn-scikit-learn-ebook-dp-B09NW48MR1/dp/B09NW48MR1/",
        "https://nostarch.com/machine-learning-and-ai-beyond-basics",
        "http://mng.bz/M96o",
        "https://substack.com/profile/96251662-kak",
        "https://substack.com/profile/85853406-sairam-sundaresan",
        "https://substack.com/profile/1134131-sid-kapur",
        "https://substack.com/profile/147444258-lukasz-ostrowski",
        "https://substack.com/profile/108287441-gabriel",
        "https://substack.com/note/p-128842439/restacks?utm_source=substack&utm_content=facepile-restacks",
        "https://substack.com/profile/11904497-richard-hackathorn?utm_source=comment",
        "https://substack.com/profile/11904497-richard-hackathorn?utm_source=substack-feed-item",
        "https://substack.com/profile/185804816-viswa-kumar?utm_source=comment",
        "https://substack.com/profile/185804816-viswa-kumar?utm_source=substack-feed-item",
        "https://substack.com/home/post/p-115060492?utm_campaign=post&utm_medium=web",
        "https://substack.com/home/post/p-140464659?utm_campaign=post&utm_medium=web",
        "https://substack.com/home/post/p-148329414?utm_campaign=post&utm_medium=web",
        "https://substack.com/privacy",
        "https://substack.com/tos",
        "https://substack.com/ccpa#personal-data-collected",
        "https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer",
        "https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button",
        "https://substack.com",
        "https://substack.com/sign-in?redirect=%2Fp%2Funderstanding-encoder-and-decoder&for_pub=sebastianraschka&change_user=false",
        "https://substack.com/sign-in?redirect=%2Fp%2Funderstanding-encoder-and-decoder&for_pub=sebastianraschka&with_password=true"
    ]
}
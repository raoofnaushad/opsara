[Skip to main content](#__docusaurus_skipToContent_fallback)

[![ü¶úÔ∏èüîó LangChain](/img/brand/wordmark.png)![ü¶úÔ∏èüîó LangChain](/img/brand/wordmark-dark.png)](/)[Integrations](/docs/integrations/providers/)[API Reference](https://python.langchain.com/api_reference/)

[More](#)

  * [Contributing](/docs/contributing/)
  * [People](/docs/people/)
  * [Error reference](/docs/troubleshooting/errors/)
  * [LangSmith](https://docs.smith.langchain.com)
  * [LangGraph](https://langchain-ai.github.io/langgraph/)
  * [LangChain Hub](https://smith.langchain.com/hub)
  * [LangChain JS/TS](https://js.langchain.com)



[v0.3](#)

  * [v0.3](/docs/introduction/)
  * [v0.2](https://python.langchain.com/v0.2/docs/introduction)
  * [v0.1](https://python.langchain.com/v0.1/docs/get_started/introduction)



[üí¨](https://chat.langchain.com)[](https://github.com/langchain-ai/langchain)

Search

  * [Introduction](/docs/introduction/)
  * [Tutorials](/docs/tutorials/)

    * [Build a Question Answering application over a Graph Database](/docs/tutorials/graph/)
    * [Tutorials](/docs/tutorials/)
    * [Build a simple LLM application with chat models and prompt templates](/docs/tutorials/llm_chain/)
    * [Build a Chatbot](/docs/tutorials/chatbot/)
    * [Build a Retrieval Augmented Generation (RAG) App: Part 2](/docs/tutorials/qa_chat_history/)
    * [Build an Extraction Chain](/docs/tutorials/extraction/)
    * [Build an Agent](/docs/tutorials/agents/)
    * [Tagging](/docs/tutorials/classification/)
    * [Build a Retrieval Augmented Generation (RAG) App: Part 1](/docs/tutorials/rag/)
    * [Build a semantic search engine](/docs/tutorials/retrievers/)
    * [Build a Question/Answering system over SQL data](/docs/tutorials/sql_qa/)
    * [Summarize Text](/docs/tutorials/summarization/)
  * [How-to guides](/docs/how_to/)

    * [How-to guides](/docs/how_to/)
    * [How to use tools in a chain](/docs/how_to/tools_chain/)
    * [How to use a vectorstore as a retriever](/docs/how_to/vectorstore_retriever/)
    * [How to add memory to chatbots](/docs/how_to/chatbots_memory/)
    * [How to use example selectors](/docs/how_to/example_selectors/)
    * [How to add a semantic layer over graph database](/docs/how_to/graph_semantic/)
    * [How to invoke runnables in parallel](/docs/how_to/parallel/)
    * [How to stream chat model responses](/docs/how_to/chat_streaming/)
    * [How to add default invocation args to a Runnable](/docs/how_to/binding/)
    * [How to add retrieval to chatbots](/docs/how_to/chatbots_retrieval/)
    * [How to use few shot examples in chat models](/docs/how_to/few_shot_examples_chat/)
    * [How to do tool/function calling](/docs/how_to/function_calling/)
    * [How to install LangChain packages](/docs/how_to/installation/)
    * [How to add examples to the prompt for query analysis](/docs/how_to/query_few_shot/)
    * [How to use few shot examples](/docs/how_to/few_shot_examples/)
    * [How to run custom functions](/docs/how_to/functions/)
    * [How to use output parsers to parse an LLM response into structured format](/docs/how_to/output_parser_structured/)
    * [How to handle cases where no queries are generated](/docs/how_to/query_no_queries/)
    * [How to route between sub-chains](/docs/how_to/routing/)
    * [How to return structured data from a model](/docs/how_to/structured_output/)
    * [How to summarize text through parallelization](/docs/how_to/summarize_map_reduce/)
    * [How to summarize text through iterative refinement](/docs/how_to/summarize_refine/)
    * [How to summarize text in a single LLM call](/docs/how_to/summarize_stuff/)
    * [How to use toolkits](/docs/how_to/toolkits/)
    * [How to add ad-hoc tool calling capability to LLMs and Chat Models](/docs/how_to/tools_prompting/)
    * [Build an Agent with AgentExecutor (Legacy)](/docs/how_to/agent_executor/)
    * [How to construct knowledge graphs](/docs/how_to/graph_constructing/)
    * [How to partially format prompt templates](/docs/how_to/prompts_partial/)
    * [How to handle multiple queries when doing query analysis](/docs/how_to/query_multiple_queries/)
    * [How to use built-in tools and toolkits](/docs/how_to/tools_builtin/)
    * [How to pass through arguments from one step to the next](/docs/how_to/passthrough/)
    * [How to compose prompts together](/docs/how_to/prompts_composition/)
    * [How to handle multiple retrievers when doing query analysis](/docs/how_to/query_multiple_retrievers/)
    * [How to add values to a chain's state](/docs/how_to/assign/)
    * [How to construct filters for query analysis](/docs/how_to/query_constructing_filters/)
    * [How to configure runtime chain internals](/docs/how_to/configure/)
    * [How deal with high cardinality categoricals when doing query analysis](/docs/how_to/query_high_cardinality/)
    * [Custom Document Loader](/docs/how_to/document_loader_custom/)
    * [How to use the MultiQueryRetriever](/docs/how_to/MultiQueryRetriever/)
    * [How to add scores to retriever results](/docs/how_to/add_scores_retriever/)
    * [Caching](/docs/how_to/caching_embeddings/)
    * [How to use callbacks in async environments](/docs/how_to/callbacks_async/)
    * [How to attach callbacks to a runnable](/docs/how_to/callbacks_attach/)
    * [How to propagate callbacks constructor](/docs/how_to/callbacks_constructor/)
    * [How to dispatch custom callback events](/docs/how_to/callbacks_custom_events/)
    * [How to pass callbacks in at runtime](/docs/how_to/callbacks_runtime/)
    * [How to split by character](/docs/how_to/character_text_splitter/)
    * [How to cache chat model responses](/docs/how_to/chat_model_caching/)
    * [How to handle rate limits](/docs/how_to/chat_model_rate_limiting/)
    * [How to init any model in one line](/docs/how_to/chat_models_universal_init/)
    * [How to track token usage in ChatModels](/docs/how_to/chat_token_usage_tracking/)
    * [How to add tools to chatbots](/docs/how_to/chatbots_tools/)
    * [How to split code](/docs/how_to/code_splitter/)
    * [How to do retrieval with contextual compression](/docs/how_to/contextual_compression/)
    * [How to convert Runnables to Tools](/docs/how_to/convert_runnable_to_tool/)
    * [How to create custom callback handlers](/docs/how_to/custom_callbacks/)
    * [How to create a custom chat model class](/docs/how_to/custom_chat_model/)
    * [Custom Embeddings](/docs/how_to/custom_embeddings/)
    * [How to create a custom LLM class](/docs/how_to/custom_llm/)
    * [Custom Retriever](/docs/how_to/custom_retriever/)
    * [How to create tools](/docs/how_to/custom_tools/)
    * [How to debug your LLM apps](/docs/how_to/debugging/)
    * [How to load CSVs](/docs/how_to/document_loader_csv/)
    * [How to load documents from a directory](/docs/how_to/document_loader_directory/)
    * [How to load HTML](/docs/how_to/document_loader_html/)
    * [How to load JSON](/docs/how_to/document_loader_json/)
    * [How to load Markdown](/docs/how_to/document_loader_markdown/)
    * [How to load Microsoft Office files](/docs/how_to/document_loader_office_file/)
    * [How to load PDFs](/docs/how_to/document_loader_pdf/)
    * [How to load web pages](/docs/how_to/document_loader_web/)
    * [How to create a dynamic (self-constructing) chain](/docs/how_to/dynamic_chain/)
    * [Text embedding models](/docs/how_to/embed_text/)
    * [How to combine results from multiple retrievers](/docs/how_to/ensemble_retriever/)
    * [How to select examples from a LangSmith dataset](/docs/how_to/example_selectors_langsmith/)
    * [How to select examples by length](/docs/how_to/example_selectors_length_based/)
    * [How to select examples by maximal marginal relevance (MMR)](/docs/how_to/example_selectors_mmr/)
    * [How to select examples by n-gram overlap](/docs/how_to/example_selectors_ngram/)
    * [How to select examples by similarity](/docs/how_to/example_selectors_similarity/)
    * [How to use reference examples when doing extraction](/docs/how_to/extraction_examples/)
    * [How to handle long text when doing extraction](/docs/how_to/extraction_long_text/)
    * [How to use prompting alone (no tool calling) to do extraction](/docs/how_to/extraction_parse/)
    * [How to add fallbacks to a runnable](/docs/how_to/fallbacks/)
    * [How to filter messages](/docs/how_to/filter_messages/)
    * [Hybrid Search](/docs/how_to/hybrid/)
    * [How to use the LangChain indexing API](/docs/how_to/indexing/)
    * [How to inspect runnables](/docs/how_to/inspect/)
    * [LangChain Expression Language Cheatsheet](/docs/how_to/lcel_cheatsheet/)
    * [How to cache LLM responses](/docs/how_to/llm_caching/)
    * [How to track token usage for LLMs](/docs/how_to/llm_token_usage_tracking/)
    * [Run models locally](/docs/how_to/local_llms/)
    * [How to get log probabilities](/docs/how_to/logprobs/)
    * [How to reorder retrieved results to mitigate the "lost in the middle" effect](/docs/how_to/long_context_reorder/)
    * [How to split Markdown by Headers](/docs/how_to/markdown_header_metadata_splitter/)
    * [How to merge consecutive messages of the same type](/docs/how_to/merge_message_runs/)
    * [How to add message history](/docs/how_to/message_history/)
    * [How to migrate from legacy LangChain agents to LangGraph](/docs/how_to/migrate_agent/)
    * [How to retrieve using multiple vectors per document](/docs/how_to/multi_vector/)
    * [How to pass multimodal data directly to models](/docs/how_to/multimodal_inputs/)
    * [How to use multimodal prompts](/docs/how_to/multimodal_prompts/)
    * [How to create a custom Output Parser](/docs/how_to/output_parser_custom/)
    * [How to use the output-fixing parser](/docs/how_to/output_parser_fixing/)
    * [How to parse JSON output](/docs/how_to/output_parser_json/)
    * [How to retry when a parsing error occurs](/docs/how_to/output_parser_retry/)
    * [How to parse text from message objects](/docs/how_to/output_parser_string/)
    * [How to parse XML output](/docs/how_to/output_parser_xml/)
    * [How to parse YAML output](/docs/how_to/output_parser_yaml/)
    * [How to use the Parent Document Retriever](/docs/how_to/parent_document_retriever/)
    * [How to use LangChain with different Pydantic versions](/docs/how_to/pydantic_compatibility/)
    * [How to add chat history](/docs/how_to/qa_chat_history_how_to/)
    * [How to get a RAG application to add citations](/docs/how_to/qa_citations/)
    * [How to do per-user retrieval](/docs/how_to/qa_per_user/)
    * [How to get your RAG application to return sources](/docs/how_to/qa_sources/)
    * [How to stream results from your RAG application](/docs/how_to/qa_streaming/)
    * [How to split JSON data](/docs/how_to/recursive_json_splitter/)
    * [How to recursively split text by characters](/docs/how_to/recursive_text_splitter/)
    * [Response metadata](/docs/how_to/response_metadata/)
    * [How to pass runtime secrets to runnables](/docs/how_to/runnable_runtime_secrets/)
    * [How to do "self-querying" retrieval](/docs/how_to/self_query/)
    * [How to split text based on semantic similarity](/docs/how_to/semantic-chunker/)
    * [How to chain runnables](/docs/how_to/sequence/)
    * [How to save and load LangChain objects](/docs/how_to/serialization/)
    * [How to split text by tokens](/docs/how_to/split_by_token/)
    * [How to split HTML](/docs/how_to/split_html/)
    * [How to do question answering over CSVs](/docs/how_to/sql_csv/)
    * [How to deal with large databases when doing SQL question-answering](/docs/how_to/sql_large_db/)
    * [How to better prompt when doing SQL question-answering](/docs/how_to/sql_prompting/)
    * [How to do query validation as part of SQL question-answering](/docs/how_to/sql_query_checking/)
    * [How to stream runnables](/docs/how_to/streaming/)
    * [How to stream responses from an LLM](/docs/how_to/streaming_llm/)
    * [How to use a time-weighted vector store retriever](/docs/how_to/time_weighted_vectorstore/)
    * [How to return artifacts from a tool](/docs/how_to/tool_artifacts/)
    * [How to use chat models to call tools](/docs/how_to/tool_calling/)
    * [How to disable parallel tool calling](/docs/how_to/tool_calling_parallel/)
    * [How to force models to call a tool](/docs/how_to/tool_choice/)
    * [How to access the RunnableConfig from a tool](/docs/how_to/tool_configure/)
    * [How to pass tool outputs to chat models](/docs/how_to/tool_results_pass_to_model/)
    * [How to pass run time values to tools](/docs/how_to/tool_runtime/)
    * [How to stream events from a tool](/docs/how_to/tool_stream_events/)
    * [How to stream tool calls](/docs/how_to/tool_streaming/)
    * [How to convert tools to OpenAI Functions](/docs/how_to/tools_as_openai_functions/)
    * [How to handle tool errors](/docs/how_to/tools_error/)
    * [How to use few-shot prompting with tool calling](/docs/how_to/tools_few_shot/)
    * [How to add a human-in-the-loop for tools](/docs/how_to/tools_human/)
    * [How to bind model-specific tools](/docs/how_to/tools_model_specific/)
    * [How to trim messages](/docs/how_to/trim_messages/)
    * [How to create and query vector stores](/docs/how_to/vectorstores/)
  * [Conceptual guide](/docs/concepts/)

    * [Agents](/docs/concepts/agents/)
    * [Architecture](/docs/concepts/architecture/)
    * [Async programming with langchain](/docs/concepts/async/)
    * [Callbacks](/docs/concepts/callbacks/)
    * [Chat history](/docs/concepts/chat_history/)
    * [Chat models](/docs/concepts/chat_models/)
    * [Document loaders](/docs/concepts/document_loaders/)
    * [Embedding models](/docs/concepts/embedding_models/)
    * [Evaluation](/docs/concepts/evaluation/)
    * [Example selectors](/docs/concepts/example_selectors/)
    * [Few-shot prompting](/docs/concepts/few_shot_prompting/)
    * [Conceptual guide](/docs/concepts/)
    * [Key-value stores](/docs/concepts/key_value_stores/)
    * [LangChain Expression Language (LCEL)](/docs/concepts/lcel/)
    * [Messages](/docs/concepts/messages/)
    * [Multimodality](/docs/concepts/multimodality/)
    * [Output parsers](/docs/concepts/output_parsers/)
    * [Prompt Templates](/docs/concepts/prompt_templates/)
    * [Retrieval augmented generation (RAG)](/docs/concepts/rag/)
    * [Retrieval](/docs/concepts/retrieval/)
    * [Retrievers](/docs/concepts/retrievers/)
    * [Runnable interface](/docs/concepts/runnables/)
    * [Streaming](/docs/concepts/streaming/)
    * [Structured outputs](/docs/concepts/structured_outputs/)
    * [Testing](/docs/concepts/testing/)
    * [String-in, string-out llms](/docs/concepts/text_llms/)
    * [Text splitters](/docs/concepts/text_splitters/)
    * [Tokens](/docs/concepts/tokens/)
    * [Tool calling](/docs/concepts/tool_calling/)
    * [Tools](/docs/concepts/tools/)
    * [Tracing](/docs/concepts/tracing/)
    * [Vector stores](/docs/concepts/vectorstores/)
    * [Why LangChain?](/docs/concepts/why_langchain/)
  * Ecosystem

    * [ü¶úüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/)
    * [ü¶úüï∏Ô∏è LangGraph](https://langchain-ai.github.io/langgraph/)
  * Versions

    * [v0.3](/docs/versions/v0_3/)
    * [v0.2](/docs/versions/v0_2/overview/)

    * [Pydantic compatibility](/docs/how_to/pydantic_compatibility/)
    * [Migrating from v0.0 chains](/docs/versions/migrating_chains/)

      * [How to migrate from v0.0 chains](/docs/versions/migrating_chains/)
      * [Migrating from ConstitutionalChain](/docs/versions/migrating_chains/constitutional_chain/)
      * [Migrating from ConversationalChain](/docs/versions/migrating_chains/conversation_chain/)
      * [Migrating from ConversationalRetrievalChain](/docs/versions/migrating_chains/conversation_retrieval_chain/)
      * [Migrating from LLMChain](/docs/versions/migrating_chains/llm_chain/)
      * [Migrating from LLMMathChain](/docs/versions/migrating_chains/llm_math_chain/)
      * [Migrating from LLMRouterChain](/docs/versions/migrating_chains/llm_router_chain/)
      * [Migrating from MapReduceDocumentsChain](/docs/versions/migrating_chains/map_reduce_chain/)
      * [Migrating from MapRerankDocumentsChain](/docs/versions/migrating_chains/map_rerank_docs_chain/)
      * [Migrating from MultiPromptChain](/docs/versions/migrating_chains/multi_prompt_chain/)
      * [Migrating from RefineDocumentsChain](/docs/versions/migrating_chains/refine_docs_chain/)
      * [Migrating from RetrievalQA](/docs/versions/migrating_chains/retrieval_qa/)
      * [Migrating from StuffDocumentsChain](/docs/versions/migrating_chains/stuff_docs_chain/)
    * [Upgrading to LangGraph memory](/docs/versions/migrating_memory/)

      * [How to migrate to LangGraph memory](/docs/versions/migrating_memory/)
      * [How to use BaseChatMessageHistory with LangGraph](/docs/versions/migrating_memory/chat_history/)
      * [Migrating off ConversationBufferMemory or ConversationStringBufferMemory](/docs/versions/migrating_memory/conversation_buffer_memory/)
      * [Migrating off ConversationBufferWindowMemory or ConversationTokenBufferMemory](/docs/versions/migrating_memory/conversation_buffer_window_memory/)
      * [Migrating off ConversationSummaryMemory or ConversationSummaryBufferMemory](/docs/versions/migrating_memory/conversation_summary_memory/)
      * [A Long-Term Memory Agent](/docs/versions/migrating_memory/long_term_memory_agent/)
    * [Release policy](/docs/versions/release_policy/)
  * [Security Policy](/docs/security/)



  * [](/)
  * [How-to guides](/docs/how_to/)
  * How to load PDFs



On this page

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/document_loader_pdf.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/document_loader_pdf.ipynb)

# How to load PDFs

[Portable Document Format (PDF)](https://en.wikipedia.org/wiki/PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.

This guide covers how to [load](/docs/concepts/document_loaders/) `PDF` documents into the LangChain [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) format that we use downstream.

Text in PDFs is typically represented via text boxes. They may also contain images. A PDF parser might do some combination of the following:

  * Agglomerate text boxes into lines, paragraphs, and other structures via heuristics or ML inference;
  * Run [OCR](https://en.wikipedia.org/wiki/Optical_character_recognition) on images to detect text therein;
  * Classify text as belonging to paragraphs, lists, tables, or other structures;
  * Structure text into table rows and columns, or key-value pairs.



LangChain integrates with a host of PDF parsers. Some are simple and relatively low-level; others will support OCR and image-processing, or perform advanced document layout analysis. The right choice will depend on your needs. Below we enumerate the possibilities.

We will demonstrate these approaches on a [sample file](https://github.com/langchain-ai/langchain/blob/master/libs/community/tests/integration_tests/examples/layout-parser-paper.pdf):

```
`file_path =("../../docs/integrations/document_loaders/example_data/layout-parser-paper.pdf")`
```

A note on multimodal models

Many modern LLMs support inference over multimodal inputs (e.g., images). In some applications -- such as question-answering over PDFs with complex layouts, diagrams, or scans -- it may be advantageous to skip the PDF parsing, instead casting a PDF page to an image and passing it to a model directly. We demonstrate an example of this in the [Use of multimodal models](/docs/how_to/document_loader_pdf/#use-of-multimodal-models) section below.

## Simple and fast text extraction[‚Äã](#simple-and-fast-text-extraction "Direct link to Simple and fast text extraction")

If you are looking for a simple string representation of text that is embedded in a PDF, the method below is appropriate. It will return a list of [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) objects-- one per page-- containing a single string of the page's text in the Document's `page_content` attribute. It will not parse text in images or scanned PDF pages. Under the hood it uses the [pypdf](https://pypdf.readthedocs.io/en/stable/) Python library.

LangChain [document loaders](/docs/concepts/document_loaders/) implement `lazy_load` and its async variant, `alazy_load`, which return iterators of `Document` objects. We will use these below.

```
`%pip install -qU pypdf`
```

```
`from langchain_community.document_loaders import PyPDFLoaderloader = PyPDFLoader(file_path)pages =[]asyncfor page in loader.alazy_load(): pages.append(page)`
```

**API Reference:**[PyPDFLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.pdf.PyPDFLoader.html)

```
`print(f"{pages[0].metadata}\n")print(pages[0].page_content)`
```

```
`{'source': '../../docs/integrations/document_loaders/example_data/layout-parser-paper.pdf', 'page': 0}LayoutParser : A UniÔ¨Åed Toolkit for DeepLearning Based Document Image AnalysisZejiang Shen1( ÔøΩ), Ruochen Zhang2, Melissa Dell3, Benjamin Charles GermainLee4, Jacob Carlson3, and Weining Li51Allen Institute for AIshannons@allenai.org2Brown Universityruochen zhang@brown.edu3Harvard University{melissadell,jacob carlson }@fas.harvard.edu4University of Washingtonbcgl@cs.washington.edu5University of Waterloow422li@uwaterloo.caAbstract. Recent advances in document image analysis (DIA) have beenprimarily driven by the application of neural networks. Ideally, researchoutcomes could be easily deployed in production and extended for furtherinvestigation. However, various factors like loosely organized codebasesand sophisticated model conÔ¨Ågurations complicate the easy reuse of im-portant innovations by a wide audience. Though there have been on-goingeÔ¨Äorts to improve reusability and simplify deep learning (DL) modeldevelopment in disciplines like natural language processing and computervision, none of them are optimized for challenges in the domain of DIA.This represents a major gap in the existing toolkit, as DIA is central toacademic research across a wide range of disciplines in the social sciencesand humanities. This paper introduces LayoutParser , an open-sourcelibrary for streamlining the usage of DL in DIA research and applica-tions. The core LayoutParser library comes with a set of simple andintuitive interfaces for applying and customizing DL models for layout de-tection, character recognition, and many other document processing tasks.To promote extensibility, LayoutParser also incorporates a communityplatform for sharing both pre-trained models and full document digiti-zation pipelines. We demonstrate that LayoutParser is helpful for bothlightweight and large-scale digitization pipelines in real-word use cases.The library is publicly available at https://layout-parser.github.io .Keywords: Document Image Analysis ¬∑Deep Learning ¬∑Layout Analysis¬∑Character Recognition ¬∑Open Source library ¬∑Toolkit.1 IntroductionDeep Learning(DL)-based approaches are the state-of-the-art for a wide range ofdocument image analysis (DIA) tasks including document image classiÔ¨Åcation [ 11,arXiv:2103.15348v2 [cs.CV] 21 Jun 2021`
```

Note that the metadata of each document stores the corresponding page number.

### Vector search over PDFs[‚Äã](#vector-search-over-pdfs "Direct link to Vector search over PDFs")

Once we have loaded PDFs into LangChain `Document` objects, we can index them (e.g., a RAG application) in the usual way. Below we use OpenAI embeddings, although any LangChain [embeddings](https://python.langchain.com/docs/concepts/embedding_models) model will suffice.

```
`%pip install -qU langchain-openai`
```

```
`import getpassimport osif"OPENAI_API_KEY"notin os.environ: os.environ["OPENAI_API_KEY"]= getpass.getpass("OpenAI API Key:")`
```

```
`from langchain_core.vectorstores import InMemoryVectorStorefrom langchain_openai import OpenAIEmbeddingsvector_store = InMemoryVectorStore.from_documents(pages, OpenAIEmbeddings())docs = vector_store.similarity_search("What is LayoutParser?", k=2)for doc in docs:print(f'Page {doc.metadata["page"]}: {doc.page_content[:300]}\n')`
```

**API Reference:**[InMemoryVectorStore](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.in_memory.InMemoryVectorStore.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)

```
`Page 13: 14 Z. Shen et al.6 ConclusionLayoutParser provides a comprehensive toolkit for deep learning-based documentimage analysis. The oÔ¨Ä-the-shelf library is easy to install, and can be used tobuild Ô¨Çexible and accurate pipelines for processing documents with complicatedstructures. It also supports hiPage 0: LayoutParser : A UniÔ¨Åed Toolkit for DeepLearning Based Document Image AnalysisZejiang Shen1( ÔøΩ), Ruochen Zhang2, Melissa Dell3, Benjamin Charles GermainLee4, Jacob Carlson3, and Weining Li51Allen Institute for AIshannons@allenai.org2Brown Universityruochen zhang@brown.edu3Harvard University`
```

## Layout analysis and extraction of text from images[‚Äã](#layout-analysis-and-extraction-of-text-from-images "Direct link to Layout analysis and extraction of text from images")

If you require a more granular segmentation of text (e.g., into distinct paragraphs, titles, tables, or other structures) or require extraction of text from images, the method below is appropriate. It will return a list of [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) objects, where each object represents a structure on the page. The Document's metadata stores the page number and other information related to the object (e.g., it might store table rows and columns in the case of a table object).

Under the hood it uses the `langchain-unstructured` library. See the [integration docs](/docs/integrations/document_loaders/unstructured_file/) for more information about using [Unstructured](https://docs.unstructured.io/welcome) with LangChain.

Unstructured supports multiple parameters for PDF parsing:

  * `strategy` (e.g., `"fast"` or `"hi-res"`)
  * API or local processing. You will need an API key to use the API.



The [hi-res](https://docs.unstructured.io/api-reference/how-to/choose-hi-res-model) strategy provides support for document layout analysis and OCR. We demonstrate it below via the API. See [local parsing](/docs/how_to/document_loader_pdf/#local-parsing) section below for considerations when running locally.

```
`%pip install -qU langchain-unstructured`
```

```
`import getpassimport osif"UNSTRUCTURED_API_KEY"notin os.environ: os.environ["UNSTRUCTURED_API_KEY"]= getpass.getpass("Unstructured API Key:")`
```

```
`Unstructured API Key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑`
```

As before, we initialize a loader and load documents lazily:

```
`from langchain_unstructured import UnstructuredLoaderloader = UnstructuredLoader( file_path=file_path, strategy="hi_res", partition_via_api=True, coordinates=True,)docs =[]for doc in loader.lazy_load(): docs.append(doc)`
```

**API Reference:**[UnstructuredLoader](https://python.langchain.com/api_reference/unstructured/document_loaders/langchain_unstructured.document_loaders.UnstructuredLoader.html)

```
`INFO: Preparing to split document for partition.INFO: Starting page number set to 1INFO: Allow failed set to 0INFO: Concurrency level set to 5INFO: Splitting pages 1 to 16 (16 total)INFO: Determined optimal split size of 4 pages.INFO: Partitioning 4 files with 4 page(s) each.INFO: Partitioning set #1 (pages 1-4).INFO: Partitioning set #2 (pages 5-8).INFO: Partitioning set #3 (pages 9-12).INFO: Partitioning set #4 (pages 13-16).INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general "HTTP/1.1 200 OK"INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general "HTTP/1.1 200 OK"INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general "HTTP/1.1 200 OK"INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general "HTTP/1.1 200 OK"INFO: Successfully partitioned set #1, elements added to the final result.INFO: Successfully partitioned set #2, elements added to the final result.INFO: Successfully partitioned set #3, elements added to the final result.INFO: Successfully partitioned set #4, elements added to the final result.`
```

Here we recover 171 distinct structures over the 16 page document:

```
`print(len(docs))`
```

```
`171`
```

We can use the document metadata to recover content from a single page:

```
`first_page_docs =[doc for doc in docs if doc.metadata.get("page_number")==1]for doc in first_page_docs:print(doc.page_content)`
```

```
`LayoutParser: A UniÔ¨Åed Toolkit for Deep Learning Based Document Image Analysis1 2 0 2 n u J 1 2 ] V C . s c [ 2 v 8 4 3 5 1 . 3 0 1 2 : v i X r aZejiang Shen¬Æ (<), Ruochen Zhang?, Melissa Dell¬Æ, Benjamin Charles Germain Lee?, Jacob Carlson¬Æ, and Weining Li¬Æ1 Allen Institute for AI shannons@allenai.org 2 Brown University ruochen zhang@brown.edu 3 Harvard University {melissadell,jacob carlson}@fas.harvard.edu 4 University of Washington bcgl@cs.washington.edu 5 University of Waterloo w422li@uwaterloo.caAbstract. Recent advances in document image analysis (DIA) have been primarily driven by the application of neural networks. Ideally, research outcomes could be easily deployed in production and extended for further investigation. However, various factors like loosely organized codebases and sophisticated model conÔ¨Ågurations complicate the easy reuse of im- portant innovations by a wide audience. Though there have been on-going eÔ¨Äorts to improve reusability and simplify deep learning (DL) model development in disciplines like natural language processing and computer vision, none of them are optimized for challenges in the domain of DIA. This represents a major gap in the existing toolkit, as DIA is central to academic research across a wide range of disciplines in the social sciences and humanities. This paper introduces LayoutParser, an open-source library for streamlining the usage of DL in DIA research and applica- tions. The core LayoutParser library comes with a set of simple and intuitive interfaces for applying and customizing DL models for layout de- tection, character recognition, and many other document processing tasks. To promote extensibility, LayoutParser also incorporates a community platform for sharing both pre-trained models and full document digiti- zation pipelines. We demonstrate that LayoutParser is helpful for both lightweight and large-scale digitization pipelines in real-word use cases. The library is publicly available at https://layout-parser.github.io.Keywords: Document Image Analysis ¬∑ Deep Learning ¬∑ Layout Analysis ¬∑ Character Recognition ¬∑ Open Source library ¬∑ Toolkit.1 IntroductionDeep Learning(DL)-based approaches are the state-of-the-art for a wide range of document image analysis (DIA) tasks including document image classiÔ¨Åcation [11,`
```

### Extracting tables and other structures[‚Äã](#extracting-tables-and-other-structures "Direct link to Extracting tables and other structures")

Each `Document` we load represents a structure, like a title, paragraph, or table.

Some structures may be of special interest for indexing or question-answering tasks. These structures may be:

  1. Classified for easy identification;
  2. Parsed into a more structured representation.



Below, we identify and extract a table:

Click to expand code for rendering pages

%pip install -qU matplotlib PyMuPDF pillow

```
`import fitzimport matplotlib.patches as patchesimport matplotlib.pyplot as pltfrom PIL import Imagedefplot_pdf_with_boxes(pdf_page, segments): pix = pdf_page.get_pixmap() pil_image = Image.frombytes("RGB",[pix.width, pix.height], pix.samples) fig, ax = plt.subplots(1, figsize=(10,10)) ax.imshow(pil_image) categories =set() category_to_color ={"Title":"orchid","Image":"forestgreen","Table":"tomato",}for segment in segments: points = segment["coordinates"]["points"] layout_width = segment["coordinates"]["layout_width"] layout_height = segment["coordinates"]["layout_height"] scaled_points =[(x * pix.width / layout_width, y * pix.height / layout_height)for x, y in points] box_color = category_to_color.get(segment["category"],"deepskyblue") categories.add(segment["category"]) rect = patches.Polygon( scaled_points, linewidth=1, edgecolor=box_color, facecolor="none") ax.add_patch(rect)# Make legend legend_handles =[patches.Patch(color="deepskyblue", label="Text")]for category in["Title","Image","Table"]:if category in categories: legend_handles.append( patches.Patch(color=category_to_color[category], label=category)) ax.axis("off") ax.legend(handles=legend_handles, loc="upper right") plt.tight_layout() plt.show()defrender_page(doc_list:list, page_number:int, print_text=True)->None: pdf_page = fitz.open(file_path).load_page(page_number -1) page_docs =[ doc for doc in doc_list if doc.metadata.get("page_number")== page_number] segments =[doc.metadata for doc in page_docs] plot_pdf_with_boxes(pdf_page, segments)if print_text:for doc in page_docs:print(f"{doc.page_content}\n")`
```

```
`render_page(docs,5)`
```

![]()

```
`LayoutParser: A UniÔ¨Åed Toolkit for DL-Based DIA5Table 1: Current layout detection models in the LayoutParser model zooDataset Base Model1 Large Model Notes PubLayNet [38] PRImA [3] Newspaper [17] TableBank [18] HJDataset [31] F / M M F F F / M M - - F - Layouts of modern scientiÔ¨Åc documents Layouts of scanned modern magazines and scientiÔ¨Åc reports Layouts of scanned US newspapers from the 20th century Table region on modern scientiÔ¨Åc and business document Layouts of history Japanese documents1 For each dataset, we train several models of diÔ¨Äerent sizes for diÔ¨Äerent needs (the trade-oÔ¨Ä between accuracy vs. computational cost). For ‚Äúbase model‚Äù and ‚Äúlarge model‚Äù, we refer to using the ResNet 50 or ResNet 101 backbones [13], respectively. One can train models of diÔ¨Äerent architectures, like Faster R-CNN [28] (F) and Mask R-CNN [12] (M). For example, an F in the Large Model column indicates it has a Faster R-CNN model trained using the ResNet 101 backbone. The platform is maintained and a number of additions will be made to the model zoo in coming months.layout data structures, which are optimized for eÔ¨Éciency and versatility. 3) When necessary, users can employ existing or customized OCR models via the uniÔ¨Åed API provided in the OCR module. 4) LayoutParser comes with a set of utility functions for the visualization and storage of the layout data. 5) LayoutParser is also highly customizable, via its integration with functions for layout data annotation and model training. We now provide detailed descriptions for each component.3.1 Layout Detection ModelsIn LayoutParser, a layout model takes a document image as an input and generates a list of rectangular boxes for the target content regions. DiÔ¨Äerent from traditional methods, it relies on deep convolutional neural networks rather than manually curated rules to identify content regions. It is formulated as an object detection problem and state-of-the-art models like Faster R-CNN [28] and Mask R-CNN [12] are used. This yields prediction results of high accuracy and makes it possible to build a concise, generalized interface for layout detection. LayoutParser, built upon Detectron2 [35], provides a minimal API that can perform layout detection with only four lines of code in Python:1 import layoutparser as lp 2 image = cv2 . imread ( " image_file " ) # load images 3 model = lp . De t e c tro n2 Lay outM odel ( " lp :// PubLayNet / f as t er _ r c nn _ R _ 50 _ F P N_ 3 x / config " ) 4 5 layout = model . detect ( image )LayoutParser provides a wealth of pre-trained model weights using various datasets covering diÔ¨Äerent languages, time periods, and document types. Due to domain shift [7], the prediction performance can notably drop when models are ap- plied to target samples that are signiÔ¨Åcantly diÔ¨Äerent from the training dataset. As document structures and layouts vary greatly in diÔ¨Äerent domains, it is important to select models trained on a dataset similar to the test samples. A semantic syntax is used for initializing the model weights in LayoutParser, using both the dataset name and model name lp://<dataset-name>/<model-architecture-name>.`
```

Note that although the table text is collapsed into a single string in the document's content, the metadata contains a representation of its rows and columns:

```
`from IPython.display import HTML, displaysegments =[ doc.metadatafor doc in docsif doc.metadata.get("page_number")==5and doc.metadata.get("category")=="Table"]display(HTML(segments[0]["text_as_html"]))`
```

able 1. LUllclll 1ayoul actCCLloll 1110AdCs 111 L1C LayoOulralsel 1110U4cl 200  
---  
Dataset| | Base Model'|| Notes  
PubLayNet [38]| F/M| Layouts of modern scientific documents  
PRImA| M| Layouts of scanned modern magazines and scientific reports  
Newspaper| F| Layouts of scanned US newspapers from the 20th century  
TableBank [18]| F| Table region on modern scientific and business document  
HJDataset| F/M| Layouts of history Japanese documents  
  
### Extracting text from specific sections[‚Äã](#extracting-text-from-specific-sections "Direct link to Extracting text from specific sections")

Structures may have parent-child relationships -- for example, a paragraph might belong to a section with a title. If a section is of particular interest (e.g., for indexing) we can isolate the corresponding `Document` objects.

Below, we extract all text associated with the document's "Conclusion" section:

```
`render_page(docs,14, print_text=False)`
```

![]()

```
`conclusion_docs =[]parent_id =-1for doc in docs:if doc.metadata["category"]=="Title"and"Conclusion"in doc.page_content: parent_id = doc.metadata["element_id"]if doc.metadata.get("parent_id")== parent_id: conclusion_docs.append(doc)for doc in conclusion_docs:print(doc.page_content)`
```

```
`LayoutParser provides a comprehensive toolkit for deep learning-based document image analysis. The oÔ¨Ä-the-shelf library is easy to install, and can be used to build Ô¨Çexible and accurate pipelines for processing documents with complicated structures. It also supports high-level customization and enables easy labeling and training of DL models on unique document image datasets. The LayoutParser community platform facilitates sharing DL models and DIA pipelines, inviting discussion and promoting code reproducibility and reusability. The LayoutParser team is committed to keeping the library updated continuously and bringing the most recent advances in DL-based DIA, such as multi-modal document modeling [37, 36, 9] (an upcoming priority), to a diverse audience of end-users.Acknowledgements We thank the anonymous reviewers for their comments and suggestions. This project is supported in part by NSF Grant OIA-2033558 and funding from the Harvard Data Science Initiative and Harvard Catalyst. Zejiang Shen thanks Doug Downey for suggestions.`
```

### Extracting text from images[‚Äã](#extracting-text-from-images "Direct link to Extracting text from images")

OCR is run on images, enabling the extraction of text therein:

```
`render_page(docs,11)`
```

![]()

```
`LayoutParser: A UniÔ¨Åed Toolkit for DL-Based DIAfocuses on precision, eÔ¨Éciency, and robustness. The target documents may have complicated structures, and may require training multiple layout detection models to achieve the optimal accuracy. Light-weight pipelines are built for relatively simple documents, with an emphasis on development ease, speed and Ô¨Çexibility. Ideally one only needs to use existing resources, and model training should be avoided. Through two exemplar projects, we show how practitioners in both academia and industry can easily build such pipelines using LayoutParser and extract high-quality structured document data for their downstream tasks. The source code for these projects will be publicly available in the LayoutParser community hub.115.1 A Comprehensive Historical Document Digitization PipelineThe digitization of historical documents can unlock valuable data that can shed light on many important social, economic, and historical questions. Yet due to scan noises, page wearing, and the prevalence of complicated layout structures, ob- taining a structured representation of historical document scans is often extremely complicated. In this example, LayoutParser was used to develop a comprehensive pipeline, shown in Figure 5, to gener- ate high-quality structured data from historical Japanese Ô¨Årm Ô¨Ånancial ta- bles with complicated layouts. The pipeline applies two layout models to identify diÔ¨Äerent levels of document structures and two customized OCR engines for optimized character recog- nition accuracy.‚ÄòActive Learning Layout Annotate Layout Dataset | +‚Äî‚Äî Annotation Toolkit A4 Deep Learning Layout Layout Detection Model Training & Inference, A Post-processing ‚Äî Handy Data Structures & \ Lo orajport 7 ) Al Pls for Layout Data A4 Default and Customized Text Recognition 0CR Models ¬• Visualization & Export Layout Structure Visualization & Storage The Japanese Document Helpful LayoutParser Modules Digitization PipelineAs shown in Figure 4 (a), the document contains columns of text written vertically 15, a common style in Japanese. Due to scanning noise and archaic printing technology, the columns can be skewed or have vari- able widths, and hence cannot be eas- ily identiÔ¨Åed via rule-based methods. Within each column, words are sepa- rated by white spaces of variable size, and the vertical positions of objects can be an indicator of their layout type.Fig. 5: Illustration of how LayoutParser helps with the historical document digi- tization pipeline.15 A document page consists of eight rows like this. For simplicity we skip the row segmentation discussion and refer readers to the source code when available.`
```

Note that the text from the figure on the right is extracted and incorporated into the content of the `Document`.

### Local parsing[‚Äã](#local-parsing "Direct link to Local parsing")

Parsing locally requires the installation of additional dependencies.

**Poppler** (PDF analysis)

  * Linux: `apt-get install poppler-utils`
  * Mac: `brew install poppler`
  * Windows: <https://github.com/oschwartz10612/poppler-windows>



**Tesseract** (OCR)

  * Linux: `apt-get install tesseract-ocr`
  * Mac: `brew install tesseract`
  * Windows: <https://github.com/UB-Mannheim/tesseract/wiki#tesseract-installer-for-windows>



We will also need to install the `unstructured` PDF extras:

```
`%pip install -qU "unstructured[pdf]"`
```

We can then use the [UnstructuredLoader](https://python.langchain.com/api_reference/unstructured/document_loaders/langchain_unstructured.document_loaders.UnstructuredLoader.html) much the same way, forgoing the API key and `partition_via_api` setting:

```
`loader_local = UnstructuredLoader( file_path=file_path, strategy="hi_res",)docs_local =[]for doc in loader_local.lazy_load(): docs_local.append(doc)`
```

```
`WARNING: This function will be deprecated in a future release and `unstructured` will simply use the DEFAULT_MODEL from `unstructured_inference.model.base` to set default model nameINFO: Reading PDF for file: /Users/chestercurme/repos/langchain/libs/community/tests/integration_tests/examples/layout-parser-paper.pdf ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: padding image by 20 for structure detectionINFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: padding image by 20 for structure detectionINFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...`
```

The list of documents can then be processed similarly to those obtained from the API.

## Use of multimodal models[‚Äã](#use-of-multimodal-models "Direct link to Use of multimodal models")

Many modern LLMs support inference over multimodal inputs (e.g., images). In some applications-- such as question-answering over PDFs with complex layouts, diagrams, or scans-- it may be advantageous to skip the PDF parsing, instead casting a PDF page to an image and passing it to a model directly. This allows a model to reason over the two dimensional content on the page, instead of a "one-dimensional" string representation.

In principle we can use any LangChain [chat model](/docs/concepts/chat_models/) that supports multimodal inputs. A list of these models is documented [here](/docs/integrations/chat/). Below we use OpenAI's `gpt-4o-mini`.

First we define a short utility function to convert a PDF page to a base64-encoded image:

```
`%pip install -qU PyMuPDF pillow langchain-openai`
```

```
`import base64import ioimport fitzfrom PIL import Imagedefpdf_page_to_base64(pdf_path:str, page_number:int): pdf_document = fitz.open(pdf_path) page = pdf_document.load_page(page_number -1)# input is one-indexed pix = page.get_pixmap() img = Image.frombytes("RGB",[pix.width, pix.height], pix.samples)buffer= io.BytesIO() img.save(buffer,format="PNG")return base64.b64encode(buffer.getvalue()).decode("utf-8")`
```

```
`from IPython.display import Image as IPImagefrom IPython.display import displaybase64_image = pdf_page_to_base64(file_path,11)display(IPImage(data=base64.b64decode(base64_image)))`
```

![]()

We can then query the model in the [usual way](/docs/how_to/multimodal_inputs/). Below we ask it a question on related to the diagram on the page.

```
`from langchain_openai import ChatOpenAIllm = ChatOpenAI(model="gpt-4o-mini")`
```

**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

```
`from langchain_core.messages import HumanMessagequery ="What is the name of the first step in the pipeline?"message = HumanMessage( content=[{"type":"text","text": query},{"type":"image_url","image_url":{"url":f"data:image/jpeg;base64,{base64_image}"},},],)response = llm.invoke([message])print(response.content)`
```

**API Reference:**[HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html)

```
`INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"``````outputThe first step in the pipeline is "Annotate Layout Dataset."`
```

## Other PDF loaders[‚Äã](#other-pdf-loaders "Direct link to Other PDF loaders")

For a list of available LangChain PDF loaders, please see [this table](/docs/integrations/document_loaders/#pdfs).

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/document_loader_pdf.ipynb)

#### Was this page helpful?

[PreviousHow to load Microsoft Office files](/docs/how_to/document_loader_office_file/)[NextHow to load web pages](/docs/how_to/document_loader_web/)

  * [Simple and fast text extraction](#simple-and-fast-text-extraction)
    * [Vector search over PDFs](#vector-search-over-pdfs)
  * [Layout analysis and extraction of text from images](#layout-analysis-and-extraction-of-text-from-images)
    * [Extracting tables and other structures](#extracting-tables-and-other-structures)
    * [Extracting text from specific sections](#extracting-text-from-specific-sections)
    * [Extracting text from images](#extracting-text-from-images)
    * [Local parsing](#local-parsing)
  * [Use of multimodal models](#use-of-multimodal-models)
  * [Other PDF loaders](#other-pdf-loaders)



Community

  * [Twitter](https://twitter.com/LangChainAI)



GitHub

  * [Organization](https://github.com/langchain-ai)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)



More

  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [YouTube](https://www.youtube.com/@LangChain)



Copyright ¬© 2025 LangChain, Inc.

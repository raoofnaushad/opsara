{
    "id": "9355eaf8e19ae0ccad220fbf09efef50",
    "metadata": {
        "id": "9355eaf8e19ae0ccad220fbf09efef50",
        "url": "https://github.com/NVIDIA/TensorRT-LLM/",
        "title": "GitHub - NVIDIA/TensorRT-LLM: TensorRT-LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and build TensorRT engines that contain state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. TensorRT-LLM also contains components to create Python and C++ runtimes that execute those TensorRT engines.",
        "properties": {
            "description": "TensorRT-LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and build TensorRT engines that contain state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. TensorRT-LLM also contains components to create Python and C++ runtimes that execute those TensorRT engines. - NVIDIA/TensorRT-LLM",
            "keywords": null,
            "author": null,
            "og:image": "https://opengraph.githubassets.com/b6de68282daa6e5bf0f125ed1d4d2b8ec9c61f677ae85a4597182c3dd1d3868a/NVIDIA/TensorRT-LLM",
            "og:image:alt": "TensorRT-LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and build TensorRT engines that contain state-of-the-art optimizations to perform inference efficie...",
            "og:image:width": "1200",
            "og:image:height": "600",
            "og:site_name": "GitHub",
            "og:type": "object",
            "og:title": "GitHub - NVIDIA/TensorRT-LLM: TensorRT-LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and build TensorRT engines that contain state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. TensorRT-LLM also contains components to create Python and C++ runtimes that execute those TensorRT engines.",
            "og:url": "https://github.com/NVIDIA/TensorRT-LLM",
            "og:description": "TensorRT-LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and build TensorRT engines that contain state-of-the-art optimizations to perform inference efficie...",
            "twitter:image": "https://opengraph.githubassets.com/b6de68282daa6e5bf0f125ed1d4d2b8ec9c61f677ae85a4597182c3dd1d3868a/NVIDIA/TensorRT-LLM",
            "twitter:site": "@github",
            "twitter:card": "summary_large_image",
            "twitter:title": "GitHub - NVIDIA/TensorRT-LLM: TensorRT-LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and build TensorRT engines that contain state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. TensorRT-LLM also contains components to create Python and C++ runtimes that execute those TensorRT engines.",
            "twitter:description": "TensorRT-LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and build TensorRT engines that contain state-of-the-art optimizations to perform inference efficie..."
        }
    },
    "parent_metadata": {
        "id": "c541dc56eaccd6b081a4719df85abb7e",
        "url": "https://www.notion.so/Inference-Engines-c541dc56eaccd6b081a4719df85abb7e",
        "title": "Inference Engines",
        "properties": {
            "Type": "Leaf"
        }
    },
    "content": "[Skip to content](#start-of-content)\n\n## Navigation Menu\n\nToggle navigation\n\n[ ](/)\n\n[ Sign in ](/login?return_to=https%3A%2F%2Fgithub.com%2FNVIDIA%2FTensorRT-LLM%2F)\n\n  * Product \n\n    * [ GitHub Copilot Write better code with AI  ](https://github.com/features/copilot)\n    * [ Security Find and fix vulnerabilities  ](https://github.com/features/security)\n    * [ Actions Automate any workflow  ](https://github.com/features/actions)\n    * [ Codespaces Instant dev environments  ](https://github.com/features/codespaces)\n    * [ Issues Plan and track work  ](https://github.com/features/issues)\n    * [ Code Review Manage code changes  ](https://github.com/features/code-review)\n    * [ Discussions Collaborate outside of code  ](https://github.com/features/discussions)\n    * [ Code Search Find more, search less  ](https://github.com/features/code-search)\n\nExplore\n    * [ All features ](https://github.com/features)\n    * [ Documentation ](https://docs.github.com)\n    * [ GitHub Skills ](https://skills.github.com)\n    * [ Blog ](https://github.blog)\n\n  * Solutions \n\nBy company size\n    * [ Enterprises ](https://github.com/enterprise)\n    * [ Small and medium teams ](https://github.com/team)\n    * [ Startups ](https://github.com/enterprise/startups)\n    * [ Nonprofits ](/solutions/industry/nonprofits)\n\nBy use case\n    * [ DevSecOps ](/solutions/use-case/devsecops)\n    * [ DevOps ](/solutions/use-case/devops)\n    * [ CI/CD ](/solutions/use-case/ci-cd)\n    * [ View all use cases ](/solutions/use-case)\n\nBy industry\n    * [ Healthcare ](/solutions/industry/healthcare)\n    * [ Financial services ](/solutions/industry/financial-services)\n    * [ Manufacturing ](/solutions/industry/manufacturing)\n    * [ Government ](/solutions/industry/government)\n    * [ View all industries ](/solutions/industry)\n\n[ View all solutions ](/solutions)\n\n  * Resources \n\nTopics\n    * [ AI ](/resources/articles/ai)\n    * [ DevOps ](/resources/articles/devops)\n    * [ Security ](/resources/articles/security)\n    * [ Software Development ](/resources/articles/software-development)\n    * [ View all ](/resources/articles)\n\nExplore\n    * [ Learning Pathways ](https://resources.github.com/learn/pathways)\n    * [ White papers, Ebooks, Webinars ](https://resources.github.com)\n    * [ Customer Stories ](https://github.com/customer-stories)\n    * [ Partners ](https://partner.github.com)\n    * [ Executive Insights ](https://github.com/solutions/executive-insights)\n\n  * Open Source \n\n    * [ GitHub Sponsors Fund open source developers  ](/sponsors)\n\n    * [ The ReadME Project GitHub community articles  ](https://github.com/readme)\n\nRepositories\n    * [ Topics ](https://github.com/topics)\n    * [ Trending ](https://github.com/trending)\n    * [ Collections ](https://github.com/collections)\n\n  * Enterprise \n\n    * [ Enterprise platform AI-powered developer platform  ](/enterprise)\n\nAvailable add-ons\n    * [ Advanced Security Enterprise-grade security features  ](https://github.com/enterprise/advanced-security)\n    * [ GitHub Copilot Enterprise-grade AI features  ](/features/copilot#enterprise)\n    * [ Premium Support Enterprise-grade 24/7 support  ](/premium-support)\n\n  * [Pricing](https://github.com/pricing)\n\n\n\nSearch or jump to...\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch \n\nClear\n\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n\n#  Provide feedback \n\nWe read every piece of feedback, and take your input very seriously.\n\nInclude my email address so I can be contacted\n\nCancel  Submit feedback \n\n#  Saved searches \n\n## Use saved searches to filter your results more quickly\n\nName\n\nQuery\n\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax). \n\nCancel  Create saved search \n\n[ Sign in ](/login?return_to=https%3A%2F%2Fgithub.com%2FNVIDIA%2FTensorRT-LLM%2F)\n\n[ Sign up ](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=NVIDIA%2FTensorRT-LLM) Reseting focus\n\nYou signed in with another tab or window. [Reload]() to refresh your session. You signed out in another tab or window. [Reload]() to refresh your session. You switched accounts on another tab or window. [Reload]() to refresh your session. Dismiss alert\n\n{{ message }}\n\n[ NVIDIA ](/NVIDIA) / **[TensorRT-LLM](/NVIDIA/TensorRT-LLM) ** Public\n\n  * [ Notifications ](/login?return_to=%2FNVIDIA%2FTensorRT-LLM) You must be signed in to change notification settings\n  * [ Fork 1.1k ](/login?return_to=%2FNVIDIA%2FTensorRT-LLM)\n  * [ Star  9.2k ](/login?return_to=%2FNVIDIA%2FTensorRT-LLM)\n\n\n\n\nTensorRT-LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and build TensorRT engines that contain state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. TensorRT-LLM also contains components to create Python and C++ runtimes that execute those TensorRT engines. \n\n[nvidia.github.io/TensorRT-LLM](https://nvidia.github.io/TensorRT-LLM \"https://nvidia.github.io/TensorRT-LLM\")\n\n### License\n\n[ Apache-2.0 license ](/NVIDIA/TensorRT-LLM/blob/main/LICENSE)\n\n[ 9.2k stars ](/NVIDIA/TensorRT-LLM/stargazers) [ 1.1k forks ](/NVIDIA/TensorRT-LLM/forks) [ Branches ](/NVIDIA/TensorRT-LLM/branches) [ Tags ](/NVIDIA/TensorRT-LLM/tags) [ Activity ](/NVIDIA/TensorRT-LLM/activity)\n\n[ Star  ](/login?return_to=%2FNVIDIA%2FTensorRT-LLM)\n\n[ Notifications ](/login?return_to=%2FNVIDIA%2FTensorRT-LLM) You must be signed in to change notification settings\n\n  * [ Code ](/NVIDIA/TensorRT-LLM)\n  * [ Issues 360 ](/NVIDIA/TensorRT-LLM/issues)\n  * [ Pull requests 72 ](/NVIDIA/TensorRT-LLM/pulls)\n  * [ Discussions ](/NVIDIA/TensorRT-LLM/discussions)\n  * [ Actions ](/NVIDIA/TensorRT-LLM/actions)\n  * [ Projects 0 ](/NVIDIA/TensorRT-LLM/projects)\n  * [ Security ](/NVIDIA/TensorRT-LLM/security)\n  * [ Insights ](/NVIDIA/TensorRT-LLM/pulse)\n\n\n\nAdditional navigation options\n\n  * [ Code  ](/NVIDIA/TensorRT-LLM)\n  * [ Issues  ](/NVIDIA/TensorRT-LLM/issues)\n  * [ Pull requests  ](/NVIDIA/TensorRT-LLM/pulls)\n  * [ Discussions  ](/NVIDIA/TensorRT-LLM/discussions)\n  * [ Actions  ](/NVIDIA/TensorRT-LLM/actions)\n  * [ Projects  ](/NVIDIA/TensorRT-LLM/projects)\n  * [ Security  ](/NVIDIA/TensorRT-LLM/security)\n  * [ Insights  ](/NVIDIA/TensorRT-LLM/pulse)\n\n\n\n# NVIDIA/TensorRT-LLM\n\nmain\n\n[**11** Branches](/NVIDIA/TensorRT-LLM/branches)[**16** Tags](/NVIDIA/TensorRT-LLM/tags)\n\n[](/NVIDIA/TensorRT-LLM/branches)[](/NVIDIA/TensorRT-LLM/tags)\n\nGo to file\n\nCode\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\n## History\n\n[124 Commits](/NVIDIA/TensorRT-LLM/commits/main/)[](/NVIDIA/TensorRT-LLM/commits/main/)  \n[.github](/NVIDIA/TensorRT-LLM/tree/main/.github \".github\")| [.github](/NVIDIA/TensorRT-LLM/tree/main/.github \".github\")| [Update TensorRT-LLM (](/NVIDIA/TensorRT-LLM/commit/be1788106245496872d18e702978e59b6bfd50e0 \"Update TensorRT-LLM \\(#2582\\)\")[#2582](https://github.com/NVIDIA/TensorRT-LLM/pull/2582)[)](/NVIDIA/TensorRT-LLM/commit/be1788106245496872d18e702978e59b6bfd50e0 \"Update TensorRT-LLM \\(#2582\\)\")| Dec 17, 2024  \n[3rdparty](/NVIDIA/TensorRT-LLM/tree/main/3rdparty \"3rdparty\")| [3rdparty](/NVIDIA/TensorRT-LLM/tree/main/3rdparty \"3rdparty\")| [Update TensorRT-LLM (](/NVIDIA/TensorRT-LLM/commit/548b5b73106aaf7374955e1c37aad677678ebc7b \"Update TensorRT-LLM \\(#2532\\)\n* blossom-ci.yml: run vulnerability scan on blossom\n* open source efb18c1256f8c9c3d47b7d0c740b83e5d5ebe0ec\n---------\nCo-authored-by: niukuo <6831097+niukuo@users.noreply.github.com>\nCo-authored-by: pei0033 <59505847+pei0033@users.noreply.github.com>\nCo-authored-by: Kyungmin Lee <30465912+lkm2835@users.noreply.github.com>\nCo-authored-by: Kaiyu Xie <26294424+kaiyux@users.noreply.github.com>\")[#2532](https://github.com/NVIDIA/TensorRT-LLM/pull/2532)[)](/NVIDIA/TensorRT-LLM/commit/548b5b73106aaf7374955e1c37aad677678ebc7b \"Update TensorRT-LLM \\(#2532\\)\n* blossom-ci.yml: run vulnerability scan on blossom\n* open source efb18c1256f8c9c3d47b7d0c740b83e5d5ebe0ec\n---------\nCo-authored-by: niukuo <6831097+niukuo@users.noreply.github.com>\nCo-authored-by: pei0033 <59505847+pei0033@users.noreply.github.com>\nCo-authored-by: Kyungmin Lee <30465912+lkm2835@users.noreply.github.com>\nCo-authored-by: Kaiyu Xie <26294424+kaiyux@users.noreply.github.com>\")| Dec 4, 2024  \n[benchmarks](/NVIDIA/TensorRT-LLM/tree/main/benchmarks \"benchmarks\")| [benchmarks](/NVIDIA/TensorRT-LLM/tree/main/benchmarks \"benchmarks\")| [Update TensorRT-LLM (](/NVIDIA/TensorRT-LLM/commit/be1788106245496872d18e702978e59b6bfd50e0 \"Update TensorRT-LLM \\(#2582\\)\")[#2582](https://github.com/NVIDIA/TensorRT-LLM/pull/2582)[)](/NVIDIA/TensorRT-LLM/commit/be1788106245496872d18e702978e59b6bfd50e0 \"Update TensorRT-LLM \\(#2582\\)\")| Dec 17, 2024  \n[cpp](/NVIDIA/TensorRT-LLM/tree/main/cpp \"cpp\")| [cpp](/NVIDIA/TensorRT-LLM/tree/main/cpp \"cpp\")| [Update TensorRT-LLM (](/NVIDIA/TensorRT-LLM/commit/be1788106245496872d18e702978e59b6bfd50e0 \"Update TensorRT-LLM \\(#2582\\)\")[#2582](https://github.com/NVIDIA/TensorRT-LLM/pull/2582)[)](/NVIDIA/TensorRT-LLM/commit/be1788106245496872d18e702978e59b6bfd50e0 \"Update TensorRT-LLM \\(#2582\\)\")| Dec 17, 2024  \n[docker](/NVIDIA/TensorRT-LLM/tree/main/docker \"docker\")| [docker](/NVIDIA/TensorRT-LLM/tree/main/docker \"docker\")| [Update TensorRT-LLM (](/NVIDIA/TensorRT-LLM/commit/aaacc9bd68b3e839fd25a0c7a80964d9b393ac99 \"Update TensorRT-LLM \\(#2562\\)\n* Update TensorRT-LLM\n---------\nCo-authored-by: Starrick Liu <73152103+StarrickLiu@users.noreply.github.com>\")[#2562](https://github.com/NVIDIA/TensorRT-LLM/pull/2562)[)](/NVIDIA/TensorRT-LLM/commit/aaacc9bd68b3e839fd25a0c7a80964d9b393ac99 \"Update TensorRT-LLM \\(#2562\\)\n* Update TensorRT-LLM\n---------\nCo-authored-by: Starrick Liu <73152103+StarrickLiu@users.noreply.github.com>\")| Dec 11, 2024  \n[docs](/NVIDIA/TensorRT-LLM/tree/main/docs \"docs\")| [docs](/NVIDIA/TensorRT-LLM/tree/main/docs \"docs\")| [Update TensorRT-LLM (](/NVIDIA/TensorRT-LLM/commit/be1788106245496872d18e702978e59b6bfd50e0 \"Update TensorRT-LLM \\(#2582\\)\")[#2582](https://github.com/NVIDIA/TensorRT-LLM/pull/2582)[)](/NVIDIA/TensorRT-LLM/commit/be1788106245496872d18e702978e59b6bfd50e0 \"Update TensorRT-LLM \\(#2582\\)\")| Dec 17, 2024  \n[examples](/NVIDIA/TensorRT-LLM/tree/main/examples \"examples\")| [examples](/NVIDIA/TensorRT-LLM/tree/main/examples \"examples\")| [Update TensorRT-LLM (](/NVIDIA/TensorRT-LLM/commit/be1788106245496872d18e702978e59b6bfd50e0 \"Update TensorRT-LLM \\(#2582\\)\")[#2582](https://github.com/NVIDIA/TensorRT-LLM/pull/2582)[)](/NVIDIA/TensorRT-LLM/commit/be1788106245496872d18e702978e59b6bfd50e0 \"Update TensorRT-LLM \\(#2582\\)\")| Dec 17, 2024  \n[scripts](/NVIDIA/TensorRT-LLM/tree/main/scripts \"scripts\")| [scripts](/NVIDIA/TensorRT-LLM/tree/main/scripts \"scripts\")| [Update TensorRT-LLM (](/NVIDIA/TensorRT-LLM/commit/548b5b73106aaf7374955e1c37aad677678ebc7b \"Update TensorRT-LLM \\(#2532\\)\n* blossom-ci.yml: run vulnerability scan on blossom\n* open source efb18c1256f8c9c3d47b7d0c740b83e5d5ebe0ec\n---------\nCo-authored-by: niukuo <6831097+niukuo@users.noreply.github.com>\nCo-authored-by: pei0033 <59505847+pei0033@users.noreply.github.com>\nCo-authored-by: Kyungmin Lee <30465912+lkm2835@users.noreply.github.com>\nCo-authored-by: Kaiyu Xie <26294424+kaiyux@users.noreply.github.com>\")[#2532](https://github.com/NVIDIA/TensorRT-LLM/pull/2532)[)](/NVIDIA/TensorRT-LLM/commit/548b5b73106aaf7374955e1c37aad677678ebc7b \"Update TensorRT-LLM \\(#2532\\)\n* blossom-ci.yml: run vulnerability scan on blossom\n* open source efb18c1256f8c9c3d47b7d0c740b83e5d5ebe0ec\n---------\nCo-authored-by: niukuo <6831097+niukuo@users.noreply.github.com>\nCo-authored-by: pei0033 <59505847+pei0033@users.noreply.github.com>\nCo-authored-by: Kyungmin Lee <30465912+lkm2835@users.noreply.github.com>\nCo-authored-by: Kaiyu Xie <26294424+kaiyux@users.noreply.github.com>\")| Dec 4, 2024  \n[tensorrt_llm](/NVIDIA/TensorRT-LLM/tree/main/tensorrt_llm \"tensorrt_llm\")| [tensorrt_llm](/NVIDIA/TensorRT-LLM/tree/main/tensorrt_llm \"tensorrt_llm\")| [Fix kwarg name (](/NVIDIA/TensorRT-LLM/commit/d93a2dde84eada06ae2339b4fb4e6432167a1cfd \"Fix kwarg name \\(#2691\\)\")[#2691](https://github.com/NVIDIA/TensorRT-LLM/pull/2691)[)](/NVIDIA/TensorRT-LLM/commit/d93a2dde84eada06ae2339b4fb4e6432167a1cfd \"Fix kwarg name \\(#2691\\)\")| Jan 20, 2025  \n[tests](/NVIDIA/TensorRT-LLM/tree/main/tests \"tests\")| [tests](/NVIDIA/TensorRT-LLM/tree/main/tests \"tests\")| [Update TensorRT-LLM (](/NVIDIA/TensorRT-LLM/commit/be1788106245496872d18e702978e59b6bfd50e0 \"Update TensorRT-LLM \\(#2582\\)\")[#2582](https://github.com/NVIDIA/TensorRT-LLM/pull/2582)[)](/NVIDIA/TensorRT-LLM/commit/be1788106245496872d18e702978e59b6bfd50e0 \"Update TensorRT-LLM \\(#2582\\)\")| Dec 17, 2024  \n[windows](/NVIDIA/TensorRT-LLM/tree/main/windows \"windows\")| [windows](/NVIDIA/TensorRT-LLM/tree/main/windows \"windows\")| [Update TensorRT-LLM (](/NVIDIA/TensorRT-LLM/commit/be1788106245496872d18e702978e59b6bfd50e0 \"Update TensorRT-LLM \\(#2582\\)\")[#2582](https://github.com/NVIDIA/TensorRT-LLM/pull/2582)[)](/NVIDIA/TensorRT-LLM/commit/be1788106245496872d18e702978e59b6bfd50e0 \"Update TensorRT-LLM \\(#2582\\)\")| Dec 17, 2024  \n[.clang-format](/NVIDIA/TensorRT-LLM/blob/main/.clang-format \".clang-format\")| [.clang-format](/NVIDIA/TensorRT-LLM/blob/main/.clang-format \".clang-format\")| [Update TensorRT-LLM (](/NVIDIA/TensorRT-LLM/commit/4bb65f216f2f1dbddb022f4fdd8925c2856baa58 \"Update TensorRT-LLM \\(#1274\\)\n* Update TensorRT-LLM\n---------\nCo-authored-by: meghagarwal <16129366+megha95@users.noreply.github.com>\nCo-authored-by: Shixiaowei02 <39303645+Shixiaowei02@users.noreply.github.com>\")[#1274](https://github.com/NVIDIA/TensorRT-LLM/pull/1274)[)](/NVIDIA/TensorRT-LLM/commit/4bb65f216f2f1dbddb022f4fdd8925c2856baa58 \"Update TensorRT-LLM \\(#1274\\)\n* Update TensorRT-LLM\n---------\nCo-authored-by: meghagarwal <16129366+megha95@users.noreply.github.com>\nCo-authored-by: Shixiaowei02 <39303645+Shixiaowei02@users.noreply.github.com>\")| Mar 12, 2024  \n[.dockerignore](/NVIDIA/TensorRT-LLM/blob/main/.dockerignore \".dockerignore\")| [.dockerignore](/NVIDIA/TensorRT-LLM/blob/main/.dockerignore \".dockerignore\")| [Update TensorRT-LLM (](/NVIDIA/TensorRT-LLM/commit/b57221b764bc579cbb2490154916a871f620e2c4 \"Update TensorRT-LLM \\(#941\\)\n* Update TensorRT-LLM\n---------\nCo-authored-by: Shixiaowei02 <39303645+Shixiaowei02@users.noreply.github.com>\")[#941](https://github.com/NVIDIA/TensorRT-LLM/pull/941)[)](/NVIDIA/TensorRT-LLM/commit/b57221b764bc579cbb2490154916a871f620e2c4 \"Update TensorRT-LLM \\(#941\\)\n* Update TensorRT-LLM\n---------\nCo-authored-by: Shixiaowei02 <39303645+Shixiaowei02@users.noreply.github.com>\")| Jan 23, 2024  \n[.gitattributes](/NVIDIA/TensorRT-LLM/blob/main/.gitattributes \".gitattributes\")| [.gitattributes](/NVIDIA/TensorRT-LLM/blob/main/.gitattributes \".gitattributes\")| [Update TensorRT-LLM (](/NVIDIA/TensorRT-LLM/commit/89ba1b1a67d570e41b03da87e5518eaff0d31fbf \"Update TensorRT-LLM \\(#1554\\)\")[#1554](https://github.com/NVIDIA/TensorRT-LLM/pull/1554)[)](/NVIDIA/TensorRT-LLM/commit/89ba1b1a67d570e41b03da87e5518eaff0d31fbf \"Update TensorRT-LLM \\(#1554\\)\")| May 7, 2024  \n[.gitignore](/NVIDIA/TensorRT-LLM/blob/main/.gitignore \".gitignore\")| [.gitignore](/NVIDIA/TensorRT-LLM/blob/main/.gitignore \".gitignore\")| [Update TensorRT-LLM (](/NVIDIA/TensorRT-LLM/commit/548b5b73106aaf7374955e1c37aad677678ebc7b \"Update TensorRT-LLM \\(#2532\\)\n* blossom-ci.yml: run vulnerability scan on blossom\n* open source efb18c1256f8c9c3d47b7d0c740b83e5d5ebe0ec\n---------\nCo-authored-by: niukuo <6831097+niukuo@users.noreply.github.com>\nCo-authored-by: pei0033 <59505847+pei0033@users.noreply.github.com>\nCo-authored-by: Kyungmin Lee <30465912+lkm2835@users.noreply.github.com>\nCo-authored-by: Kaiyu Xie <26294424+kaiyux@users.noreply.github.com>\")[#2532](https://github.com/NVIDIA/TensorRT-LLM/pull/2532)[)](/NVIDIA/TensorRT-LLM/commit/548b5b73106aaf7374955e1c37aad677678ebc7b \"Update TensorRT-LLM \\(#2532\\)\n* blossom-ci.yml: run vulnerability scan on blossom\n* open source efb18c1256f8c9c3d47b7d0c740b83e5d5ebe0ec\n---------\nCo-authored-by: niukuo <6831097+niukuo@users.noreply.github.com>\nCo-authored-by: pei0033 <59505847+pei0033@users.noreply.github.com>\nCo-authored-by: Kyungmin Lee <30465912+lkm2835@users.noreply.github.com>\nCo-authored-by: Kaiyu Xie <26294424+kaiyux@users.noreply.github.com>\")| Dec 4, 2024  \n[.gitmodules](/NVIDIA/TensorRT-LLM/blob/main/.gitmodules \".gitmodules\")| [.gitmodules](/NVIDIA/TensorRT-LLM/blob/main/.gitmodules \".gitmodules\")| [Update TensorRT-LLM (](/NVIDIA/TensorRT-LLM/commit/548b5b73106aaf7374955e1c37aad677678ebc7b \"Update TensorRT-LLM \\(#2532\\)\n* blossom-ci.yml: run vulnerability scan on blossom\n* open source efb18c1256f8c9c3d47b7d0c740b83e5d5ebe0ec\n---------\nCo-authored-by: niukuo <6831097+niukuo@users.noreply.github.com>\nCo-authored-by: pei0033 <59505847+pei0033@users.noreply.github.com>\nCo-authored-by: Kyungmin Lee <30465912+lkm2835@users.noreply.github.com>\nCo-authored-by: Kaiyu Xie <26294424+kaiyux@users.noreply.github.com>\")[#2532](https://github.com/NVIDIA/TensorRT-LLM/pull/2532)[)](/NVIDIA/TensorRT-LLM/commit/548b5b73106aaf7374955e1c37aad677678ebc7b \"Update TensorRT-LLM \\(#2532\\)\n* blossom-ci.yml: run vulnerability scan on blossom\n* open source efb18c1256f8c9c3d47b7d0c740b83e5d5ebe0ec\n---------\nCo-authored-by: niukuo <6831097+niukuo@users.noreply.github.com>\nCo-authored-by: pei0033 <59505847+pei0033@users.noreply.github.com>\nCo-authored-by: Kyungmin Lee <30465912+lkm2835@users.noreply.github.com>\nCo-authored-by: Kaiyu Xie <26294424+kaiyux@users.noreply.github.com>\")| Dec 4, 2024  \n[.pre-commit-config.yaml](/NVIDIA/TensorRT-LLM/blob/main/.pre-commit-config.yaml \".pre-commit-config.yaml\")| [.pre-commit-config.yaml](/NVIDIA/TensorRT-LLM/blob/main/.pre-commit-config.yaml \".pre-commit-config.yaml\")| [Update TensorRT-LLM (](/NVIDIA/TensorRT-LLM/commit/be1788106245496872d18e702978e59b6bfd50e0 \"Update TensorRT-LLM \\(#2582\\)\")[#2582](https://github.com/NVIDIA/TensorRT-LLM/pull/2582)[)](/NVIDIA/TensorRT-LLM/commit/be1788106245496872d18e702978e59b6bfd50e0 \"Update TensorRT-LLM \\(#2582\\)\")| Dec 17, 2024  \n[LICENSE](/NVIDIA/TensorRT-LLM/blob/main/LICENSE \"LICENSE\")| [LICENSE](/NVIDIA/TensorRT-LLM/blob/main/LICENSE \"LICENSE\")| [Initial commit](/NVIDIA/TensorRT-LLM/commit/23bc5b7c490576071a021d6fcfc32c5808db36a2 \"Initial commit\")| Sep 20, 2023  \n[README.md](/NVIDIA/TensorRT-LLM/blob/main/README.md \"README.md\")| [README.md](/NVIDIA/TensorRT-LLM/blob/main/README.md \"README.md\")| [Update README.md (](/NVIDIA/TensorRT-LLM/commit/0d0583a639cb120f09ae4af50dd0722bdd60a5df \"Update README.md \\(#2668\\)\")[#2668](https://github.com/NVIDIA/TensorRT-LLM/pull/2668)[)](/NVIDIA/TensorRT-LLM/commit/0d0583a639cb120f09ae4af50dd0722bdd60a5df \"Update README.md \\(#2668\\)\")| Jan 8, 2025  \n[requirements-dev-windows.txt](/NVIDIA/TensorRT-LLM/blob/main/requirements-dev-windows.txt \"requirements-dev-windows.txt\")| [requirements-dev-windows.txt](/NVIDIA/TensorRT-LLM/blob/main/requirements-dev-windows.txt \"requirements-dev-windows.txt\")| [Update TensorRT-LLM (](/NVIDIA/TensorRT-LLM/commit/be1788106245496872d18e702978e59b6bfd50e0 \"Update TensorRT-LLM \\(#2582\\)\")[#2582](https://github.com/NVIDIA/TensorRT-LLM/pull/2582)[)](/NVIDIA/TensorRT-LLM/commit/be1788106245496872d18e702978e59b6bfd50e0 \"Update TensorRT-LLM \\(#2582\\)\")| Dec 17, 2024  \n[requirements-dev.txt](/NVIDIA/TensorRT-LLM/blob/main/requirements-dev.txt \"requirements-dev.txt\")| [requirements-dev.txt](/NVIDIA/TensorRT-LLM/blob/main/requirements-dev.txt \"requirements-dev.txt\")| [Update TensorRT-LLM (](/NVIDIA/TensorRT-LLM/commit/aaacc9bd68b3e839fd25a0c7a80964d9b393ac99 \"Update TensorRT-LLM \\(#2562\\)\n* Update TensorRT-LLM\n---------\nCo-authored-by: Starrick Liu <73152103+StarrickLiu@users.noreply.github.com>\")[#2562](https://github.com/NVIDIA/TensorRT-LLM/pull/2562)[)](/NVIDIA/TensorRT-LLM/commit/aaacc9bd68b3e839fd25a0c7a80964d9b393ac99 \"Update TensorRT-LLM \\(#2562\\)\n* Update TensorRT-LLM\n---------\nCo-authored-by: Starrick Liu <73152103+StarrickLiu@users.noreply.github.com>\")| Dec 11, 2024  \n[requirements-windows.txt](/NVIDIA/TensorRT-LLM/blob/main/requirements-windows.txt \"requirements-windows.txt\")| [requirements-windows.txt](/NVIDIA/TensorRT-LLM/blob/main/requirements-windows.txt \"requirements-windows.txt\")| [Update TensorRT-LLM (](/NVIDIA/TensorRT-LLM/commit/be1788106245496872d18e702978e59b6bfd50e0 \"Update TensorRT-LLM \\(#2582\\)\")[#2582](https://github.com/NVIDIA/TensorRT-LLM/pull/2582)[)](/NVIDIA/TensorRT-LLM/commit/be1788106245496872d18e702978e59b6bfd50e0 \"Update TensorRT-LLM \\(#2582\\)\")| Dec 17, 2024  \n[requirements.txt](/NVIDIA/TensorRT-LLM/blob/main/requirements.txt \"requirements.txt\")| [requirements.txt](/NVIDIA/TensorRT-LLM/blob/main/requirements.txt \"requirements.txt\")| [Update TensorRT-LLM (](/NVIDIA/TensorRT-LLM/commit/aaacc9bd68b3e839fd25a0c7a80964d9b393ac99 \"Update TensorRT-LLM \\(#2562\\)\n* Update TensorRT-LLM\n---------\nCo-authored-by: Starrick Liu <73152103+StarrickLiu@users.noreply.github.com>\")[#2562](https://github.com/NVIDIA/TensorRT-LLM/pull/2562)[)](/NVIDIA/TensorRT-LLM/commit/aaacc9bd68b3e839fd25a0c7a80964d9b393ac99 \"Update TensorRT-LLM \\(#2562\\)\n* Update TensorRT-LLM\n---------\nCo-authored-by: Starrick Liu <73152103+StarrickLiu@users.noreply.github.com>\")| Dec 11, 2024  \n[setup.cfg](/NVIDIA/TensorRT-LLM/blob/main/setup.cfg \"setup.cfg\")| [setup.cfg](/NVIDIA/TensorRT-LLM/blob/main/setup.cfg \"setup.cfg\")| [Initial commit](/NVIDIA/TensorRT-LLM/commit/23bc5b7c490576071a021d6fcfc32c5808db36a2 \"Initial commit\")| Sep 20, 2023  \n[setup.py](/NVIDIA/TensorRT-LLM/blob/main/setup.py \"setup.py\")| [setup.py](/NVIDIA/TensorRT-LLM/blob/main/setup.py \"setup.py\")| [Update TensorRT-LLM (](/NVIDIA/TensorRT-LLM/commit/c629546ce429623c8a163633095230154a6f0574 \"Update TensorRT-LLM \\(#2436\\)\")[#2436](https://github.com/NVIDIA/TensorRT-LLM/pull/2436)[)](/NVIDIA/TensorRT-LLM/commit/c629546ce429623c8a163633095230154a6f0574 \"Update TensorRT-LLM \\(#2436\\)\")| Nov 12, 2024  \nView all files  \n  \n## Repository files navigation\n\n  * [README](#)\n  * [Apache-2.0 license](#)\n\n\n\n# TensorRT-LLM\n\n[](#tensorrt-llm)\n\n####  A TensorRT Toolbox for Optimized Large Language Model Inference\n\n[](#-a-tensorrt-toolbox-for-optimized-large-language-model-inference)\n\n[![Documentation](https://camo.githubusercontent.com/c191e0610a0b83ed5bef19569b59b7f27f7da4b11c720841af9028c9f239bf97/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6c61746573742d627269676874677265656e2e7376673f7374796c653d666c6174)](https://nvidia.github.io/TensorRT-LLM/) [![python](https://camo.githubusercontent.com/1686d8c10049560ea7e86689059cfa94ff2dfaf3a92ef54a22cb2fce0ac016ec/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d332e31322e332d677265656e)](https://www.python.org/downloads/release/python-3123/) [![cuda](https://camo.githubusercontent.com/24ac3b4b60f19b90404f18434b20a18506d73ce5a16e50c8d0b936e77747ed1e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f637564612d31322e362e332d677265656e)](https://developer.nvidia.com/cuda-downloads) [![trt](https://camo.githubusercontent.com/4853fe0291ac2e66c60a7be7b2da06aa4cec5ff6d45b89005c89bfd8f24c485b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5452542d31302e372e302d677265656e)](https://developer.nvidia.com/tensorrt) [![version](https://camo.githubusercontent.com/636a19e4d1d679cc96c567736adcde797294f553e02887e9f8b961b48bcabbca/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f72656c656173652d302e31372e302e6465762d677265656e)](/NVIDIA/TensorRT-LLM/blob/main/tensorrt_llm/version.py) [![license](https://camo.githubusercontent.com/babc55b476ce60b545de3012f13503eea326b5d8d8b9957b2d850c2e3f0cf507/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d417061636865253230322d626c7565)](/NVIDIA/TensorRT-LLM/blob/main/LICENSE)\n\n[Architecture](/NVIDIA/TensorRT-LLM/blob/main/docs/source/architecture/overview.md) | [Performance](/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/perf-overview.md) | [Examples](/NVIDIA/TensorRT-LLM/blob/main/examples) | [Documentation](/NVIDIA/TensorRT-LLM/blob/main/docs/source) | [Roadmap](https://docs.google.com/presentation/d/1gycPmtdh7uUcH6laOvW65Dbp9F1McUkGDIcAyjicBZs/edit?usp=sharing)\n\n## Latest News\n\n[](#latest-news)\n\n  * [2025/01/07] 🌟 Getting Started with TensorRT-LLM [➡️ link](https://www.youtube.com/watch?v=TwWqPnuNHV8)\n\n  * [2025/01/04] ⚡Boost Llama 3.3 70B Inference Throughput 3x with NVIDIA TensorRT-LLM Speculative Decoding [➡️ link](https://developer.nvidia.com/blog/boost-llama-3-3-70b-inference-throughput-3x-with-nvidia-tensorrt-llm-speculative-decoding/)\n\n\n\n\n[![](https://camo.githubusercontent.com/3b3df5efd7cfeb94df932d4ae96fd4a2f4267a2b0b24cac30df8d5910b2d924f/68747470733a2f2f646576656c6f7065722d626c6f67732e6e76696469612e636f6d2f77702d636f6e74656e742f75706c6f6164732f323032342f31322f74687265652d6c6c616d61732d77656172696e672d676f67676c65732e706e67)](https://camo.githubusercontent.com/3b3df5efd7cfeb94df932d4ae96fd4a2f4267a2b0b24cac30df8d5910b2d924f/68747470733a2f2f646576656c6f7065722d626c6f67732e6e76696469612e636f6d2f77702d636f6e74656e742f75706c6f6164732f323032342f31322f74687265652d6c6c616d61732d77656172696e672d676f67676c65732e706e67)\n\n  * [2024/12/10] ⚡ Llama 3.3 70B from AI at Meta is accelerated by TensorRT-LLM. 🌟 State-of-the-art model on par with Llama 3.1 405B for reasoning, math, instruction following and tool use. Explore the preview [➡️ link](https://build.nvidia.com/meta/llama-3_3-70b-instruct)\n\n  * [2024/12/03] 🌟 Boost your AI inference throughput by up to 3.6x. We now support speculative decoding and tripling token throughput with our NVIDIA TensorRT-LLM. Perfect for your generative AI apps. ⚡Learn how in this technical deep dive [➡️ link](https://nvda.ws/3ZCZTzD)\n\n  * [2024/12/02] Working on deploying ONNX models for performance-critical applications? Try our NVIDIA Nsight Deep Learning Designer ⚡ A user-friendly GUI and tight integration with NVIDIA TensorRT that offers: ✅ Intuitive visualization of ONNX model graphs ✅ Quick tweaking of model architecture and parameters ✅ Detailed performance profiling with either ORT or TensorRT ✅ Easy building of TensorRT engines [➡️ link](https://developer.nvidia.com/nsight-dl-designer?ncid=so-link-485689&linkId=100000315016072)\n\n  * [2024/11/26] 📣 Introducing TensorRT-LLM for Jetson AGX Orin, making it even easier to deploy on Jetson AGX Orin with initial support in JetPack 6.1 via the v0.12.0-jetson branch of the TensorRT-LLM repo. ✅ Pre-compiled TensorRT-LLM wheels & containers for easy integration ✅ Comprehensive guides & docs to get you started [➡️ link](https://forums.developer.nvidia.com/t/tensorrt-llm-for-jetson/313227?linkId=100000312718869)\n\n  * [2024/11/21] NVIDIA TensorRT-LLM Multiblock Attention Boosts Throughput by More Than 3x for Long Sequence Lengths on NVIDIA HGX H200 [➡️ link](https://developer.nvidia.com/blog/nvidia-tensorrt-llm-multiblock-attention-boosts-throughput-by-more-than-3x-for-long-sequence-lengths-on-nvidia-hgx-h200/)\n\n  * [2024/11/19] Llama 3.2 Full-Stack Optimizations Unlock High Performance on NVIDIA GPUs [➡️ link](https://developer.nvidia.com/blog/llama-3-2-full-stack-optimizations-unlock-high-performance-on-nvidia-gpus/?ncid=so-link-721194)\n\n  * [2024/11/09] 🚀🚀🚀 3x Faster AllReduce with NVSwitch and TensorRT-LLM MultiShot [➡️ link](https://developer.nvidia.com/blog/3x-faster-allreduce-with-nvswitch-and-tensorrt-llm-multishot/)\n\n  * [2024/11/09] ✨ NVIDIA advances the AI ecosystem with the AI model of LG AI Research 🙌 [➡️ link](https://blogs.nvidia.co.kr/blog/nvidia-lg-ai-research/)\n\n  * [2024/11/02] 🌟🌟🌟 NVIDIA and LlamaIndex Developer Contest 🙌 Enter for a chance to win prizes including an NVIDIA® GeForce RTX™ 4080 SUPER GPU, DLI credits, and more🙌 [➡️ link](https://developer.nvidia.com/llamaindex-developer-contest)\n\n\nPrevious News\n\n  * [2024/10/28] 🏎️🏎️🏎️ NVIDIA GH200 Superchip Accelerates Inference by 2x in Multiturn Interactions with Llama Models [➡️ link](https://developer.nvidia.com/blog/nvidia-gh200-superchip-accelerates-inference-by-2x-in-multiturn-interactions-with-llama-models/)\n\n  * [2024/10/22] New 📝 Step-by-step instructions on how to ✅ Optimize LLMs with NVIDIA TensorRT-LLM, ✅ Deploy the optimized models with Triton Inference Server, ✅ Autoscale LLMs deployment in a Kubernetes environment. 🙌 Technical Deep Dive: [➡️ link](https://nvda.ws/3YgI8UT)\n\n  * [2024/10/07] 🚀🚀🚀Optimizing Microsoft Bing Visual Search with NVIDIA Accelerated Libraries [➡️ link](https://developer.nvidia.com/blog/optimizing-microsoft-bing-visual-search-with-nvidia-accelerated-libraries/)\n\n  * [2024/09/29] 🌟 AI at Meta PyTorch + TensorRT v2.4 🌟 ⚡TensorRT 10.1 ⚡PyTorch 2.4 ⚡CUDA 12.4 ⚡Python 3.12 [➡️ link](https://github.com/pytorch/TensorRT/releases/tag/v2.4.0)\n\n  * [2024/09/17] ✨ NVIDIA TensorRT-LLM Meetup [➡️ link](https://drive.google.com/file/d/1RR8GqC-QbuaKuHj82rZcXb3MS20SWo6F/view?usp=share_link)\n\n  * [2024/09/17] ✨ Accelerating LLM Inference at Databricks with TensorRT-LLM [➡️ link](https://drive.google.com/file/d/1NeSmrLaWRJAY1rxD9lJmzpB9rzr38j8j/view?usp=sharing)\n\n  * [2024/09/17] ✨ TensorRT-LLM @ Baseten [➡️ link](https://drive.google.com/file/d/1Y7L2jqW-aRmt31mCdqhwvGMmCSOzBUjG/view?usp=share_link)\n\n  * [2024/09/04] 🏎️🏎️🏎️ Best Practices for Tuning TensorRT-LLM for Optimal Serving with BentoML [➡️ link](https://www.bentoml.com/blog/tuning-tensor-rt-llm-for-optimal-serving-with-bentoml)\n\n  * [2024/08/20] 🏎️SDXL with #TensorRT Model Optimizer ⏱️⚡ 🏁 cache diffusion 🏁 quantization aware training 🏁 QLoRA 🏁 #Python 3.12 [➡️ link](https://developer.nvidia.com/blog/nvidia-tensorrt-model-optimizer-v0-15-boosts-inference-performance-and-expands-model-support/)\n\n  * [2024/08/13] 🐍 DIY Code Completion with #Mamba ⚡ #TensorRT #LLM for speed 🤖 NIM for ease ☁️ deploy anywhere [➡️ link](https://developer.nvidia.com/blog/revolutionizing-code-completion-with-codestral-mamba-the-next-gen-coding-llm/)\n\n  * [2024/08/06] 🗫 Multilingual Challenge Accepted 🗫 🤖 #TensorRT #LLM boosts low-resource languages like Hebrew, Indonesian and Vietnamese ⚡[➡️ link](https://developer.nvidia.com/blog/accelerating-hebrew-llm-performance-with-nvidia-tensorrt-llm/?linkId=100000278659647)\n\n  * [2024/07/30] Introducing🍊 @SliceXAI ELM Turbo 🤖 train ELM once ⚡ #TensorRT #LLM optimize ☁️ deploy anywhere [➡️ link](https://developer.nvidia.com/blog/supercharging-llama-3-1-across-nvidia-platforms)\n\n  * [2024/07/23] 👀 @AIatMeta Llama 3.1 405B trained on 16K NVIDIA H100s - inference is #TensorRT #LLM optimized ⚡ 🦙 400 tok/s - per node 🦙 37 tok/s - per user 🦙 1 node inference [➡️ link](https://developer.nvidia.com/blog/supercharging-llama-3-1-across-nvidia-platforms)\n\n  * [2024/07/09] Checklist to maximize multi-language performance of @meta #Llama3 with #TensorRT #LLM inference: ✅ MultiLingual ✅ NIM ✅ LoRA tuned adaptors[➡️ Tech blog](https://developer.nvidia.com/blog/deploy-multilingual-llms-with-nvidia-nim/)\n\n  * [2024/07/02] Let the @MistralAI MoE tokens fly 📈 🚀 #Mixtral 8x7B with NVIDIA #TensorRT #LLM on #H100. [➡️ Tech blog](https://developer.nvidia.com/blog/achieving-high-mixtral-8x7b-performance-with-nvidia-h100-tensor-core-gpus-and-tensorrt-llm?ncid=so-twit-928467)\n\n  * [2024/06/24] Enhanced with NVIDIA #TensorRT #LLM, @upstage.ai’s solar-10.7B-instruct is ready to power your developer projects through our API catalog 🏎️. ✨[➡️ link](https://build.nvidia.com/upstage/solar-10_7b-instruct?snippet_tab=Try)\n\n  * [2024/06/18] CYMI: 🤩 Stable Diffusion 3 dropped last week 🎊 🏎️ Speed up your SD3 with #TensorRT INT8 Quantization[➡️ link](https://build.nvidia.com/upstage/solar-10_7b-instruct?snippet_tab=Try)\n\n  * [2024/06/18] 🧰Deploying ComfyUI with TensorRT? Here’s your setup guide [➡️ link](https://github.com/comfyanonymous/ComfyUI_TensorRT)\n\n  * [2024/06/11] ✨#TensorRT Weight-Stripped Engines ✨ Technical Deep Dive for serious coders ✅+99% compression ✅1 set of weights → ** GPUs ✅0 performance loss ✅** models…LLM, CNN, etc.[➡️ link](https://developer.nvidia.com/blog/maximum-performance-and-minimum-footprint-for-ai-apps-with-nvidia-tensorrt-weight-stripped-engines/)\n\n  * [2024/06/04] ✨ #TensorRT and GeForce #RTX unlock ComfyUI SD superhero powers 🦸⚡ 🎥 Demo: [➡️ link](https://youtu.be/64QEVfbPHyg) 📗 DIY notebook: [➡️ link](https://console.brev.dev/launchable/deploy?userID=2x2sil999&orgID=ktj33l4xj&name=ComfyUI_TensorRT&instance=L4%40g2-standard-4%3Anvidia-l4%3A1&diskStorage=500&cloudID=GCP&baseImage=docker.io%2Fpytorch%2Fpytorch%3A2.2.0-cuda12.1-cudnn8-runtime&ports=ComfUI%3A8188&file=https%3A%2F%2Fgithub.com%2Fbrevdev%2Fnotebooks%2Fblob%2Fmain%2Ftensorrt-comfyui.ipynb&launchableID=env-2hQX3n7ae5mq3NjNZ32DfAG0tJf)\n\n  * [2024/05/28] ✨#TensorRT weight stripping for ResNet-50 ✨ ✅+99% compression ✅1 set of weights → ** GPUs\\ ✅0 performance loss ✅** models…LLM, CNN, etc 👀 📚 DIY [➡️ link](https://console.brev.dev/launchable/deploy?userID=2x2sil999&orgID=ktj33l4xj&launchableID=env-2h6bym7h5GFNho3vpWQQeUYMwTM&instance=L4%40g6.xlarge&diskStorage=500&cloudID=devplane-brev-1&baseImage=nvcr.io%2Fnvidia%2Ftensorrt%3A24.05-py3&file=https%3A%2F%2Fgithub.com%2FNVIDIA%2FTensorRT%2Fblob%2Frelease%2F10.0%2Fsamples%2Fpython%2Fsample_weight_stripping%2Fnotebooks%2Fweight_stripping.ipynb&name=tensorrt_weight_stripping_resnet50)\n\n  * [2024/05/21] ✨@modal_labs has the codes for serverless @AIatMeta Llama 3 on #TensorRT #LLM ✨👀 📚 Marvelous Modal Manual: Serverless TensorRT-LLM (LLaMA 3 8B) | Modal Docs [➡️ link](https://modal.com/docs/examples/trtllm_llama)\n\n  * [2024/05/08] NVIDIA TensorRT Model Optimizer -- the newest member of the #TensorRT ecosystem is a library of post-training and training-in-the-loop model optimization techniques ✅quantization ✅sparsity ✅QAT [➡️ blog](https://developer.nvidia.com/blog/accelerate-generative-ai-inference-performance-with-nvidia-tensorrt-model-optimizer-now-publicly-available/)\n\n  * [2024/05/07] 🦙🦙🦙 24,000 tokens per second 🛫Meta Llama 3 takes off with #TensorRT #LLM 📚[➡️ link](https://blogs.nvidia.com/blog/meta-llama3-inference-acceleration/)\n\n  * [2024/02/06] [🚀 Speed up inference with SOTA quantization techniques in TRT-LLM](/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/quantization-in-TRT-LLM.md)\n\n  * [2024/01/30] [ New XQA-kernel provides 2.4x more Llama-70B throughput within the same latency budget](/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/XQA-kernel.md)\n\n  * [2023/12/04] [Falcon-180B on a single H200 GPU with INT4 AWQ, and 6.7x faster Llama-70B over A100](/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/Falcon180B-H200.md)\n\n  * [2023/11/27] [SageMaker LMI now supports TensorRT-LLM - improves throughput by 60%, compared to previous version](https://aws.amazon.com/blogs/machine-learning/boost-inference-performance-for-llms-with-new-amazon-sagemaker-containers/)\n\n  * [2023/11/13] [H200 achieves nearly 12,000 tok/sec on Llama2-13B](/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/H200launch.md)\n\n  * [2023/10/22] [🚀 RAG on Windows using TensorRT-LLM and LlamaIndex 🦙](https://github.com/NVIDIA/trt-llm-rag-windows#readme)\n\n  * [2023/10/19] Getting Started Guide - [Optimizing Inference on Large Language Models with NVIDIA TensorRT-LLM, Now Publicly Available ](https://developer.nvidia.com/blog/optimizing-inference-on-llms-with-tensorrt-llm-now-publicly-available/)\n\n  * [2023/10/17] [Large Language Models up to 4x Faster on RTX With TensorRT-LLM for Windows ](https://blogs.nvidia.com/blog/2023/10/17/tensorrt-llm-windows-stable-diffusion-rtx/)\n\n\n\n\n## TensorRT-LLM Overview\n\n[](#tensorrt-llm-overview)\n\nTensorRT-LLM is a library for optimizing Large Language Model (LLM) inference. It provides state-of-the-art optimizations, including custom attention kernels, inflight batching, paged KV caching, quantization (FP8, INT4 [AWQ](https://arxiv.org/abs/2306.00978), INT8 [SmoothQuant](https://arxiv.org/abs/2211.10438), ++) and much more, to perform inference efficiently on NVIDIA GPUs\n\nTensorRT-LLM provides a Python API to build LLMs into optimized [TensorRT](https://developer.nvidia.com/tensorrt) engines. It contains runtimes in Python (bindings) and C++ to execute those TensorRT engines. It also includes a [backend](https://github.com/triton-inference-server/tensorrtllm_backend) for integration with the [NVIDIA Triton Inference Server](https://developer.nvidia.com/nvidia-triton-inference-server). Models built with TensorRT-LLM can be executed on a wide range of configurations from a single GPU to multiple nodes with multiple GPUs (using [Tensor Parallelism](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/features/parallelisms.html#tensor-parallelism) and/or [Pipeline Parallelism](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/features/parallelisms.html#pipeline-parallelism)).\n\nTensorRT-LLM comes with several popular models pre-defined. They can easily be modified and extended to fit custom needs via a PyTorch-like Python API. Refer to the [Support Matrix](https://nvidia.github.io/TensorRT-LLM/reference/support-matrix.html) for a list of supported models.\n\nTensorRT-LLM is built on top of the [TensorRT](https://developer.nvidia.com/tensorrt) Deep Learning Inference library. It leverages much of TensorRT's deep learning optimizations and adds LLM-specific optimizations on top, as described above. TensorRT is an ahead-of-time compiler; it builds \"[Engines](https://docs.nvidia.com/deeplearning/tensorrt/quick-start-guide/index.html#ecosystem)\" which are optimized representations of the compiled model containing the entire execution graph. These engines are optimized for a specific GPU architecture, and can be validated, benchmarked, and serialized for later deployment in a production environment.\n\n## Getting Started\n\n[](#getting-started)\n\nTo get started with TensorRT-LLM, visit our documentation:\n\n  * [Quick Start Guide](https://nvidia.github.io/TensorRT-LLM/quick-start-guide.html)\n  * [Release Notes](https://nvidia.github.io/TensorRT-LLM/release-notes.html)\n  * [Installation Guide for Linux](https://nvidia.github.io/TensorRT-LLM/installation/linux.html)\n  * [Installation Guide for Windows](https://nvidia.github.io/TensorRT-LLM/installation/windows.html)\n  * [Installation Guide for Grace Hopper](https://nvidia.github.io/TensorRT-LLM/installation/grace-hopper.html)\n  * [Supported Hardware, Models, and other Software](https://nvidia.github.io/TensorRT-LLM/reference/support-matrix.html)\n\n\n\n## Community\n\n[](#community)\n\n  * [Model zoo](https://huggingface.co/TheFloat16) (generated by TRT-LLM rel 0.9 a9356d4b7610330e89c1010f342a9ac644215c52)\n\n\n\n## About\n\nTensorRT-LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and build TensorRT engines that contain state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. TensorRT-LLM also contains components to create Python and C++ runtimes that execute those TensorRT engines. \n\n[nvidia.github.io/TensorRT-LLM](https://nvidia.github.io/TensorRT-LLM \"https://nvidia.github.io/TensorRT-LLM\")\n\n### Resources\n\n[ Readme ](#readme-ov-file)\n\n### License\n\n[ Apache-2.0 license ](#Apache-2.0-1-ov-file)\n\n[ Activity](/NVIDIA/TensorRT-LLM/activity)\n\n[ Custom properties](/NVIDIA/TensorRT-LLM/custom-properties)\n\n### Stars\n\n[ **9.2k** stars](/NVIDIA/TensorRT-LLM/stargazers)\n\n### Watchers\n\n[ **98** watching](/NVIDIA/TensorRT-LLM/watchers)\n\n### Forks\n\n[ **1.1k** forks](/NVIDIA/TensorRT-LLM/forks)\n\n[ Report repository ](/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2FNVIDIA%2FTensorRT-LLM&report=NVIDIA+%28user%29)\n\n##  [Releases 12](/NVIDIA/TensorRT-LLM/releases)\n\n[ TensorRT-LLM Release 0.16.0 Latest  Dec 24, 2024 ](/NVIDIA/TensorRT-LLM/releases/tag/v0.16.0)\n\n[+ 11 releases](/NVIDIA/TensorRT-LLM/releases)\n\n##  [Packages 0](/orgs/NVIDIA/packages?repo_name=TensorRT-LLM)\n\nNo packages published \n\n##  [Contributors 78](/NVIDIA/TensorRT-LLM/graphs/contributors)\n\n  * [ ![@kaiyux](https://avatars.githubusercontent.com/u/26294424?s=64&v=4) ](https://github.com/kaiyux)\n  * [ ![@Shixiaowei02](https://avatars.githubusercontent.com/u/39303645?s=64&v=4) ](https://github.com/Shixiaowei02)\n  * [ ![@juney-nvidia](https://avatars.githubusercontent.com/u/143764042?s=64&v=4) ](https://github.com/juney-nvidia)\n  * [ ![@nv-guomingz](https://avatars.githubusercontent.com/u/137257613?s=64&v=4) ](https://github.com/nv-guomingz)\n  * [ ![@niukuo](https://avatars.githubusercontent.com/u/6831097?s=64&v=4) ](https://github.com/niukuo)\n  * [ ![@lkm2835](https://avatars.githubusercontent.com/u/30465912?s=64&v=4) ](https://github.com/lkm2835)\n  * [ ![@fjosw](https://avatars.githubusercontent.com/u/22637881?s=64&v=4) ](https://github.com/fjosw)\n  * [ ![@wangkuiyi](https://avatars.githubusercontent.com/u/1548775?s=64&v=4) ](https://github.com/wangkuiyi)\n  * [ ![@mfuntowicz](https://avatars.githubusercontent.com/u/2241520?s=64&v=4) ](https://github.com/mfuntowicz)\n  * [ ![@megha95](https://avatars.githubusercontent.com/u/16129366?s=64&v=4) ](https://github.com/megha95)\n  * [ ![@Pzzzzz5142](https://avatars.githubusercontent.com/u/31173671?s=64&v=4) ](https://github.com/Pzzzzz5142)\n  * [ ![@Eddie-Wang1120](https://avatars.githubusercontent.com/u/81598289?s=64&v=4) ](https://github.com/Eddie-Wang1120)\n  * [ ![@pathorn](https://avatars.githubusercontent.com/u/47250?s=64&v=4) ](https://github.com/pathorn)\n\n\n\n[+ 64 contributors](/NVIDIA/TensorRT-LLM/graphs/contributors)\n\n## Languages\n\n  * [ C++ 99.1% ](/NVIDIA/TensorRT-LLM/search?l=c%2B%2B)\n  * [ Python 0.7% ](/NVIDIA/TensorRT-LLM/search?l=python)\n  * [ Cuda 0.2% ](/NVIDIA/TensorRT-LLM/search?l=cuda)\n  * [ CMake 0.0% ](/NVIDIA/TensorRT-LLM/search?l=cmake)\n  * [ Shell 0.0% ](/NVIDIA/TensorRT-LLM/search?l=shell)\n  * [ Smarty 0.0% ](/NVIDIA/TensorRT-LLM/search?l=smarty)\n\n\n\n## Footer\n\n[ ](https://github.com \"GitHub\") © 2025 GitHub, Inc. \n\n### Footer navigation\n\n  * [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)\n  * [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)\n  * [Security](https://github.com/security)\n  * [Status](https://www.githubstatus.com/)\n  * [Docs](https://docs.github.com/)\n  * [Contact](https://support.github.com?tags=dotcom-footer)\n  * Manage cookies \n  * Do not share my personal information \n\n\n\nYou can’t perform that action at this time. \n",
    "content_quality_score": 0.1,
    "summary": null,
    "child_urls": [
        "https://github.com/NVIDIA/TensorRT-LLM/#start-of-content",
        "https://github.com/",
        "https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2FNVIDIA%2FTensorRT-LLM%2F",
        "https://github.com/features/copilot",
        "https://github.com/features/security",
        "https://github.com/features/actions",
        "https://github.com/features/codespaces",
        "https://github.com/features/issues",
        "https://github.com/features/code-review",
        "https://github.com/features/discussions",
        "https://github.com/features/code-search",
        "https://github.com/features",
        "https://docs.github.com",
        "https://skills.github.com",
        "https://github.com/enterprise",
        "https://github.com/team",
        "https://github.com/enterprise/startups",
        "https://github.com/solutions/industry/nonprofits",
        "https://github.com/solutions/use-case/devsecops",
        "https://github.com/solutions/use-case/devops",
        "https://github.com/solutions/use-case/ci-cd",
        "https://github.com/solutions/use-case",
        "https://github.com/solutions/industry/healthcare",
        "https://github.com/solutions/industry/financial-services",
        "https://github.com/solutions/industry/manufacturing",
        "https://github.com/solutions/industry/government",
        "https://github.com/solutions/industry",
        "https://github.com/solutions",
        "https://github.com/resources/articles/ai",
        "https://github.com/resources/articles/devops",
        "https://github.com/resources/articles/security",
        "https://github.com/resources/articles/software-development",
        "https://github.com/resources/articles",
        "https://resources.github.com/learn/pathways",
        "https://resources.github.com",
        "https://github.com/customer-stories",
        "https://partner.github.com",
        "https://github.com/solutions/executive-insights",
        "https://github.com/sponsors",
        "https://github.com/readme",
        "https://github.com/topics",
        "https://github.com/trending",
        "https://github.com/collections",
        "https://github.com/enterprise/advanced-security",
        "https://github.com/features/copilot#enterprise",
        "https://github.com/premium-support",
        "https://github.com/pricing",
        "https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax",
        "https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=NVIDIA%2FTensorRT-LLM",
        "https://github.com/NVIDIA",
        "https://github.com/NVIDIA/TensorRT-LLM",
        "https://github.com/login?return_to=%2FNVIDIA%2FTensorRT-LLM",
        "https://github.com/NVIDIA/TensorRT-LLM/blob/main/LICENSE",
        "https://github.com/NVIDIA/TensorRT-LLM/stargazers",
        "https://github.com/NVIDIA/TensorRT-LLM/forks",
        "https://github.com/NVIDIA/TensorRT-LLM/branches",
        "https://github.com/NVIDIA/TensorRT-LLM/tags",
        "https://github.com/NVIDIA/TensorRT-LLM/activity",
        "https://github.com/NVIDIA/TensorRT-LLM/issues",
        "https://github.com/NVIDIA/TensorRT-LLM/pulls",
        "https://github.com/NVIDIA/TensorRT-LLM/discussions",
        "https://github.com/NVIDIA/TensorRT-LLM/actions",
        "https://github.com/NVIDIA/TensorRT-LLM/projects",
        "https://github.com/NVIDIA/TensorRT-LLM/security",
        "https://github.com/NVIDIA/TensorRT-LLM/pulse",
        "https://github.com/NVIDIA/TensorRT-LLM/commits/main/",
        "https://github.com/NVIDIA/TensorRT-LLM/tree/main/.github",
        "https://github.com/NVIDIA/TensorRT-LLM/commit/be1788106245496872d18e702978e59b6bfd50e0",
        "https://github.com/NVIDIA/TensorRT-LLM/pull/2582",
        "https://github.com/NVIDIA/TensorRT-LLM/tree/main/3rdparty",
        "https://github.com/NVIDIA/TensorRT-LLM/commit/548b5b73106aaf7374955e1c37aad677678ebc7b",
        "https://github.com/NVIDIA/TensorRT-LLM/pull/2532",
        "https://github.com/NVIDIA/TensorRT-LLM/tree/main/benchmarks",
        "https://github.com/NVIDIA/TensorRT-LLM/tree/main/cpp",
        "https://github.com/NVIDIA/TensorRT-LLM/tree/main/docker",
        "https://github.com/NVIDIA/TensorRT-LLM/commit/aaacc9bd68b3e839fd25a0c7a80964d9b393ac99",
        "https://github.com/NVIDIA/TensorRT-LLM/pull/2562",
        "https://github.com/NVIDIA/TensorRT-LLM/tree/main/docs",
        "https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples",
        "https://github.com/NVIDIA/TensorRT-LLM/tree/main/scripts",
        "https://github.com/NVIDIA/TensorRT-LLM/tree/main/tensorrt_llm",
        "https://github.com/NVIDIA/TensorRT-LLM/commit/d93a2dde84eada06ae2339b4fb4e6432167a1cfd",
        "https://github.com/NVIDIA/TensorRT-LLM/pull/2691",
        "https://github.com/NVIDIA/TensorRT-LLM/tree/main/tests",
        "https://github.com/NVIDIA/TensorRT-LLM/tree/main/windows",
        "https://github.com/NVIDIA/TensorRT-LLM/blob/main/.clang-format",
        "https://github.com/NVIDIA/TensorRT-LLM/commit/4bb65f216f2f1dbddb022f4fdd8925c2856baa58",
        "https://github.com/NVIDIA/TensorRT-LLM/pull/1274",
        "https://github.com/NVIDIA/TensorRT-LLM/blob/main/.dockerignore",
        "https://github.com/NVIDIA/TensorRT-LLM/commit/b57221b764bc579cbb2490154916a871f620e2c4",
        "https://github.com/NVIDIA/TensorRT-LLM/pull/941",
        "https://github.com/NVIDIA/TensorRT-LLM/blob/main/.gitattributes",
        "https://github.com/NVIDIA/TensorRT-LLM/commit/89ba1b1a67d570e41b03da87e5518eaff0d31fbf",
        "https://github.com/NVIDIA/TensorRT-LLM/pull/1554",
        "https://github.com/NVIDIA/TensorRT-LLM/blob/main/.gitignore",
        "https://github.com/NVIDIA/TensorRT-LLM/blob/main/.gitmodules",
        "https://github.com/NVIDIA/TensorRT-LLM/blob/main/.pre-commit-config.yaml",
        "https://github.com/NVIDIA/TensorRT-LLM/commit/23bc5b7c490576071a021d6fcfc32c5808db36a2",
        "https://github.com/NVIDIA/TensorRT-LLM/blob/main/README.md",
        "https://github.com/NVIDIA/TensorRT-LLM/commit/0d0583a639cb120f09ae4af50dd0722bdd60a5df",
        "https://github.com/NVIDIA/TensorRT-LLM/pull/2668",
        "https://github.com/NVIDIA/TensorRT-LLM/blob/main/requirements-dev-windows.txt",
        "https://github.com/NVIDIA/TensorRT-LLM/blob/main/requirements-dev.txt",
        "https://github.com/NVIDIA/TensorRT-LLM/blob/main/requirements-windows.txt",
        "https://github.com/NVIDIA/TensorRT-LLM/blob/main/requirements.txt",
        "https://github.com/NVIDIA/TensorRT-LLM/blob/main/setup.cfg",
        "https://github.com/NVIDIA/TensorRT-LLM/blob/main/setup.py",
        "https://github.com/NVIDIA/TensorRT-LLM/commit/c629546ce429623c8a163633095230154a6f0574",
        "https://github.com/NVIDIA/TensorRT-LLM/pull/2436",
        "https://github.com/NVIDIA/TensorRT-LLM/",
        "https://github.com/NVIDIA/TensorRT-LLM/#tensorrt-llm",
        "https://github.com/NVIDIA/TensorRT-LLM/#-a-tensorrt-toolbox-for-optimized-large-language-model-inference",
        "https://github.com/NVIDIA/TensorRT-LLM/blob/main/tensorrt_llm/version.py",
        "https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/architecture/overview.md",
        "https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/perf-overview.md",
        "https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples",
        "https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source",
        "https://github.com/NVIDIA/TensorRT-LLM/#latest-news",
        "https://github.com/pytorch/TensorRT/releases/tag/v2.4.0",
        "https://github.com/comfyanonymous/ComfyUI_TensorRT",
        "https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/quantization-in-TRT-LLM.md",
        "https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/XQA-kernel.md",
        "https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/Falcon180B-H200.md",
        "https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/H200launch.md",
        "https://github.com/NVIDIA/trt-llm-rag-windows#readme",
        "https://github.com/NVIDIA/TensorRT-LLM/#tensorrt-llm-overview",
        "https://github.com/triton-inference-server/tensorrtllm_backend",
        "https://github.com/NVIDIA/TensorRT-LLM/#getting-started",
        "https://github.com/NVIDIA/TensorRT-LLM/#community",
        "https://github.com/NVIDIA/TensorRT-LLM/#readme-ov-file",
        "https://github.com/NVIDIA/TensorRT-LLM/#Apache-2.0-1-ov-file",
        "https://github.com/NVIDIA/TensorRT-LLM/custom-properties",
        "https://github.com/NVIDIA/TensorRT-LLM/watchers",
        "https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2FNVIDIA%2FTensorRT-LLM&report=NVIDIA+%28user%29",
        "https://github.com/NVIDIA/TensorRT-LLM/releases",
        "https://github.com/NVIDIA/TensorRT-LLM/releases/tag/v0.16.0",
        "https://github.com/orgs/NVIDIA/packages?repo_name=TensorRT-LLM",
        "https://github.com/NVIDIA/TensorRT-LLM/graphs/contributors",
        "https://github.com/kaiyux",
        "https://github.com/Shixiaowei02",
        "https://github.com/juney-nvidia",
        "https://github.com/nv-guomingz",
        "https://github.com/niukuo",
        "https://github.com/lkm2835",
        "https://github.com/fjosw",
        "https://github.com/wangkuiyi",
        "https://github.com/mfuntowicz",
        "https://github.com/megha95",
        "https://github.com/Pzzzzz5142",
        "https://github.com/Eddie-Wang1120",
        "https://github.com/pathorn",
        "https://github.com/NVIDIA/TensorRT-LLM/search?l=c%2B%2B",
        "https://github.com/NVIDIA/TensorRT-LLM/search?l=python",
        "https://github.com/NVIDIA/TensorRT-LLM/search?l=cuda",
        "https://github.com/NVIDIA/TensorRT-LLM/search?l=cmake",
        "https://github.com/NVIDIA/TensorRT-LLM/search?l=shell",
        "https://github.com/NVIDIA/TensorRT-LLM/search?l=smarty",
        "https://github.com",
        "https://docs.github.com/site-policy/github-terms/github-terms-of-service",
        "https://docs.github.com/site-policy/privacy-policies/github-privacy-statement",
        "https://github.com/security",
        "https://docs.github.com/",
        "https://support.github.com?tags=dotcom-footer",
        "https://github.blog",
        "https://nvidia.github.io/TensorRT-LLM",
        "https://nvidia.github.io/TensorRT-LLM/",
        "https://www.python.org/downloads/release/python-3123/",
        "https://developer.nvidia.com/cuda-downloads",
        "https://developer.nvidia.com/tensorrt",
        "https://docs.google.com/presentation/d/1gycPmtdh7uUcH6laOvW65Dbp9F1McUkGDIcAyjicBZs/edit?usp=sharing",
        "https://www.youtube.com/watch?v=TwWqPnuNHV8",
        "https://developer.nvidia.com/blog/boost-llama-3-3-70b-inference-throughput-3x-with-nvidia-tensorrt-llm-speculative-decoding/",
        "https://camo.githubusercontent.com/3b3df5efd7cfeb94df932d4ae96fd4a2f4267a2b0b24cac30df8d5910b2d924f/68747470733a2f2f646576656c6f7065722d626c6f67732e6e76696469612e636f6d2f77702d636f6e74656e742f75706c6f6164732f323032342f31322f74687265652d6c6c616d61732d77656172696e672d676f67676c65732e706e67",
        "https://build.nvidia.com/meta/llama-3_3-70b-instruct",
        "https://nvda.ws/3ZCZTzD",
        "https://developer.nvidia.com/nsight-dl-designer?ncid=so-link-485689&linkId=100000315016072",
        "https://forums.developer.nvidia.com/t/tensorrt-llm-for-jetson/313227?linkId=100000312718869",
        "https://developer.nvidia.com/blog/nvidia-tensorrt-llm-multiblock-attention-boosts-throughput-by-more-than-3x-for-long-sequence-lengths-on-nvidia-hgx-h200/",
        "https://developer.nvidia.com/blog/llama-3-2-full-stack-optimizations-unlock-high-performance-on-nvidia-gpus/?ncid=so-link-721194",
        "https://developer.nvidia.com/blog/3x-faster-allreduce-with-nvswitch-and-tensorrt-llm-multishot/",
        "https://blogs.nvidia.co.kr/blog/nvidia-lg-ai-research/",
        "https://developer.nvidia.com/llamaindex-developer-contest",
        "https://developer.nvidia.com/blog/nvidia-gh200-superchip-accelerates-inference-by-2x-in-multiturn-interactions-with-llama-models/",
        "https://nvda.ws/3YgI8UT",
        "https://developer.nvidia.com/blog/optimizing-microsoft-bing-visual-search-with-nvidia-accelerated-libraries/",
        "https://drive.google.com/file/d/1RR8GqC-QbuaKuHj82rZcXb3MS20SWo6F/view?usp=share_link",
        "https://drive.google.com/file/d/1NeSmrLaWRJAY1rxD9lJmzpB9rzr38j8j/view?usp=sharing",
        "https://drive.google.com/file/d/1Y7L2jqW-aRmt31mCdqhwvGMmCSOzBUjG/view?usp=share_link",
        "https://www.bentoml.com/blog/tuning-tensor-rt-llm-for-optimal-serving-with-bentoml",
        "https://developer.nvidia.com/blog/nvidia-tensorrt-model-optimizer-v0-15-boosts-inference-performance-and-expands-model-support/",
        "https://developer.nvidia.com/blog/revolutionizing-code-completion-with-codestral-mamba-the-next-gen-coding-llm/",
        "https://developer.nvidia.com/blog/accelerating-hebrew-llm-performance-with-nvidia-tensorrt-llm/?linkId=100000278659647",
        "https://developer.nvidia.com/blog/supercharging-llama-3-1-across-nvidia-platforms",
        "https://developer.nvidia.com/blog/deploy-multilingual-llms-with-nvidia-nim/",
        "https://developer.nvidia.com/blog/achieving-high-mixtral-8x7b-performance-with-nvidia-h100-tensor-core-gpus-and-tensorrt-llm?ncid=so-twit-928467",
        "https://build.nvidia.com/upstage/solar-10_7b-instruct?snippet_tab=Try",
        "https://developer.nvidia.com/blog/maximum-performance-and-minimum-footprint-for-ai-apps-with-nvidia-tensorrt-weight-stripped-engines/",
        "https://youtu.be/64QEVfbPHyg",
        "https://console.brev.dev/launchable/deploy?userID=2x2sil999&orgID=ktj33l4xj&name=ComfyUI_TensorRT&instance=L4%40g2-standard-4%3Anvidia-l4%3A1&diskStorage=500&cloudID=GCP&baseImage=docker.io%2Fpytorch%2Fpytorch%3A2.2.0-cuda12.1-cudnn8-runtime&ports=ComfUI%3A8188&file=https%3A%2F%2Fgithub.com%2Fbrevdev%2Fnotebooks%2Fblob%2Fmain%2Ftensorrt-comfyui.ipynb&launchableID=env-2hQX3n7ae5mq3NjNZ32DfAG0tJf",
        "https://console.brev.dev/launchable/deploy?userID=2x2sil999&orgID=ktj33l4xj&launchableID=env-2h6bym7h5GFNho3vpWQQeUYMwTM&instance=L4%40g6.xlarge&diskStorage=500&cloudID=devplane-brev-1&baseImage=nvcr.io%2Fnvidia%2Ftensorrt%3A24.05-py3&file=https%3A%2F%2Fgithub.com%2FNVIDIA%2FTensorRT%2Fblob%2Frelease%2F10.0%2Fsamples%2Fpython%2Fsample_weight_stripping%2Fnotebooks%2Fweight_stripping.ipynb&name=tensorrt_weight_stripping_resnet50",
        "https://modal.com/docs/examples/trtllm_llama",
        "https://developer.nvidia.com/blog/accelerate-generative-ai-inference-performance-with-nvidia-tensorrt-model-optimizer-now-publicly-available/",
        "https://blogs.nvidia.com/blog/meta-llama3-inference-acceleration/",
        "https://aws.amazon.com/blogs/machine-learning/boost-inference-performance-for-llms-with-new-amazon-sagemaker-containers/",
        "https://developer.nvidia.com/blog/optimizing-inference-on-llms-with-tensorrt-llm-now-publicly-available/",
        "https://blogs.nvidia.com/blog/2023/10/17/tensorrt-llm-windows-stable-diffusion-rtx/",
        "https://arxiv.org/abs/2306.00978",
        "https://arxiv.org/abs/2211.10438",
        "https://developer.nvidia.com/nvidia-triton-inference-server",
        "https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/features/parallelisms.html#tensor-parallelism",
        "https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/features/parallelisms.html#pipeline-parallelism",
        "https://nvidia.github.io/TensorRT-LLM/reference/support-matrix.html",
        "https://docs.nvidia.com/deeplearning/tensorrt/quick-start-guide/index.html#ecosystem",
        "https://nvidia.github.io/TensorRT-LLM/quick-start-guide.html",
        "https://nvidia.github.io/TensorRT-LLM/release-notes.html",
        "https://nvidia.github.io/TensorRT-LLM/installation/linux.html",
        "https://nvidia.github.io/TensorRT-LLM/installation/windows.html",
        "https://nvidia.github.io/TensorRT-LLM/installation/grace-hopper.html",
        "https://huggingface.co/TheFloat16",
        "https://www.githubstatus.com/"
    ]
}
[ Tell 120+K peers about your AI research → Learn more 💡 ![](https://neptune.ai/wp-content/themes/neptune/img/icon-cancel.svg) ](/neurips-2024)

[ ![logo](https://neptune.ai/wp-content/themes/neptune/img/logo-neptune.svg) ](https://neptune.ai "neptune.ai")

  * [Product![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](#)

    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/09/show.svg)Overview](#)
      * [Walkthrough [2 min]](https://neptune.ai/resources/foundation-model-training)
      * [Deployment options](https://neptune.ai/product/deployment-options)
      * [Security](https://security.neptune.ai/)
    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/08/compare-1.svg)Compare](#)
      * [Neptune vs WandB](https://neptune.ai/vs/wandb)
      * [Neptune vs MLflow](https://neptune.ai/vs/mlflow)
      * [Neptune vs TensorBoard](https://neptune.ai/vs/tensorboard)
      * [Other comparisons](https://neptune.ai/vs)

[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/07/bg-5.jpg?fit=768%2C432&ssl=1)Live Neptune projectPlay with a public example project that showcases Neptune's upcoming product release. It offers enhanced scalability and exciting new features. ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://scale.neptune.ai/o/neptune/org/LLM-training-example/runs/compare?viewId=9d0e032a-5a78-4a0e-81d1-98e0a7c81a8f&detailsTab=metadata&dash=charts&type=run&experimentOnly=true&compare=u0MsW4a1PJIUJ75nglpjHa9XUKFfAmcBRbLhNatCHX20)

  * [Solutions![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](#)

    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/08/bulb.svg)By role](#)
      * [AI Researcher](https://neptune.ai/product/ai-researcher)
      * [ML Team Lead](https://neptune.ai/product/ml-team-lead)
      * [ML Platform Engineer](https://neptune.ai/product/ml-platform)
      * [Academia & Kagglers](https://neptune.ai/research)
    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/08/star.svg)By use case](#)
      * [Monitor training](https://neptune.ai/product/monitor-training)
      * [Compare experiments](https://neptune.ai/product/compare-experiments)
      * [Collaborate with a team](https://neptune.ai/product/team-collaboration)
      * [Reports](https://neptune.ai/product/reports)

[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/11/deepsense.ai-logo-e1667753808279.png?fit=75%2C75&ssl=1)Case studyHow deepsense.ai Tracked and Analyzed 120K+ Models Using Neptune ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](/customers/deepsense-ai)[ ![Menu thumbnail](https://neptune.ai/wp-content/uploads/2023/06/Respo_square.svg)Case studyHow ReSpo.Vision Uses Neptune to Easily Track Training Pipelines at Scale ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](/customers/respo-vision)

[See all case studies ![chevron](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg)](/resources?ct=case-study)

  * [Developers![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](#)

    * [Menu Item](#)
      * [Documentation](https://docs.neptune.ai/)
      * [Quickstart](https://docs.neptune.ai/usage/quickstart/)
      * [Integrations](https://docs.neptune.ai/integrations/)
      * [Code examples](https://github.com/neptune-ai/examples)

  * [Resources![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](#)

    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/08/test-tube.svg)Use Neptune](#)
      * [Play with public sandbox](https://scale.neptune.ai/o/neptune/org/LLM-training-example/runs/compare?viewId=9d0e032a-5a78-4a0e-81d1-98e0a7c81a8f&detailsTab=metadata&dash=charts&type=run&experimentOnly=true&compare=u0MsW4a1PJIUJ75nglpjHa9XUKFfAmcBRbLhNatCHX20)
      * [Case studies](/resources?ct=case-study)
      * [Example projects](/resources?ct=example-project)
      * [Video tutorials](/resources?ct=video)
      * [All Neptune resources](/resources)
    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/08/file.svg)Learn AI/ML](#)
      * [Blog](https://neptune.ai/blog)
      * [Experiment Tracking Learning Hub](https://neptune.ai/experiment-tracking-learn-hub)
      * [LLMOps Learning Hub](https://neptune.ai/llmops-learning-hub)
      * [MLOps Learning Hub](https://neptune.ai/mlops-learn-hub)
      * [100 Second Research Playlist](https://www.youtube.com/watch?v=_sKZsx7Iprg&list=PLKePQLVx9tOcAGAKvmRuQ5Y1_k6wqG0cX&pp=iAQB)

[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/12/Building-The-Most-Scalable-Experiment-Tracker-For-Foundation-Models.png?fit=768%2C403&ssl=1)ArticleFrom Research to Production: Building The Most Scalable Experiment Tracker For Foundation ModelsAurimas Griciūnas discusses the journey and challenges behind building the most scalable experiment tracker for foundation model training. ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](/blog/building-the-most-scalable-experiment-tracker-for-foundation-models)[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/11/reasercher-3-levels-challenge.webp?fit=768%2C432&ssl=1)VideoBreaking Down AI Research Across 3 Levels of DifficultyWe challenged AI/ML researchers to explain their work across 3 levels of difficulty: for a young learner, university student, and a fellow researcher.  ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://www.youtube.com/watch?v=zSEv3KBGlJQ&list=PLKePQLVx9tOfqC8ho2g_tQVxRga-XbIWa&index=3&t=9s)

  * [Pricing](https://neptune.ai/pricing)
  * [Enterprise](https://neptune.ai/product/enterprise)
  * [Company![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](#)

    * [Menu Item](#)
      * [About us](https://neptune.ai/about-us)
      * [Customers](https://neptune.ai/customers)
      * [Careers](https://neptune.ai/jobs)
      * [In the news](/about-us/#press)
      * [Security](https://security.neptune.ai/)
      * [Contact us](https://neptune.ai/contact-us)




![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)

What do you want to find? 

Search

![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)

[ Log in  ](https://app.neptune.ai/login) [ Sign up  ](https://app.neptune.ai/register) [ Contact us  ](https://neptune.ai/contact-us)

[![Home](/wp-content/themes/neptune/img/icon-breadcrumbs-home.svg)](https://neptune.ai/) > [Blog](https://neptune.ai/blog) > [LLMOps](https://neptune.ai/blog/category/llmops)

  * [Topics![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](#)

    * [Categories](#)
      * [ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)
      * [MLOps](https://neptune.ai/blog/category/mlops)
      * [LLMOps](https://neptune.ai/blog/category/llmops)
      * [ML Tools](https://neptune.ai/blog/category/machine-learning-tools)
      * [Computer Vision](https://neptune.ai/blog/category/computer-vision)
    * [Categories](#)
      * [Natural Language Processing](https://neptune.ai/blog/category/natural-language-processing)
      * [Reinforcement Learning](https://neptune.ai/blog/category/reinforcement-learning)
      * [Tabular Data](https://neptune.ai/blog/category/tabular-data)
      * [Time Series](https://neptune.ai/blog/category/time-series-forecasting)




[ LLMOps Learning Hub ](https://neptune.ai/llmops-learning-hub)

Search in Blog... 

![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)

![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg) ![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)

Search in Blog... 

![search](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)

  * [Topics![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](#)

    * [Categories](#)
      * [ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)
      * [MLOps](https://neptune.ai/blog/category/mlops)
      * [LLMOps](https://neptune.ai/blog/category/llmops)
      * [ML Tools](https://neptune.ai/blog/category/machine-learning-tools)
      * [Computer Vision](https://neptune.ai/blog/category/computer-vision)
    * [Categories](#)
      * [Natural Language Processing](https://neptune.ai/blog/category/natural-language-processing)
      * [Reinforcement Learning](https://neptune.ai/blog/category/reinforcement-learning)
      * [Tabular Data](https://neptune.ai/blog/category/tabular-data)
      * [Time Series](https://neptune.ai/blog/category/time-series-forecasting)




[ LLMOps Learning Hub ](https://neptune.ai/llmops-learning-hub)

  * [Product![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](#)

    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/09/show.svg)Overview](#)
      * [Walkthrough [2 min]](https://neptune.ai/resources/foundation-model-training)
      * [Deployment options](https://neptune.ai/product/deployment-options)
      * [Security](https://security.neptune.ai/)
    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/08/compare-1.svg)Compare](#)
      * [Neptune vs WandB](https://neptune.ai/vs/wandb)
      * [Neptune vs MLflow](https://neptune.ai/vs/mlflow)
      * [Neptune vs TensorBoard](https://neptune.ai/vs/tensorboard)
      * [Other comparisons](https://neptune.ai/vs)

[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/07/bg-5.jpg?fit=768%2C432&ssl=1)Live Neptune projectPlay with a public example project that showcases Neptune's upcoming product release. It offers enhanced scalability and exciting new features. ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://scale.neptune.ai/o/neptune/org/LLM-training-example/runs/compare?viewId=9d0e032a-5a78-4a0e-81d1-98e0a7c81a8f&detailsTab=metadata&dash=charts&type=run&experimentOnly=true&compare=u0MsW4a1PJIUJ75nglpjHa9XUKFfAmcBRbLhNatCHX20)

  * [Solutions![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](#)

    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/08/bulb.svg)By role](#)
      * [AI Researcher](https://neptune.ai/product/ai-researcher)
      * [ML Team Lead](https://neptune.ai/product/ml-team-lead)
      * [ML Platform Engineer](https://neptune.ai/product/ml-platform)
      * [Academia & Kagglers](https://neptune.ai/research)
    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/08/star.svg)By use case](#)
      * [Monitor training](https://neptune.ai/product/monitor-training)
      * [Compare experiments](https://neptune.ai/product/compare-experiments)
      * [Collaborate with a team](https://neptune.ai/product/team-collaboration)
      * [Reports](https://neptune.ai/product/reports)

[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/11/deepsense.ai-logo-e1667753808279.png?fit=75%2C75&ssl=1)Case studyHow deepsense.ai Tracked and Analyzed 120K+ Models Using Neptune ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](/customers/deepsense-ai)[ ![Menu thumbnail](https://neptune.ai/wp-content/uploads/2023/06/Respo_square.svg)Case studyHow ReSpo.Vision Uses Neptune to Easily Track Training Pipelines at Scale ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](/customers/respo-vision)

[See all case studies ![chevron](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg)](/resources?ct=case-study)

  * [Developers![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](#)

    * [Menu Item](#)
      * [Documentation](https://docs.neptune.ai/)
      * [Quickstart](https://docs.neptune.ai/usage/quickstart/)
      * [Integrations](https://docs.neptune.ai/integrations/)
      * [Code examples](https://github.com/neptune-ai/examples)

  * [Resources![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](#)

    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/08/test-tube.svg)Use Neptune](#)
      * [Play with public sandbox](https://scale.neptune.ai/o/neptune/org/LLM-training-example/runs/compare?viewId=9d0e032a-5a78-4a0e-81d1-98e0a7c81a8f&detailsTab=metadata&dash=charts&type=run&experimentOnly=true&compare=u0MsW4a1PJIUJ75nglpjHa9XUKFfAmcBRbLhNatCHX20)
      * [Case studies](/resources?ct=case-study)
      * [Example projects](/resources?ct=example-project)
      * [Video tutorials](/resources?ct=video)
      * [All Neptune resources](/resources)
    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/08/file.svg)Learn AI/ML](#)
      * [Blog](https://neptune.ai/blog)
      * [Experiment Tracking Learning Hub](https://neptune.ai/experiment-tracking-learn-hub)
      * [LLMOps Learning Hub](https://neptune.ai/llmops-learning-hub)
      * [MLOps Learning Hub](https://neptune.ai/mlops-learn-hub)
      * [100 Second Research Playlist](https://www.youtube.com/watch?v=_sKZsx7Iprg&list=PLKePQLVx9tOcAGAKvmRuQ5Y1_k6wqG0cX&pp=iAQB)

[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/12/Building-The-Most-Scalable-Experiment-Tracker-For-Foundation-Models.png?fit=768%2C403&ssl=1)ArticleFrom Research to Production: Building The Most Scalable Experiment Tracker For Foundation ModelsAurimas Griciūnas discusses the journey and challenges behind building the most scalable experiment tracker for foundation model training. ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](/blog/building-the-most-scalable-experiment-tracker-for-foundation-models)[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/11/reasercher-3-levels-challenge.webp?fit=768%2C432&ssl=1)VideoBreaking Down AI Research Across 3 Levels of DifficultyWe challenged AI/ML researchers to explain their work across 3 levels of difficulty: for a young learner, university student, and a fellow researcher.  ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://www.youtube.com/watch?v=zSEv3KBGlJQ&list=PLKePQLVx9tOfqC8ho2g_tQVxRga-XbIWa&index=3&t=9s)

  * [Pricing](https://neptune.ai/pricing)
  * [Enterprise](https://neptune.ai/product/enterprise)
  * [Company![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](#)

    * [Menu Item](#)
      * [About us](https://neptune.ai/about-us)
      * [Customers](https://neptune.ai/customers)
      * [Careers](https://neptune.ai/jobs)
      * [In the news](/about-us/#press)
      * [Security](https://security.neptune.ai/)
      * [Contact us](https://neptune.ai/contact-us)




[ Log in  ](https://app.neptune.ai/login) [ Sign up  ](https://app.neptune.ai/register) [ Contact us  ](https://neptune.ai/contact-us)

[Neptune Blog](/blog)

#  LLM Fine-Tuning and Model Selection Using Neptune and Transformers 

![Author image](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/01/Pedro-Gabriel-Gengo-Lourenco.jpeg?fit=722%2C722&ssl=1)

[ Pedro Gabriel Gengo Lourenço  ](https://neptune.ai/blog/author/pedro-gabriel-gengo-lourenco)

![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-time.svg) 13 min 

![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-date.svg) 13th December, 2024 

[LLMOps](https://neptune.ai/blog/category/llmops)[Natural Language Processing](https://neptune.ai/blog/category/natural-language-processing)

![](data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 3 2'%3E%3C/svg%3E)

Imagine you’re facing the following challenge: you want to develop a Large Language Model (LLM) that can proficiently respond to inquiries in Portuguese. You have a valuable dataset and can choose from various base models. But here’s the catch — you’re working with limited computational resources and can’t rely on expensive, high-power machines for fine-tuning. How do you decide on the right model to use in this scenario?

This post explores these questions, offering insights and strategies for selecting the best model and conducting efficient fine-tuning, even when resources are constrained. We’ll look at ways to reduce a model’s memory footprint, speed up training, and best practices for monitoring.

![LLM fine-tuning and model selection, implemented workflow](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/01/LLM-Fine-Tuning-and-Model-Selection-Using-Neptune-and-Transformers-4.png?resize=1800%2C942&ssl=1) The workflow we’ll implement. We will fine-tune different foundation LLM models on a dataset, evaluate them, and select the best model.

## Large language models

[Large Language Models (LLMs)](https://arxiv.org/pdf/2303.18223.pdf) are huge deep-learning models pre-trained on vast data. These models are usually based on [an architecture called transformers](https://arxiv.org/pdf/1706.03762.pdf). Unlike the earlier recurrent neural networks (RNN) that sequentially process inputs, transformers process entire sequences in parallel. Initially, the transformer architecture was designed for translation tasks. But nowadays, it is used for various tasks, ranging from language modeling to computer vision and generative AI.

Below, you can see a basic transformer architecture consisting of an encoder (left) and a decoder (right). The encoder receives the inputs and generates a contextualized interpretation of the inputs, called embeddings. The decoder uses the information in the embeddings to generate the model’s output, one token at a time.

[![Large Language Models \(LLMs\) are huge deep-learning models pre-trained on vast data. These models are usually based on an architecture called transformers.](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/01/LLM-Fine-Tuning-and-Model-Selection-Using-Neptune-and-Transformers.png?resize=1800%2C1884&ssl=1)](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/01/LLM-Fine-Tuning-and-Model-Selection-Using-Neptune-and-Transformers.png?ssl=1)Transformers architecture. On the left side, we can see the encoder part, which is composed of a stack of multi-head attention and fully connected layers. On the right side, we can see the decoder, which is also composed of a stack of multi-head attention, cross-attention to leverage the information from the encoder, and fully connected layers.

## Hands-on: fine-tuning and selecting an LLM for Brazilian Portuguese

In this project, we’re taking on the challenge of fine-tuning three LLMs: [GPT-2](https://huggingface.co/gpt2), [GPT2-medium](https://huggingface.co/gpt2-medium), [GPT2-large](https://huggingface.co/gpt2-large), and [OPT 125M](https://huggingface.co/facebook/opt-125m). The models have 137 million, 380 million, 812 million, and 125 million parameters, respectively. The largest one, GPT2-large, takes up over 3GB when stored on disk. All these models were trained to generate English-language text.

Our goal is to optimize these models for enhanced performance in Portuguese question answering, addressing the growing demand for AI capabilities in diverse languages. To accomplish this, we’ll need to have a dataset with inputs and labels and use it to “teach” the LLM. Taking a pre-trained model and specializing it to solve new tasks is called fine-tuning. The main advantage of this technique is you can leverage the knowledge the model has to use as a starting point.

### Setting up

I have designed this project to be accessible and reproducible, with a setup that can be replicated on a Colab environment using T4 GPUs. I encourage you to follow along and experiment with the fine-tuning process yourself.

Note that I used a V100 GPU to produce the examples below, which is available if you have a Colab Pro subscription. You can see that I’ve already made a first trade-off between time and money spent here. Colab does not reveal detailed prices, but a [T4 costs $0.35/hour on the underlying Google Cloud Platform, while a V100 costs $2.48/hour.](https://cloud.google.com/compute/gpus-pricing) According to [this benchmark](https://www.dell.com/support/kbdoc/en-us/000132094/deep-learning-performance-on-t4-gpus-with-mlperf-benchmarks), a V100 is three times faster than a T4. Thus, by spending seven times more, we save two-thirds of our time.

You can find all the code in two Colab notebooks:

  * [Fine-tuning](https://colab.research.google.com/drive/1wOg944mbbjLqbW0BvQiu8Qze3-5BPsUH?usp=sharing)
  * [Model selection](https://colab.research.google.com/drive/1TK9vvFpiStxb1veAIBtKXWJPOgzPRFr0?usp=sharing)

[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-related--article.svg) Related post  How to Version and Organize ML Experiments That You Run in Google Colab  Read more  ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](/blog/google-colab-version-organize-ml-experiments)

We will use Python 3.10 in our codes. Before we begin, we’ll install all the libraries we will need. Don’t worry if you’re not familiar with them yet. We’ll go into their purpose in detail when we first use them:

```
`pip install transformers==4.35.2 bitsandbytes==0.41.3 peft==0.7.0 accelerate==0.25.0 datasets==2.16.1 neptune==1.8.6 evaluate==0.4.1 -qq`
```

Copy the JavaScript snippet!

### Loading and pre-processing the dataset

We’ll use the FaQuAD dataset to fine-tune our models. It’s a Portuguese question-answering dataset available in the Hugging Face dataset collection.

First, we’ll look at [the dataset card](https://huggingface.co/datasets/eraldoluis/faquad) to understand how the dataset is structured. We have about 1,000 samples, each consisting of a context, a question, and an answer. Our model’s task is to answer the question based on the context. (The dataset also contains a title and an ID column, but we won’t use them to fine-tune our model.)

![Fine-tunning the models using FaQuAD dataset](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/01/LLM-fine-tuning-and-model-selection-using-Neptune-and-transformers-3.png?resize=978%2C384&ssl=1)Each sample in the FaQuAD dataset consists of a context, a question, and the corresponding answer. | [Source](https://huggingface.co/datasets/eraldoluis/faquad/viewer/plain_text/train)

We can conveniently load the dataset using the Hugging Face datasets library:

```
`from datasets import load_dataset dataset = load_dataset("eraldoluis/faquad")`
```

Copy the JavaScript snippet!

Our next step is to convert the dataset into a format our models can process. For our question-answering task, that’s a sequence-to-sequence format: The model receives a sequence of tokens as the input and produces a sequence of tokens as the output. The input contains the context and the question, and the output contains the answer.

[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-related--article.svg) Related post  Tokenization in NLP: Types, Challenges, Examples, Tools  Read more  ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](/blog/tokenization-in-nlp)

For training, we’ll create a so-called prompt that contains not only the question and the context but also the answer. Using a small helper function, we concatenate the context, question, and answer, divided by section headings (Later, we’ll leave out the answer and ask the model to fill in the “Resposta” section on its own).

We’ll also prepare a helper function that wraps the tokenizer. The tokenizer is what turns the text into a sequence of integer tokens. It is specific to each model, so we’ll have to load and use a different tokenizer for each. The helper function makes that process more manageable, allowing us to process the entire dataset at once using _map_. Last, we’ll shuffle the dataset to ensure the model sees it in randomized order.

Here’s the complete code:

```
`def generate_prompt(data_point): out = f"""Dado o contexto abaixo, responda a questão ### Contexto: {data_point["context"]} ### Questão: {data_point["question"]} ### Resposta: """ if data_point.get("answers"): out += data_point["answers"]["text"][0] return out CUTOFF_LEN = 1024 def tokenize(prompt, tokenizer): result = tokenizer( prompt, truncation=True, max_length=CUTOFF_LEN + 1, padding="max_length", ) return { "input_ids": result["input_ids"][:-1], "attention_mask": result["attention_mask"][:-1], }`
```

Copy the JavaScript snippet!

### Loading and preparing the models

Next, we load and prepare the models that we’ll fine-tune. LLMs are huge models. Without any kind of optimization, for the GPT2-large model in full precision (float32), we have around 800 million parameters, and we need 2.9 GB of memory to load the model and 11.5 GB during the training to handle the gradients. That just about fits in the 16 GB of memory that the T4 in the free tier offers. But we would only be able to compute tiny batches, making training painfully slow.

Faced with these memory and compute resource constraints, we’ll not use the models as-is but use quantization and a method called LoRA to reduce their number of trainable parameters and memory footprint.

### Quantization

Quantization is a technique used to reduce a model’s size in memory by using fewer bits to represent its parameters. For example, instead of using 32 bits to represent a floating point number, we’ll use only 16 or even as little as 4 bits.

This approach can significantly decrease the memory footprint of a model, which is especially important when deploying large models on devices with limited memory or processing power. By reducing the precision of the parameters, quantization can lead to a faster inference time and lower power consumption. However, it’s essential to balance the level of quantization with the potential loss in the model’s task performance, as excessive quantization can degrade accuracy or effectiveness.

The Hugging Face transformers library has [built-in support for quantization](https://huggingface.co/docs/transformers/v4.36.1/en/quantization#bitsandbytes) through the [bitsandbytes](https://github.com/TimDettmers/bitsandbytes) library. You can pass

```
``load_in_8bit=True``
```

Copy the JavaScript snippet!

or 

```
``load_in_4bit=True` to the `from_pretrained()``
```

Copy the JavaScript snippet!

model loading methods to load a model with 8-bit or 4-bit precision, respectively.

After loading the model, we call the wrapper function prepare_model_for_kbit_training from the peft library. It prepares the model for training in a way that saves memory. It does this by freezing the model parameters, making sure all parts use the same type of data format, and using a special technique called [gradient checkpointing](https://huggingface.co/docs/transformers/v4.18.0/en/performance#gradient-checkpointing) if the model can handle it. This helps in training large AI models, even on computers with little memory.

```
`from transformers import AutoModelForCausalLM, AutoTokenizer from peft import prepare_model_for_kbit_training from peft import get_peft_model, LoraConfig model_name = 'gpt2-large' model = AutoModelForCausalLM.from_pretrained(model_name, device_map = "auto", load_in_8bit=True, trust_remote_code=True) tokenizer = AutoTokenizer.from_pretrained(model_name) tokenizer.pad_token = tokenizer.eos_token tokenizer.pad_token_id = tokenizer.eos_token_id model = prepare_model_for_kbit_training(model)`
```

Copy the JavaScript snippet!

After quantizing the model to 8 bits, it takes only a fourth of the memory to load and train the model, respectively. For GPT2-large, instead of needing 2.9 GB to load, it now takes only 734 MB.

[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-related--article.svg) Related post  Deploying Large NLP Models: Infrastructure Cost Optimization  Read more  ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](/blog/nlp-models-infrastructure-cost-optimization)

### LoRA

As we know, Large Language Models have a lot of parameters. When we want to fine-tune one of these models, we usually update all the model’s weights. That means we need to save all the gradient states in memory during fine-tuning, which requires almost twice the model size of memory. Sometimes, when updating all parameters, we can mess up with what the model already learned, leading to worse results in terms of generalization.

Given this context, a team of researchers proposed a new technique called [Low-Rank Adaptation (LoRA)](https://arxiv.org/pdf/2106.09685.pdf). This reparametrization method aims to reduce the number of trainable parameters through low-rank decomposition.

Low-rank decomposition approximates a large matrix into a product of two smaller matrices, such that multiplying a vector by the two smaller matrices yields approximately the same results as multiplying a vector by the original matrix. For example, we could decompose a 3×3 matrix into the product of a 3×1 and a 1×3 matrix so that instead of having nine parameters, we have only six.

![Low-Rank Adaptation \(LoRA\)](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/01/LLM-Fine-Tuning-and-Model-Selection-Using-Neptune-and-Transformers-1.png?resize=1800%2C1800&ssl=1)Low-rank decomposition is a method to split a large matrix M into a product of two smaller matrices, L and R, that approximates it.

When fine-tuning a model, we want to slightly change its weights to adapt it to the new task. More formally, we’re looking for new weights derived from the original weights: _W_ _new_ _=__W_ _old_ _+__W_. Looking at this equation, you can see that we keep the original weights in their original shape and just learn _W_ as LoRA matrices.

In other words, you can freeze your original weights and train just the two LoRA matrices with substantially fewer parameters in total. Or, even more simply, you create a set of new weights in parallel with the original weights and only train the new ones. During the inference, you pass your input to both sets of weights and sum them at the end.

![Fine-tuning using low-rank decomposition](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/01/LLM-Fine-Tuning-and-Model-Selection-Using-Neptune-and-Transformers-2.png?resize=1800%2C1800&ssl=1)Fine-tuning using low-rank decomposition. In blue, we can see the original set of weights of the pre-trained model. Those will be frozen during the fine-tuning. In orange, we can see the low-rank matrices A and B, which will have their weights updated during the fine-tuning.

With our base model loaded, we now want to add the LoRA layers in parallel with the original model weights for fine-tuning. To do this, we need to define a LoraConfig.

Inside the LoraConfig, we can define the rank of the LoRA matrices (parameter r), the dimension of the vector space generated by the matrix columns. We can also look at the rank as a measure of how much compression we are applying to our matrices, i.e., how small the bottleneck between A and B in the figure above will be.

When choosing the rank, keeping in mind the trade-off between the rank of your LoRA matrix and the learning process is essential. Smaller ranks mean less room to learn, i.e., as you have fewer parameters to update, it can be harder to achieve significant improvements. On the other hand, higher ranks provide more parameters, allowing for greater flexibility and adaptability during training. However, this increased capacity comes at the cost of additional computational resources and potentially longer training times. Thus, finding the optimal rank for your LoRA matrix that balances these factors well is crucial, and the best way to find this is by experimenting! A good approach is to start with lower ranks (8 or 16), as you will have fewer parameters to update, so it will be faster, and increase it if you see the model is not learning as much as you want.

You also need to define which modules inside the model you want to apply the LoRA technique to. You can think of a module as a set of layers (or a building block) inside the model. If you want to know more, I’ve prepared a deep dive, but feel free to skip it.

![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)

##  Deep dive: which modules can and should you apply LoRA to? 

Within the LoraConfig, you need to specify which modules to apply LoRA to. You can apply LoRA for most of a model’s modules, but you need to specify the module names that the original developers assigned at model creation. Which modules exist, and their names are different for each model.

The LoRA paper reports that adding LoRA layers only to the keys and values linear projections is a good tradeoff compared to adding LoRA layers to all linear projections in attention blocks. In our case, for the GPT2 model, we will apply LoRA on the c_attn layers, as we don’t have the query, value, and keys weights split, and for the OPT model, we will apply LoRA on the q_proj and v_proj.

If you use other models, you can print the modules’ names and choose the ones you want:

```
`list(model.named_modules())`
```

Copy the JavaScript snippet!

In addition to specifying the rank and modules, you must also set up a hyperparameter called alpha, which scales the LoRA matrix:

```
`scaling = alpha / r weight += (lora_B @ lora_A) * scaling `
```

Copy the JavaScript snippet!

As a rule of thumb (as discussed in [this article by Sebastian Raschka](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms)), you can start setting this to be two times the rank r. If your results are not good, you can try lower values.

Here’s the complete LoRA configuration for our experiments:

```
`config = LoraConfig( r=8, lora_alpha=16, target_modules=["c_attn"], # for gpt2 models # target_modules=["q_proj", "v_proj"], # for opt models lora_dropout=0.1, bias="none", task_type="CAUSAL_LM", ) `
```

Copy the JavaScript snippet!

We can apply this configuration to our model by calling

```
`model = get_peft_model(model, config) `
```

Copy the JavaScript snippet!

Now, just to show how many parameters we are saving, let’s print the trainable parameters of GPT2-large:

```
`model.print_trainable_parameters() >> trainable params: 2,949,120 || all params: 776,979,200 || trainable%: 0.3795622842928099 `
```

Copy the JavaScript snippet!

We can see that we are updating less than 1% of the parameters! What an efficiency gain!

### Fine-tuning the models

With the dataset and models prepared, it’s time to move on to fine-tuning. Before we start our experiments, let’s take a step back and consider our approach. We’ll be training four different models with different modifications and using different training parameters. We’re not only interested in the model’s performance but also have to work with constrained resources.

Thus, it will be crucial that we keep track of what we’re doing and progress as systematically as possible. At any point in time, we want to ensure that we’re moving in the right direction and spending our time and money wisely.

[ ![](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/07/blog_feature_image_030500_9_2_5_1.jpg?fit=200%2C105&ssl=1) ![](https://neptune.ai/wp-content/themes/neptune/img/icon-related--article.svg) Related post  ML Experiment Tracking: What It Is, Why It Matters, and How to Implement It  Read more  ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/blog/ml-experiment-tracking)

### What is essential to log and monitor during the fine-tuning process?

Aside from monitoring standard metrics like training and validation loss and training parameters such as the learning rate, in our case, we also want to be able to log and monitor other aspects of the fine-tuning:

  1. **Resource Utilization:** Since you’re operating with limited computational resources, it’s vital to keep a close eye on GPU and CPU usage, memory consumption, and disk usage. This ensures you’re not overtaxing your system and can help troubleshoot performance issues.
  2. **Model Parameters and Hyperparameters:** To ensure that others can replicate your experiment, storing all the details about the model setup and the training script is crucial. This includes the architecture of the model, such as the sizes of the layers and the dropout rates, as well as the hyperparameters, like the batch size and the number of epochs. Keeping a record of these elements is key to understanding how they affect the model’s performance and allowing others to recreate your experiment accurately.
  3. **Epoch Duration and Training Time:** Record the duration of each training epoch and the total training time. This data helps assess the time efficiency of your training process and plan future resource allocation.



### Set up logging with neptune.ai

[neptune.ai](/product) is the most scalable experiment tracker for teams that train foundation models. It lets you monitor months-long model training, track massive amounts of data, and compare thousands of metrics in the blink of an eye. Neptune is integrated with the transformers library’s Trainer module, allowing you to log and monitor your model training seamlessly. This integration was contributed by Neptune’s developers, who maintain it to this day.

To use Neptune, you’ll have to sign up for an account first (don’t worry, it’s free for personal use) and create a project in your workspace. Have a look at [the Quickstart guide in Neptune’s documentation](https://docs.neptune.ai/usage/quickstart/). There, you’ll also find up-to-date instructions for obtaining the project and token IDs you’ll need to connect your Colab environment to Neptune.

We’ll set these as environment variables:

```
`import os os.environ["NEPTUNE_PROJECT"] = "your-project-ID-goes-here" os.environ["NEPTUNE_API_TOKEN"] = "your-API-token-goes-here"`
```

Copy the JavaScript snippet!

There are two options for logging information from transformer training to Neptune: You can either setreport_to=”neptune” in the TrainingArguments or pass an instance of NeptuneCallback to the Trainer’s callbacks parameter. I prefer the second option because it gives me more control over what I log. Note that if you pass a logging callback, you should set report_to=”none” in the TrainingArgumentsto avoid duplicate data being reported.

Below, you can see how I typically instantiate the NeptuneCallback. I specified a name for my experiment run and asked Neptune to log all parameters used and the hardware metrics. Setting log_checkpoints=”last” ensures that the last model checkpoint will also be saved on Neptune.

```
`from transformers.integrations import NeptuneCallback neptune_callback = NeptuneCallback( name=f"fine-tuning-{model_name}", log_parameters=True, log_checkpoints="last", capture_hardware_metrics=True ) `
```

Copy the JavaScript snippet!

### Training a model

As the last step before configuring the Trainer, it’s time to tokenize the dataset with the model’s tokenizer. Since we’ve loaded the tokenizer together with the model, we can now put the helper function we prepared earlier into action:

```
`tokenized_datasets = dataset.shuffle().map(lambda x: tokenize(generate_prompt(x), tokenizer))`
```

Copy the JavaScript snippet!

The training is managed by a Trainer object. The Trainer uses a DataCollatorForLanguageModeling, which prepares the data in a way suitable for language model training.

Here’s the full setup of the Trainer:

```
`from transformers import ( Trainer, TrainingArguments, GenerationConfig, DataCollatorForLanguageModeling, set_seed ) set_seed(42) EPOCHS = 20 GRADIENT_ACCUMULATION_STEPS = 8 MICRO_BATCH_SIZE = 8 LEARNING_RATE = 2e-3 WARMUP_STEPS = 100 LOGGING_STEPS = 20 trainer = Trainer( model=model, train_dataset=tokenized_datasets["train"], args=TrainingArguments( per_device_train_batch_size=MICRO_BATCH_SIZE, gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS, warmup_steps=WARMUP_STEPS, num_train_epochs=EPOCHS, learning_rate=LEARNING_RATE, output_dir="lora-faquad", logging_steps=LOGGING_STEPS, save_strategy="epoch", gradient_checkpointing=True, report_to="none" ), callbacks=[neptune_callback], data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False), ) model.config.use_cache = False `
```

Copy the JavaScript snippet!

That’s a lot of code, so let’s go through it in detail:

  * The training process is defined to run for 20 epochs (EPOCHS = 20). You’ll likely find that training for even more epochs will lead to better results.


  * We’re using a technique called [gradient accumulation](https://huggingface.co/docs/transformers/v4.18.0/en/performance#gradient-accumulation), set here to 8 steps (GRADIENT_ACCUMULATION_STEPS = 8), which helps handle larger batch sizes effectively, especially when memory resources are limited. In simple terms, gradient accumulation is a technique to handle large batches. Instead of having a batch of 64 samples and updating the weights for every step, we can have a batch size of 8 samples and perform eight steps, just updating the weights in the last step. It generates the same result as a batch of 64 but saves memory.
  * The MICRO_BATCH_SIZE is set to 8, indicating the number of samples processed each step. It is extremely important to find an amount of samples that can fit in your GPU memory during the training to avoid out-of-memory issues (Have a look at [the transformers documentation](https://huggingface.co/docs/transformers/v4.18.0/en/performance#batch-sizes) to learn more about this).
  * The learning rate, a crucial hyperparameter in training neural networks, is set to 0.002 (LEARNING_RATE = 2e-3), determining the step size at each iteration when moving toward a minimum of the loss function. To facilitate a smoother and more effective training process, the model will gradually increase its learning rate for the first 100 steps (WARMUP_STEPS = 100), helping to stabilize early training phases.
  * The trainer is set not to use the model’s cache (model.config.use_cache = False) to manage memory more efficiently.



With all of that in place, we can launch the training:

```
`trainer_output = trainer.train(resume_from_checkpoint=False) `
```

Copy the JavaScript snippet!

While training is running, head over to Neptune, navigate to your project, and click on the experiment that is running. There, click on Charts to see how your training progresses (loss and learning rate). To see resource utilization, click the Monitoring tab and follow how GPU and CPU usage and memory utilization change over time. When the training finishes, you can see other information like training samples per second, training steps per second, and more.

At the end of the training, we capture the output of this process in trainer_output, which typically includes details about the training performance and metrics that we will later use to save the model on the model registry.

But first, we’ll have to check whether our training was successful.

![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)

##  **Aside**

Track months-long model training with more confidence. Use neptune.ai forking feature to iterate faster and optimize the usage of GPU resources. 

With Neptune, users can visualize forked training out of the box. This means you can:

  * Test multiple configs at the same time. Stop the runs that don’t improve accuracy. And continue from the most accurate last step. 
  * Restart failed training sessions from any previous step. The training history is inherited, and the entire experiment is visible on a single chart. 



![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)

![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)

[ ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) See in app  ](https://scale.neptune.ai/o/neptune/org/LLM-training-example/runs/compare?viewId=9d0e032a-5a78-4a0e-81d1-98e0a7c81a8f&dash=charts&query=\(\(%60sys%2Ftags%60%3AstringSet%20CONTAINS%20%22forks%22\)\)%20AND%20\(\(%60sys%2Fgroup_tags%60%3AstringSet%20CONTAINS%20%22experiments_CGB5CSFI%22\)\)&lbViewUnpacked=true&sortBy=%5B%22sys%2Fcreation_time%22%5D&sortFieldType=%5B%22datetime%22%5D&sortFieldAggregationMode=%5B%22auto%22%5D&sortDirection=%5B%22descending%22%5D&experimentsOnly=false&runsLineage=FULL&compare=u0MsW4a1PJIUJ75nglpjHa9XUKFfAmcBRbLhNatCHX20) ![zoom](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) Full screen preview 

  * ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)

Check the [documentation](https://docs-beta.neptune.ai/fork_experiment/)

  * ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)

Play with an[ interactive example project](https://scale.neptune.ai/o/neptune/org/LLM-training-example/runs/compare?viewId=9d0e032a-5a78-4a0e-81d1-98e0a7c81a8f&detailsTab=metadata&dash=charts&type=run&experimentOnly=true&compare=u0MsW4a1PJIUJ75nglpjHa9XUKFfAmcBRbLhNatCHX20)

  * ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)

[Get in touch](/contact-us) to go through a custom demo with our engineering team




### Evaluating the fine-tuned LLMs

Model evaluation in AI, particularly for language models, is a complex and multifaceted task. It involves navigating a series of trade-offs among cost, data applicability, and alignment with human preferences. This process is critical in ensuring that the developed models are not only technically proficient but also practical and user-centric.

### LLM evaluation approaches

![LLM evaluation approaches](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/01/LLM-Fine-Tuning-and-Model-Selection-Using-Neptune-and-Transformers-5.png?resize=1800%2C942&ssl=1)Diagram of different evaluation strategies organized by evaluation metrics and data | Modified based on [source](https://www.youtube.com/watch?v=2CIIQ5KZWUM)

The chart above shows that the least expensive (and most commonly used) approach is to use public benchmarks. On the one hand, this approach is highly cost-effective and easy to test. However, on the other hand, it is less likely to resemble production data. Another option, slightly more costly than benchmarks, is AutoEval, where other language models are used to evaluate the target model. For those with a higher budget, user testing, where the model is made accessible to users, or human evaluation, which involves a dedicated team of humans focused on assessing the model, is an option.

[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-related--resource.svg) Related post  How Elevatus Uses Neptune to Check Experiment Results in 1 Minute  Read more  ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](/customers/elevatus)

### Evaluating question-answering models with F1 scores and the exact match metric

In our project, considering the need to balance cost-effectiveness with maintaining evaluation standards for the dataset, we will employ two specific metrics: exact match and F1 score. We’ll use the validation set provided along with the FaQuAD dataset. Hence, our evaluation strategy falls into the `Public Benchmarks category, as it relies on a well-known dataset to evaluate PTBR models.

The exact match metric determines if the response given by the model precisely aligns with the target answer. This is a straightforward and effective way to assess the model’s accuracy in replicating expected responses. We’ll also calculate the F1 score, which combines precision and recall, of the returned tokens. This will give us a more nuanced evaluation of the model’s performance. By adopting these metrics, we aim to assess our model’s capabilities reliably without incurring significant expenses.

As we said previously, there are various ways to evaluate an LLM, and we choose this way, using standard metrics, because it is fast and cheap. However, there are some trade-offs when choosing “hard” metrics to evaluate results that can be correct, even when the metrics say it is not good.

One example is: imagine the target answer for some question is “The rat found the cheese and ate it.” and the model’s prediction is “The mouse discovered the cheese and consumed it.” Both examples have almost the same meaning, but the words chosen differ. For metrics like exact match and F1, the scores will be really low. A better – but more costly – evaluation approach would be to have humans annotate or use another LLM to verify if both sentences have the same meaning.

[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-related--article.svg) Related post  The Ultimate Guide to Evaluation and Selection of Models in Machine Learning  Read more  ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](/blog/ml-model-evaluation-and-selection)

### Implementing the evaluation functions

Let’s return to our code. I’ve decided to create my own evaluation functions instead of using the Trainer’s built-in capabilities to perform the evaluation. On the one hand, this gives us more control. On the other hand, I frequently encountered out-of-memory (OOM) errors while doing evaluations directly with the Trainer.

For our evaluation, we’ll need two functions:

  * get_logits_and_labels: Processes a sample, generates a prompt from it, passes this prompt through a model, and returns the model’s logits (scores) along with the token IDs of the target answer.
  * compute_metrics: Evaluates a model on a dataset, calculating exact match (EM) and F1 scores. It iterates through the dataset, using the _get_logits_and_labels_ function to generate model predictions and corresponding labels. Predictions are determined by selecting the most likely token indices from the logits. For the EM score, it decodes these predictions and labels into text and computes the EM score. For the F1 score, it maintains the original token IDs and calculates the score for each sample, averaging them at the end.



Here’s the complete code:

```
`import evaluate import torch from tqdm.auto import tqdm import numpy as np def get_logits_and_labels(sample_, max_new_tokens): sample = sample_.copy() del sample["answers"] prompt = generate_prompt(sample) inputs = tokenizer(prompt, return_tensors="pt") input_ids = inputs["input_ids"].cuda() attention_mask = inputs["attention_mask"].cuda() generation_output = model.generate( input_ids=input_ids, attention_mask=attention_mask, return_dict_in_generate=True, output_scores=True, max_new_tokens=max_new_tokens, num_beams=1, do_sample=False ) target_ids = tokenizer(sample_["answers"]["text"][0], return_tensors="pt") scores = torch.concat(generation_output["scores"]) return scores.cpu(), target_ids["input_ids"] def compute_metrics(dataset, max_new_tokens): metric1 = evaluate.load("exact_match") metric2 = evaluate.load("f1") em_preds = [] em_refs = [] f1_preds = [] f1_refs = [] for s in tqdm(dataset): logits, labels = get_logits_and_labels(s, max_new_tokens) predictions = np.argmax(logits, axis=-1)[:len(labels[0])] labels = np.where(labels != -100, labels, tokenizer.pad_token_id) labels = labels[0, :len(predictions)] f1_preds.append(predictions) f1_refs.append(labels) em_pred = tokenizer.batch_decode(predictions, skip_special_tokens=True) em_ref = tokenizer.batch_decode(labels, skip_special_tokens=True) em_preds.append("".join(em_pred)) em_refs.append("".join(em_ref)) em=metric1.compute(predictions=em_preds, references=em_refs)["exact_match"] f1_result = 0 for pred, ref in zip(f1_preds, f1_refs): f1_result += metric2.compute(predictions=pred, references=ref, average="macro")["f1"] return em, f1_result / len(f1_preds) `
```

Copy the JavaScript snippet!

Before assessing our model, we must switch it to evaluation mode, which deactivates dropout. Additionally, we should re-enable the model’s cache to conserve memory during prediction.

```
`model.eval() model.config.use_cache = True # We need this to avoid OOM issues`
```

Copy the JavaScript snippet!

Following this setup, simply execute the compute_metrics function on the evaluation dataset and specify the desired number of generated tokens to use (Note that using more tokens will increase processing time).

```
`em, f1 = compute_metrics(tokenized_datasets["validation"], max_new_tokens=5) `
```

Copy the JavaScript snippet!

### Storing the models and evaluation results

Now that we’ve finished fine-tuning and evaluating a model, we should save it and move on to the next model. To this end, we’ll create a model_version to store in Neptune’s model registry.

In detail, we’ll save the latest model checkpoint along with the loss, the F1 score, and the exact match metric. These metrics will later allow us to select the optimal model. To create a model and a model version, you will need to define the model key, which is the model identifier and must be uppercase and unique within the project. After defining the model key, to use this model to create a model version, you need to concatenate it with the project identifier that you can find on Neptune under “All projects” – “Edit project information” – “Project key”.

```
`import neptune try: neptune_model = neptune.init_model( key="QAPTBR", # must be uppercase and unique within the project name="ptbr qa model", # optional ) except neptune.exceptions.NeptuneModelKeyAlreadyExistsError: print("Model already exists in this project. Reusing it.") model_version = neptune.init_model_version( model="LLMFIN-QAPTBR", ## Project id + key ) model_version[f"model/artifacts"].upload_files("/content/lora-faquad/checkpoint-260") model_version["model/model-name"] = model_name model_version["model/loss"] = trainer_output.training_loss model_version["model/exact-match"] = em model_version["model/f1"] = f1 `
```

Copy the JavaScript snippet!

### Model selection

Once we’re done with all our model training and experiments, it’s time to jointly evaluate them. This is possible because we monitored the training and stored all the information on Neptune. Now, we’ll use the platform to compare different runs and models to choose the best one for our use case.

After completing all your runs, you can click Compare runs at the top of the project’s page and enable the “small eye” for the runs you want to compare. Then, you can go to the Charts tab, and you will find a joint plot of the losses for all the experiments. [Here’s how it looks in my project.](https://app.neptune.ai/pedro.gengo/llm-finetuning/runs/compare?viewId=standard-view&detailsTab=charts&shortId=LLMFIN-61&dash=charts&type=run&compare=KwBgNMYEzUA) In purple, we can see the loss for the gpt2-large model. As we trained for fewer epochs, we can see that we have a shorter curve, which nevertheless achieved a better loss.

![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)

![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)

[ ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) See in the app  ](https://app.neptune.ai/pedro.gengo/llm-finetuning/runs/compare?viewId=standard-view&detailsTab=charts&shortId=LLMFIN-61&dash=charts&type=run&compare=KwBgNMYEzUA) ![zoom](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) Full screen preview 

Comparison of the loss across different experiments. Purple: gpt2-large. Yellow: opt-125m. Red: gpt-medium. Gray: gpt2. 

The loss function is not yet saturated, indicating that our models still have room for growth and could likely achieve higher levels of performance with additional training time.

Go to the Models page and click on the model you created. You will see an overview of all the versions you trained and uploaded. You can also see the metrics reported and the model name.

![](https://neptune.ai/wp-content/themes/neptune/img/blocks/note/list-arrow.svg)

[Here’s a link to my Neptune project](https://app.neptune.ai/pedro.gengo/llm-finetuning/models?shortId=LLMFIN-QAPTBR&type=model)

![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)

![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)

[ ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) See in the app  ](https://app.neptune.ai/pedro.gengo/llm-finetuning/models?shortId=LLMFIN-QAPTBR&type=model) ![zoom](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) Full screen preview 

Model versions saved on Neptune’s model registry. Listed are the model version’s ID, the time of creation, the owner, and the metrics stored with the model version. 

You’ll notice that none of the model versions have been assigned to a “Stage” yet. Neptune allows you to [assign models to different stages](https://docs.neptune.ai/model_registry/managing_stage/), namely “Staging,” “Production,” and “Archived.”

While we can promote a model through the UI, we’ll return to our code and automatically identify the best model. For this, we first fetch all model versions’ metadata, sort by the exact match and f1 scores, and promote the best model according to these metrics to production:

```
`import neptune model = neptune.init_model(with_id="LLMFIN-QAPTBR") model_versions_df = model.fetch_model_versions_table().to_pandas() df_sorted = model_versions_df.sort_values(["model/exact-match", "model/f1"], ascending=False) model_version = df_sorted.iloc[0]["sys/id"] model_name = df_sorted.iloc[0]["model/model-name"] model_version = neptune.init_model_version( with_id=model_version, ) model_version.change_stage("production") `
```

Copy the JavaScript snippet!

After executing this, we can see, as expected, that gpt2-large (our largest model) was the best model and was chosen to go to production:

![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)

![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)

[ ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) See in the app  ](https://app.neptune.ai/pedro.gengo/llm-finetuning/models?shortId=LLMFIN-QAPTBR&type=model) ![zoom](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) Full screen preview 

The gpt2-large model achieved the best metric scores and was promoted to the “Production” stage. 

Once more, we’ll return to our code and finally use our best model to answer questions in Brazilian Portuguese:

```
`import neptune from peft import PeftModel from transformers import AutoModelForCausalLM, AutoTokenizer model = neptune.init_model(with_id="LLMFIN-QAPTBR") model_versions_df = model.fetch_model_versions_table().to_pandas() df_prod_model = model_versions_df[model_versions_df["sys/stage"] == "production"] model_version = df_prod_model.iloc[0]["sys/id"] model_name = df_prod_model.iloc[0]["model/model-name"] model_version = neptune.init_model_version( with_id=model_version, ) model = AutoModelForCausalLM.from_pretrained(model_name, device_map = "auto", load_in_8bit=True, trust_remote_code=True) tokenizer = AutoTokenizer.from_pretrained(model_name) model_version["model/artifacts"].download() !unzip artifacts model = PeftModel.from_pretrained(model, "/content/lora-faquad/checkpoint-260", local_files_only=True)`
```

Copy the JavaScript snippet!

[![LLM inference before and after fine-tuning](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/01/LLM-Fine-Tuning-and-Model-Selection-Using-Neptune-and-Transformers-6.png?resize=1920%2C4480&ssl=1)](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/01/LLM-Fine-Tuning-and-Model-Selection-Using-Neptune-and-Transformers-6.png?ssl=1)Model inference before and after fine-tuning. The text shows a small piece of information about the rules to pass a course and asks: “What does passing the course depend on?” Before fine-tuning, the model only repeats the question. After fine-tuning, the model can answer the question correctly.

Let’s compare the prediction without fine-tuning and the prediction after fine-tuning. As demonstrated, before fine-tuning, the model didn’t know how to handle Brazilian Portuguese at all and answered by repeating some part of the input or returning special characters like “##########.” However, after fine-tuning, it becomes evident that the model handles the input much better, answering the question correctly (it only added a “?” at the end, but the rest is exactly the answer we’d expect).

We can also look at the metrics before and after fine-tuning and verify how much it improved:

Exact Match  |  F1   
---|---  
**Before fine-tuning** |  0  |  0.007  
**After fine-tuning** |  0.143 |  0.157  
  
Given the metrics and the prediction example, we can conclude that the fine-tuning was in the right direction, even though we have room for improvement.

![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)

##  **Editor’s note**

Do you feel like experimenting with neptune.ai?

  * ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)

[Create a free account](https://neptune.ai/register) right away and give it a go

  * ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)

[Try it out first](https://docs.neptune.ai/usage/quickstart/) and learn how it works (zero setup, no registration)

  * ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)

[See the docs](https://docs.neptune.ai/) or watch a short [product demo (2 min)](/resources/foundation-model-training)




## How to improve the solution?

In this article, we’ve detailed a simple and efficient technique for fine-tuning LLMs.

Of course, we still have some way to go to achieve good performance and consistency. There are various additional, more advanced strategies you can employ, such as:

  * **More Data:** Add more high-quality, diverse, and relevant data to the training set to improve the model’s learning and generalization.
  * **Tokenizer Merging:** Combine tokenizers for better input processing, especially for multilingual models.
  * **Model-Weight Tuning:** Directly adjust the pre-trained model weights to fit the new data better, which can be more effective than tuning adapter weights.
  * **Reinforcement Learning with Human Feedback:** Employ human raters to provide feedback on the model’s outputs, which is used to fine-tune the model through reinforcement learning, aligning it more closely with complex objectives.
  * **More Training Steps:** Increasing the number of training steps can further enhance the model’s understanding and adaptation to the data.

[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-related--article.svg) Related post  How to Improve ML Model Performance [Best Practices From Ex-Amazon AI Researcher]  Read also  ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](/blog/improving-ml-model-performance)

## Conclusion

We engaged in four distinct trials throughout our experiments, each employing a different model. We’ve used quantization and LoRA to reduce the memory and compute resource requirements. Throughout the training and evaluation, we’ve used Neptune to log metrics and store and manage the different model versions.

I hope this article inspired you to explore the possibilities of LLMs further. In particular, if you’re a native speaker of a language that’s not English, I’d like to encourage you to explore fine-tuning LLMs in your native tongue.

##  Was the article useful? 

![yes](https://neptune.ai/wp-content/themes/neptune/img/icon-article-rating--yes.svg) Yes  ![no](https://neptune.ai/wp-content/themes/neptune/img/icon-article-rating--no.svg) No 

![](https://neptune.ai/wp-content/themes/neptune/img/icon-bulb.svg) Suggest changes 

Your email Your message (optional)

This site is protected by reCAPTCHA and the Google [Privacy Policy](https://policies.google.com/privacy) and [Terms of Service](https://policies.google.com/terms) apply. 

I am familiar with the [Privacy Policy](https://neptune.staginglab.eu/privacy-policy)*

Submit

Δ

![](https://neptune.ai/wp-content/themes/neptune/img/blocks/i-box/header-icon.svg)

### **More about** LLM Fine-Tuning and Model Selection Using Neptune and Transformers 

####  Check out our  **product resources** and  **related articles** below: 

[ ![](https://neptune.ai/wp-content/themes/neptune/img/ibox-related.svg) Related article  LLMOps: What It Is, Why It Matters, and How to Implement It  Read more  ![chevron](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/blog/llmops) [ ![](https://neptune.ai/wp-content/themes/neptune/img/ibox-related.svg) Related article  ML Experiment Tracking: What It Is, Why It Matters, and How to Implement It  Read more  ![chevron](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/blog/ml-experiment-tracking) [ ![](https://neptune.ai/wp-content/themes/neptune/img/ibox-bulb.svg) Product resource  How Veo Eliminated Work Loss With Neptune  Read more  ![chevron](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/customers/veo) [ ![](https://neptune.ai/wp-content/themes/neptune/img/ibox-related.svg) Related article  Building LLM Applications With Vector Databases  Read more  ![chevron](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/blog/building-llm-applications-with-vector-databases)

###  Explore more content topics: 

[ Computer Vision ](https://neptune.ai/blog/category/computer-vision) [ General ](https://neptune.ai/blog/category/general) [ LLMOps ](https://neptune.ai/blog/category/llmops) [ ML Model Development ](https://neptune.ai/blog/category/machine-learning-model-development) [ ML Tools ](https://neptune.ai/blog/category/machine-learning-tools) [ MLOps ](https://neptune.ai/blog/category/mlops) [ Natural Language Processing ](https://neptune.ai/blog/category/natural-language-processing) [ Paper Reflections ](https://neptune.ai/blog/category/paper-reflections) [ Product Updates ](https://neptune.ai/blog/category/product-updates) [ Reinforcement Learning ](https://neptune.ai/blog/category/reinforcement-learning) [ Tabular Data ](https://neptune.ai/blog/category/tabular-data) [ Time Series ](https://neptune.ai/blog/category/time-series-forecasting)

About neptune.ai ![chevron](https://neptune.ai/wp-content/themes/neptune/img/blocks/accordion-simple/icon-arrow-accordion-small.svg)

![](data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1920 1080'%3E%3C/svg%3E) ![](https://neptune.ai/wp-content/themes/neptune/img/icon-play.svg)

Neptune is the experiment tracker for teams that train foundation models. 

It lets you monitor months-long model training, track massive amounts of data, and compare thousands of metrics in seconds. 

[ Play with a live project  ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://demo.neptune.ai/) [ See Docs  ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://docs.neptune.ai/)

Table of contents

![chevron](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-small.svg)

  1. Large language models
  2. Hands-on: fine-tuning and selecting an LLM for Brazilian Portuguese
     * Setting up
     * Loading and pre-processing the dataset
     * Loading and preparing the models
     * Fine-tuning the models
     * Evaluating the fine-tuned LLMs
     * Storing the models and evaluation results
     * Model selection
  3. How to improve the solution?
  4. Conclusion



![chevron](https://neptune.ai/wp-content/themes/neptune/img/popup-cancel.svg) ![chevron](https://neptune.ai/wp-content/themes/neptune/img/popup-more.svg)

Check also: [Deploying Conversational AI Products to Production With Jason Flaks](https://neptune.ai/blog/deploying-conversational-ai-products-with-jason-flaks)

##  **Manage your model metadata in a single place**

Join 50,000+ ML Engineers & Data Scientists using Neptune to easily log, compare, register, and share ML metadata.

[ Try Neptune for free  ](https://app.neptune.ai/register) [ Check out the Docs  ](https://docs.neptune.ai/)

[ Take an interactive product tour  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) ](https://app.neptune.ai/o/showcase/org/onboarding-project/runs/table?viewId=98f66b32-2279-4b73-8210-863021c440ac&product_tour_id=444083)

![](https://neptune.ai/wp-content/themes/neptune/img/footer-post-illustration.svg)

#### Newsletter

Top articles, case studies, events (and more) in your inbox every month.

Get Newsletter

  * [Product](#)
    * [Resources](https://neptune.ai/resources)
    * [Pricing](https://neptune.ai/pricing)
    * [Deployment options](https://neptune.ai/product/deployment-options)
    * [Service status](https://status.neptune.ai/)


  * [Solutions](#)
    * [AI Researcher](https://neptune.ai/product/ai-researcher)
    * [ML Team Lead](https://neptune.ai/product/ml-team-lead)
    * [ML Platform Engineer](https://neptune.ai/product/ml-platform)
    * [Enterprise](https://neptune.ai/product/enterprise)
    * [Academic Research](https://neptune.ai/research)


  * [Documentation](#)
    * [Quickstart](https://docs.neptune.ai/usage/quickstart/)
    * [Neptune docs](https://docs.neptune.ai/)
    * [Neptune integrations](https://docs.neptune.ai/integrations/)


  * [Compare](#)
    * [Neptune vs Weights & Biases](https://neptune.ai/vs/wandb)
    * [Neptune vs MLflow](https://neptune.ai/vs/mlflow)
    * [Neptune vs TensorBoard](https://neptune.ai/vs/tensorboard)
    * [Other comparisons](/vs)
    * [ML experiment tracking tools](/blog/best-ml-experiment-tracking-tools)


  * [Community](#)
    * [Blog](https://neptune.ai/blog)
    * [Experiment Tracking Learning Hub](https://neptune.ai/experiment-tracking-learn-hub)
    * [LLMOps Learning Hub](https://neptune.ai/llmops-learning-hub)
    * [MLOps Learning Hub](https://neptune.ai/mlops-learn-hub)
    * [How to Build an Experiment Tracker](https://neptune.ai/blog/build-experiment-tracking-tool)


  * [Company](#)
    * [About us](https://neptune.ai/about-us)
    * [Customers](https://neptune.ai/customers)
    * [Careers](https://neptune.ai/jobs)
    * [Security portal and SOC 2](https://security.neptune.ai/)
    * [Contact us](https://neptune.ai/contact-us)



[ ![social icon](https://neptune.ai/wp-content/uploads/2022/08/icon-linked-in.svg) ](https://www.linkedin.com/company/neptuneai) [ ![social icon](https://neptune.ai/wp-content/uploads/2022/08/icon-twitter.svg) ](https://twitter.com/neptune_ai) [ ![social icon](https://neptune.ai/wp-content/uploads/2022/08/icon-github.svg) ](https://github.com/neptune-ai) [ ![social icon](https://neptune.ai/wp-content/uploads/2022/08/icon-facebook.svg) ](https://www.facebook.com/neptuneAI) [ ![social icon](https://neptune.ai/wp-content/uploads/2022/08/icon-youtube.svg) ](https://www.youtube.com/channel/UCvOJU-ubyUqxGSDRN7xK4Ng) [ ![social icon](https://neptune.ai/wp-content/uploads/2023/01/icon-spotify.svg) ](https://open.spotify.com/show/4kGi82i4wTYgHbWmVMri5x) [ ![social icon](https://neptune.ai/wp-content/uploads/2023/01/icon-apple-podcast.svg) ](https://podcasts.apple.com/us/podcast/mlops-live/id1634179447)

Copyright © 2025 Neptune Labs. All rights reserved.

![](https://neptune.ai/wp-content/themes/neptune/img/footer-stars--414w.png) ![](https://pixel.wp.com/g.gif?v=ext&blog=211928962&post=34430&tz=0&srv=neptune.ai&hp=atomic&ac=2&amp=0&j=1%3A14.3-a.3&host=neptune.ai&ref=&fcp=479&rand=0.4670529119859861)

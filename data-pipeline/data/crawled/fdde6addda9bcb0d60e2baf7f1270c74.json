{
    "id": "fdde6addda9bcb0d60e2baf7f1270c74",
    "metadata": {
        "id": "fdde6addda9bcb0d60e2baf7f1270c74",
        "url": "https://huyenchip.com/2023/10/10/multimodal.html#part_2_multimodal_training/",
        "title": "Multimodality and Large Multimodal Models (LMMs)",
        "properties": {
            "description": "For a long time, each ML model operated in one data mode – text (translation, language modeling), image (object detection, image classification), or audio (s...",
            "keywords": null,
            "author": "Chip Huyen",
            "og:title": "Multimodality and Large Multimodal Models (LMMs)",
            "og:locale": "en_US",
            "og:description": "For a long time, each ML model operated in one data mode – text (translation, language modeling), image (object detection, image classification), or audio (speech recognition).",
            "og:url": "https://huyenchip.com/2023/10/10/multimodal.html",
            "og:site_name": "Chip Huyen",
            "og:image": "https://huyenchip.com/assets/pics/multimodal/4-CLIP-architecture.png",
            "og:type": "article",
            "twitter:card": "summary_large_image"
        }
    },
    "parent_metadata": {
        "id": "ff6ebff0c6c5ef4a1faeea66f49724e3",
        "url": "https://www.notion.so/Multimodal-ff6ebff0c6c5ef4a1faeea66f49724e3",
        "title": "Multimodal",
        "properties": {
            "Type": "Leaf"
        }
    },
    "content": "[Chip Huyen](/)\n\n[Blog](/blog/) [Books](/books/) [Events](/events/)\n\nAI Guide\n\n  * [AI Roadmap](/mlops/)\n  * [Llama Police](/llama-police)\n  * [ML Interviews](https://huyenchip.com/ml-interviews-book/)\n\n\n\n[List 100](/list-100/) [VN](https://huyenchip.com/vn/)\n\n**Table of Contents** [Part 1. Understanding Multimodal](#part_1_understanding_multimodal)\n\n  * [Why multimodal](#why_multimodal)\n  * [Data modalities](#data_modalities)\n  * [Multimodal tasks](#multimodal_tasks)\n    * [Generation](#generation)\n    * [Vision-language understanding](#vision_language_understanding)\n\n[Part 2. Fundamentals of Multimodal Training](#part_2_multimodal_training)\n\n  * [CLIP: Contrastive Language-Image Pre-training](#clip)\n    * [CLIP's high-level architecture](#clip_s_high_level_architecture)\n    * [Natural language supervision](#natural_language_supervision)\n    * [Contrastive learning](#contrastive_learning)\n      * [Classifier objective](#classifier_objective)\n      * [Language model objective](#lm_objective)\n      * [Contrastive objective](#contrastive_objective)\n    * [CLIP applications](#clip_applications)\n  * [Flamingo: the dawns of LMMs](#flamingo)\n    * [Flamingo's high-level architecture](#flamingo_s_high_level_architecture)\n    * [Data](#data)\n    * [Flamingo's vision encoder](#flamingo_s_vision_encoder)\n    * [Flamingo's language model](#flamingo_s_language_model)\n  * [TL;DR: CLIP vs. Flamingo](#clip_vs_flamingo)\n\n[Part 3. Research Directions for LMMs](#part_3_research_directions_for_lmms)\n\n  * [Incorporating more data modalities](#incorporating_more_data_modalities)\n  * [Multimodal systems for instruction-following](#multimodal_systems_for_instruction_following)\n  * [Adapters for more efficient multimodal training](#adapters_for_more_efficient_multimodal_training)\n  * [Generating multimodal outputs](#generating_multimodal_outputs)\n\n[Conclusion](#conclusion) [Resources](#resources) Table of Contents \n\n# Multimodality and Large Multimodal Models (LMMs)\n\nOct 10, 2023  • Chip Huyen\n\nFor a long time, each ML model operated in one data mode – text (translation, language modeling), image (object detection, image classification), or audio (speech recognition).\n\nHowever, natural intelligence is not limited to just a single modality. Humans can read, talk, and see. We listen to music to relax and watch out for strange noises to detect danger. Being able to work with multimodal data is essential for us or any AI to operate in the real world.\n\nOpenAI noted in their [GPT-4V system card](https://cdn.openai.com/papers/GPTV_System_Card.pdf) that “ _incorporating additional modalities (such as image inputs) into LLMs is viewed by some as a key frontier in AI research and development_.”\n\nIncorporating additional modalities to LLMs (Large Language Models) creates LMMs (Large Multimodal Models). Not all multimodal systems are LMMs. For example, text-to-image models like Midjourney, Stable Diffusion, and Dall-E are multimodal but don’t have a language model component. Multimodal can mean one or more of the following:\n\n  1. Input and output are of different modalities (e.g. text-to-image, image-to-text)\n  2. Inputs are multimodal (e.g. a system that can process both text and images)\n  3. Outputs are multimodal (e.g. a system that can generate both text and images)\n\n\n\nThis post covers multimodal systems in general, including LMMs. It consists of 3 parts.\n\n  * Part 1 covers the context for multimodality, including why multimodal, different data modalities, and types of multimodal tasks.\n  * Part 2 discusses the fundamentals of a multimodal system, using the examples of CLIP, which lays the foundation for many future multimodal systems, and Flamingo, whose impressive performance gave rise to LMMs.\n  * Part 3 discusses some active research areas for LMMs, including generating multimodal outputs and adapters for more efficient multimodal training, covering newer multimodal systems such as BLIP-2, LLaVA, LLaMA-Adapter V2, LAVIN, etc.\n\n\n\nThe post is long. Feel free to skip to the sections most interesting to you.\n\n**⚠ Ambiguous terminology ⚠** Multimodal data can also refer to multimodal distributions, e.g. bimodal distribution, which is different from multimodal data in this post. \n\n**Table of contents** [Part 1. Understanding Multimodal](#part_1_understanding_multimodal) …. [Why multimodal](#why_multimodal) …. [Data modalities](#data_modalities) …. [Multimodal tasks](#multimodal_tasks) …….. [Generation](#generation) …….. [Vision-language understanding](#vision_language_understanding) [Part 2. Fundamentals of Multimodal Training](#part_2_multimodal_training) …. [CLIP: Contrastive Language-Image Pre-training](#clip) …….. [CLIP’s high-level architecture](#clip_s_high_level_architecture) …….. [Natural language supervision](#natural_language_supervision) …….. [Contrastive learning](#contrastive_learning) …….. [CLIP applications](#clip_applications) …. [Flamingo: the dawns of LMMs](#flamingo) …….. [Flamingo’s high-level architecture](#flamingo_s_high_level_architecture) …….. [Data](#data) …….. [Flamingo’s vision encoder](#flamingo_s_vision_encoder) …….. [Flamingo’s language model](#flamingo_s_language_model) …. [TL;DR: CLIP vs. Flamingo](#clip_vs_flamingo) [Part 3. Research Directions for LMMs](#part_3_research_directions_for_lmms) …. [Incorporating more data modalities](#incorporating_more_data_modalities) …. [Multimodal systems for instruction-following](#multimodal_systems_for_instruction_following) …. [Adapters for more efficient multimodal training](#adapters_for_more_efficient_multimodal_training) …. [Generating multimodal outputs](#generating_multimodal_outputs) [Conclusion](#conclusion) [Resources](#resources)\n\n## Part 1. Understanding Multimodal\n\n## Why multimodal\n\nMany use cases are impossible without multimodality, especially those in industries that deal with a mixture of data modalities such as healthcare, robotics, e-commerce, retail, gaming, etc.\n\n![Multimodal AI in healthcare](/assets/pics/multimodal/26-healthcare.png) An example of how multimodality can be used in healthcare. Image from Multimodal biomedical AI (Acosta et al., Nature Medicine 2022) \n\nNot only that, incorporating data from other modalities can help boost model performance. Shouldn’t a model that can learn from both text and images perform better than a model that can learn from only text or only image?\n\nMultimodal systems can provide a more flexible interface, allowing you to interact with them in whichever way works best for you at the moment. Imagine you can ask a question by typing, talking, or just pointing your camera at something.\n\nOne use case that I’m especially excited about, is that multimodality can also enable visually impaired people to browse the Internet and also navigate the real world.\n\n![Some cool multimodal use cases from GPT-4V](/assets/pics/multimodal/1-gpt-4v-use-cases.png) Some cool multimodal use cases from GPT-4V \n\n## Data modalities\n\nDifferent data modes are text, image, audio, tabular data, etc. One data mode can be represented or _approximated_ in another data mode. For example:\n\n  * Audio can be represented as images (mel spectrograms).\n  * Speech can be transcribed into text, though its text-only representation loses information such as volume, intonation, pauses, etc.\n  * An image can be represented as a vector, which, in turn, can be flattened and represented as a sequence of text tokens.\n  * A video is a sequence of images plus audio. ML models today mostly treat videos as sequences of images. This is a severe limitation, as sounds have proved to be just as important as visuals for videos. [88% of TikTok users shared that sound is essential for their TikTok experience](https://www.kantar.com/uki/inspiration/advertising-media/the-power-of-tiktok).\n  * A text can be represented as an image if you simply take a picture of it.\n  * A data table can be converted into a chart, which is an image.\n\n\n\n**How about other data modalities?**\n\nAll digital data formats can be represented using bitstrings (strings of 0 and 1) or bytestrings. A model that can effectively learn from bitstrings or bytestrings will be very powerful, and it can learn from any data mode.\n\nThere are other data modalities we haven’t touched on, such as graphs and 3D assets. We also haven’t touched on the formats used to represent smell and touch (haptics).\n\nIn ML today, audio is still largely treated as a voice-based alternative to text. The most common use cases for audio are still speech recognition (speech-to-text) and speech synthesis (text-to-speech). Non-speech audio use cases, e.g. music generation, are still pretty niche. See the fake Drake & Weeknd song and [MusicGen model on HuggingFace](https://huggingface.co/spaces/facebook/MusicGen).\n\nImage is perhaps the most versatile format for model inputs, as it can be used to represent text, tabular data, audio, and to some extent, videos. There’s also so much more visual data than text data. We have phones/webcams that constantly take pictures and videos today.\n\nText is a much more powerful mode for model outputs. A model that can generate images can only be used for image generation, whereas a model that can generate text can be used for many tasks: summarization, translation, reasoning, question answering, etc.\n\nFor simplicity, we’ll focus on 2 modalities: images and text. The learnings can be somewhat generalized to other modalities.\n\n## Multimodal tasks\n\nTo understand multimodal systems, it’s helpful to look at the tasks they are built to solve. In literature, I commonly see vision-language tasks divided into two groups: **generation** and **vision-language understanding** (VLU), which is the umbrella term for all tasks that don’t require generation. The line between these two groups is blurred, as being able to generate answers requires understanding too.\n\n### Generation\n\nFor generative tasks, the output can be unimodal (e.g. text, image, 3D rendering) or multimodal. While unimodal outputs are common today, multimodal outputs are still shaping up. We’ll discuss multimodal outputs at the end of this post.\n\n#### Image generation (text-to-image synthesis)\n\nThis category is straightforward. Examples: Dall-E, Stable Diffusion, and Midjourney.\n\n#### Text generation\n\nA common text generation task is visual question answering. Instead of relying only on text for the context, you can give the model both text and images. Imagine you can point your camera to anything and ask questions like: “My car won’t start. What’s wrong with it?”, “How to make this dish?”, or “What is this meme about?”.\n\nAnother common use case is image captioning, which can be used as part of a text-based image retrieval system. An organization might have millions, if not billions, of images: product images, graphs, designs, team pictures, promotional materials, etc. AI can automatically generate captions and metadata for them, making it easier to find the exact images you want.\n\n### Vision-language understanding\n\nWe’ll zoom into two task types: classification and text-based image retrieval (TBIR).\n\n#### Classification\n\nClassification models can only generate outputs that belong to a pre-determined list of classes. This works when you only care about a fixed number of outcomes. For example, an OCR system only needs to predict if a visual is one of the known characters (e.g. a digit or a letter).\n\n**Side note** : An OCR system processes data at the character level. When used together with a system that can understand the broader context, it can improve use cases such as allowing you to “talk” to any textbook, contract, assembly instructions, etc.\n\n![Document processing with GPT-4V](/assets/pics/multimodal/2-gpt-4v-ocr.png) Document processing with GPT-4V. The model's mistake is highlighted in red. \n\nOne related task to classification is **image-to-text retrieval** : given an image and a pool of pre-defined texts, find the text that’s most likely to accompany the image. This can be helpful for product image search, i.e. retrieving product reviews from a picture.\n\n#### Text-based image retrieval (image search)\n\nImage search matters not only for search engines but also for enterprises to be able to search through all their internal images and documents. Some people call text-based image retrieval “text-to-image retrieval”.\n\nThere are several approaches to text-based image retrieval. Two of them are:\n\n  1. Generate captions and metadata for each image, either manually or automatically (see image captioning in **Text generation**). Given a text query, find images whose captions/metadata are closest to this text query.\n  2. Train a joint embedding space for both images and text. Given a text query, generate an embedding for this query, and find all images whose embeddings are closest to this embedding.\n\n\n\nThe second approach is more flexible, and I believe will be more widely used. This approach requires having a strong joint embedding space for both vision and language, like the one that OpenAI’s [CLIP](https://arxiv.org/abs/2103.00020) developed.\n\n## Part 2. Fundamentals of Multimodal Training\n\nGiven the existence of so many amazing multimodal systems, a challenge of writing this post is choosing which systems to focus on. In the end, I decided to focus on two models: [CLIP](https://arxiv.org/abs/2103.00020) (2021) and [Flamingo](https://arxiv.org/abs/2204.14198) (2022) both for their significance as well as availability and clarity of public details.\n\n  * CLIP was the first model that could generalize to multiple **image classification tasks** with zero- and few-shot learning.\n  * Flamingo wasn’t the first large multimodal model that could **generate open-ended responses** ([Salesforce’s BLIP](https://arxiv.org/abs/2201.12086) came out 3 months prior). However, Flamingo’s strong performance prompted some to consider it [the GPT-3 moment in the multimodal domain](https://arxiv.org/abs/2304.08485).\n\n\n\nEven though these two models are older, many techniques they use are still relevant today. I hope they serve as the foundation to understanding newer models. The multimodal space is evolving repaidly, with many new ideas being developed. We’ll go over these newer models in [Part 3](#part_3_research_directions_for_lmms).\n\nAt a high level, a multimodal system consists of the following components:\n\n  1. An **encoder** for each data modality to generate the embeddings for data of that modality.\n  2. A way to **align embeddings** of different modalities into the same **multimodal embedding space**.\n  3. [Generative models only] A **language model to generate text responses**. Since inputs can contain both text and visuals, new techniques need to be developed to allow the language model to condition its responses on not just text, but also visuals.\n\n\n\nIdeally, as many of these components should be pretrained and reusable as possible.\n\n## CLIP: Contrastive Language-Image Pre-training\n\nCLIP’s key contribution is its ability to map data of different modalities, text and images, into a shared embedding space. This shared multimodal embedding space makes text-to-image and image-to-text tasks so much easier.\n\nTraining this multimodal embedding space also produced a strong image encoder, which allows CLIP to achieve **competitive zero-shot performance on many image classification tasks**. This strong image encoder can be used for many other tasks: image generation, visual question answering, and text-based image retrieval. Flamingo and LLaVa use CLIP as their image encoder. DALL-E uses CLIP to rerank generated images. It’s unclear if GPT-4V uses CLIP.\n\n![Zero-shot image classification with CLIP](/assets/pics/multimodal/3-CLIP-image-classification.png) Zero-shot image classification with CLIP \n\nCLIP leveraged **natural language supervision** and **contrastive learning** , which allowed CLIP to both scale up their data and make training more efficient. We’ll go over why/how these two techniques work.\n\n### CLIP's high-level architecture\n\n![Architecture of OpenAI's CLIP](/assets/pics/multimodal/4-CLIP-architecture.png) CLIP's architecture. Both encoders and projection matrices are jointly trained together from scratch. The training goal is to maximize the similarity scores of the right (image, text) pairings while minimizing the similarity scores of the wrong pairings (contrastive learning). \n\nFor the **image encoder** , the authors experimented with both ResNet and ViT. Their best-performing model is `ViT-L/14@336px`:\n\n  * Large vision transformer (ViT-L)\n  * 14 patches (each image is divided into 14x14 pixel patches/sub-images)\n  * on 336x336 pixel input\n\n\n\nFor the **text encoder** , CLIP uses a Transformer model similar to [GPT-2](https://openai.com/research/better-language-models) but smaller. Their base model has only 63M parameters with 8 attention heads. The authors found CLIP’s performance to be less sensitive to the capacity of the text encoder.\n\nEmbeddings generated by the image encoder and text encoder are projected into the same embedding space using two projection matrices \\\\(W_v\\\\) and \\\\(W_l\\\\).\n\n  * Given an image embedding \\\\(V_i\\\\), the corresponding multimodal embedding is computed as: \\\\(W_vV_i\\\\).\n  * Given a text embedding \\\\(L_i\\\\), the corresponding multimodal embedding is computed as: \\\\(W_lL_i\\\\).\n\n\n\nWhen people say CLIP embeddings, they either refer to these multimodal embeddings or the embeddings generated by CLIP’s image encoder.\n\n### Natural language supervision\n\nFor many years, image models were trained with manually annotated (image, text) datasets (e.g. ImageNet, MS COCO). This isn’t scalable. Manual annotation is time-consuming and expensive.\n\nThe CLIP paper noted that none of the then-available (image, text) datasets was big and high quality enough. They created their own dataset – 400M (image, text) pairs – as follows.\n\n  1. Construct a list of 500,000 queries. Queries are common words, bigrams, and titles of popular Wikipedia articles.\n  2. Find images matching these queries (string and substring match). The paper mentioned this search did NOT happen on search engines but didn’t specify where. My theory is that since OpenAI already scraped the entire Internet for their GPT models, they probably just queried their internal database.\n  3. Each image is paired with a text that co-occurs with it (e.g. captions, comments) instead of the query since queries are too short to be descriptive.\n\n\n\nBecause some queries are more popular than others, to avoid data imbalance, they used at most 20K images for a query.\n\n### Contrastive learning\n\nPre-CLIP, most vision-language models were trained using a classifier or language model objectives. Contrastive objective is a clever technique that allows CLIP to scale and generalize to multiple tasks.\n\nWe’ll show why the constrastive objective works better for CLIP using an example task of image captioning: given an image, generate a text that describes it.\n\n#### Classifier objective\n\nA classifier predicts the correct class among a predetermined list of classes. This works when the output space is finite. Previous models that work with (image, text) pair datasets all had this limitation. For example, models working with [ILSVRC-2012](https://www.image-net.org/challenges/LSVRC/2012/) limited themselves to 1,000 classes, and [JFT-300M](https://arxiv.org/abs/1707.02968) to 18,291 classes.\n\nThis objective limits not only the model’s capacity to output meaningful responses but also its capacity for zero-shot learning. Say, if the model was trained to predict among 10 classes, it won’t work for a task that has 100 classes.\n\n#### Language model objective\n\nIf a classifier outputs only one class for each input, a language model outputs a sequence of classes. Each generated class is called a token. Each token is from a predetermined list, the vocabulary, of the language model.\n\n![Classifier vs. language model objectives](/assets/pics/multimodal/5-classifier-vs-language-model-objectives.png) Classifier vs. language model objectives \n\n#### Contrastive objective\n\nWhile the language model objective allows for vastly more flexible outputs, CLIP authors noted this objective made the training difficult. They hypothesized that this is because the model tries to generate _exactly_ the text accompanying each image, while many possible texts can accompany an image: alt-text, caption, comments, etc.\n\nFor example, in the [Flickr30K dataset](https://arxiv.org/abs/1509.04942), each image has 5 captions provided by human annotators, and the captions for the same image can be very different.\n\n![Multiple captions for the same image](/assets/pics/multimodal/6-multiple-captions.png)\n\nContrastive learning is to overcome this challenge. Instead of predicting the exact text of each image, CLIP was trained to predict whether a text is more likely to accompany an image than other texts.\n\nFor each batch of \\\\(N\\\\) (image, text) pairs, the model generates N text embeddings and N image embeddings.\n\n  * Let \\\\(V_1, V_2, ..., V_n\\\\) be the embeddings for the \\\\(N\\\\) images.\n  * Let \\\\(L_1, L_2, ..., L_n\\\\) be the embeddings for the \\\\(N\\\\) texts.\n\n\n\nCLIP computes the cosine similarity scores of the \\\\(N^2\\\\) possible (\\\\(V_i, L_j\\\\)) pairings. The model is trained to maximize the similarity scores of the \\\\(N\\\\) correct pairings while minimizing the scores of the \\\\(N^2 - N\\\\) incorrect pairings. For CLIP, \\\\(N = 32,768\\\\).\n\n![How CLIP works](/assets/pics/multimodal/7-clip.png)\n\nAnother way to look at this is that each training batch of CLIP is two classification tasks.\n\n  1. Each image can be paired with N possible texts, and the model tries to predict the correct one. This is the same setup as image-to-text retrieval.\n\n\\\\[L_{\\text{contrastive:txt2im}} = -\\frac{1}{N}\\sum_i^N\\log(\\frac{\\exp(L_i^TV_i\\beta)}{\\sum_j^N\\exp(L_i^TV_j\\beta)})\\\\] \n  2. Each text can be paired with N possible images, and the model tries to predict the correct image. This is the same setup as text-to-image retrieval.\n\n\\\\[L_{\\text{contrastive:im2txt}} = -\\frac{1}{N}\\sum_i^N\\log(\\frac{\\exp(V_i^TL_i\\beta)}{\\sum_j^N\\exp(V_i^TL_j\\beta)})\\\\] \n\n\n\nThe sum of these two losses is minimized. 𝛽 is a trainable inverse temperature parameter.\n\nThis is what it all looks like in pseudocode.\n\n![CLIP pseudocode](/assets/pics/multimodal/8-clip-pseudocode.png)\n\nCLIP authors found that the contrastive objective provided a 12x improvement in efficiency compared to the language model objective baseline while producing higher-quality image embeddings.\n\n![CLIP constrastive learning](/assets/pics/multimodal/9-contrastive-learning-efficiency.png)\n\n### CLIP applications\n\n#### Classification\n\nToday, for many image classification tasks, CLIP is still a strong out-of-the-box baseline to be used as-is or fine-tuned.\n\n![CLIP as a strong baseline for image classification](/assets/pics/multimodal/10-clip-perf.png)\n\n#### Text-based image retrieval\n\nSince CLIP’s training process was conceptually similar to image-to-text retrieval and text-to-image retrieval, CLIP “ _displays significant promise for widely-applicable tasks like image retrieval or search_.” However, “ _on image retrieval, CLIP’s performance relative to the overall state of the art is noticeably lower._ ”\n\nThere are attempts to use CLIP for image retrieval. For example, [clip-retrieval](https://github.com/rom1504/clip-retrieval) package works as follows:\n\n  1. Generate CLIP embeddings for all your images and store them in a vector database.\n  2. For each text query, generate a CLIP embedding for this text.\n  3. Query in the vector database for all images whose embeddings are close to this text query embedding.\n\n\n\n#### Image generation\n\nCLIP’s joint image-text embeddings are useful for image generation. Given a text prompt, [DALL-E](https://openai.com/research/dall-e) (2021) generates many different visuals and uses CLIP to rerank these visuals before showing the top visuals to users.\n\nIn 2022, OpenAI introduced [unCLIP](https://openai.com/research/hierarchical-text-conditional-image-generation-with-clip-latents), a text-to-image synthesis model conditioned on CLIP latents. It consists of two main components:\n\n  1. CLIP is trained and frozen. The pretrained CLIP model can generate embeddings for both text and images in the same embedding space.\n  2. Two things happen at image generation: \n     * Use CLIP to generate embedding for this text.\n     * Use a diffusion decoder to generate images conditioned on this embedding.\n\n![unCLIP](/assets/pics/multimodal/11-unCLIP.png)\n\n#### Text generation: visual question answering, captioning\n\nCLIP authors did attempt to create a model for text generation. One version they experimented with is called LM RN50. Though this model could generate text responses, its performance was consistently around 10% below CLIP’s best-performing model on all the vision-language understanding tasks that CLIP was evaluated on.\n\nWhile today CLIP isn’t used directly for text generation, its image encoder is often the backbone for LMMs that can generate texts.\n\n## Flamingo: the dawns of LMMs\n\nUnlike CLIP, Flamingo can generate text responses. In a reductive view, Flamingo is CLIP + a language model, with added techniques to make it possible for the language model to generate text tokens conditioned on both visual and text inputs.\n\n![Conversations with Flamingo LMMs](/assets/pics/multimodal/12-flamingo-chatbots.png) Flamingo can generate text responses conditioned on both text and images \n\n### Flamingo's high-level architecture\n\nAt a high level, Flamingo consists of 2 parts:\n\n  1. **Vision encoder** : a CLIP-like model is trained using contrastive learning. The text encoder of this model is then discarded. The vision encoder is frozen to be used in the main model.\n  2. **Language model** : Flamingo finetunes Chinchilla to generate text tokens, conditioned on visuals and text, using language model loss, with two additional components Perceiver Resampler and GATED XATTN-DENSE layers. We’ll discuss them later in this blog.\n\n![Flamingo high level architecture](/assets/pics/multimodal/13-flamingo-architecture.png)\n\n### Data\n\nFlamingo used 4 datasets: 2 (image, text) pair datasets, 1 (video, text) pair dataset, and 1 interleaved image and text dataset.\n\n![Flamingo's 4 datasets](/assets/pics/multimodal/14-flamingo-data.png) **Dataset** | **Type** | **Size** | **How** | **Training weight**  \n---|---|---|---|---  \nM3W  | Interleaved image and text dataset  | 43M webpages  | For each webpage, they sample a random subsequence of 256 tokens and take up to the first 5 images included in the sampled sequence.  | 1.0   \nALIGN  | (Image, text) pairs  | 1.8B pairs  | Texts are alt-texts, averaging 12 tokens/text.  | 0.2   \nLTIP  | (Image, text) pairs  | 312M pairs  | Texts are long descriptions, averaging 20.5 tokens/text.  | 0.2   \nVTP  | (Video, text) pairs  | 27M short videos  | ~22 seconds/video on average  | 0.03   \n  \n### Flamingo's vision encoder\n\nFlamingo first trains a CLIP-like model from scratch using contrastive learning. This component only uses the 2 (image, text) pair datasets, ALIGN and LTIP, totaling 2.1B (image, text) pairs. This is 5x larger than the dataset CLIP was trained on.\n\n  * For the text encoder, Flamingo uses BERT instead of GPT-2.\n  * For the vision encoder, Flamingo uses a NormalizerFree ResNet (NFNet) F6 model.\n  * Text and vision embeddings are meanpooled before being projected to the joint embedding space.\n\n\n\n### Flamingo's language model\n\nFlamingo uses Chinchilla as their language model. More specifically, they freeze the 9 pretrained Chinchilla LM layers. A traditional language model predicts the next text token based on the preceding text tokens. Flamingo predicts the next text token based on both the preceding text and visual tokens.\n\n![Flamingo's 4 datasets](/assets/pics/multimodal/15-lmm-text-generation.png) Next token generation is conditioned on both text and visual tokens. Illustration taken from Chunyuan Li's CVPR 2023 tutorial: Large Multimodal Models. \n\nTo be able to generate text conditioned on both text and visual inputs, Flamingo relied on Perceiver Resampler and GATED XATTN-DENSE layers.\n\n#### Perceiver Resampler\n\nAs the visual inputs can be both images and videos, the vision encoder can produce a variable number of image or video features. Perceiver Resampler converts these variable features into a consistent 64 visual outputs.\n\nInterestingly enough, while training the vision encoder, the resolution used was 288 x 288. However, at this phase, visual inputs are resized to 320 × 320. It’s been shown that [a higher test-time resolution can lead to improved performance when using CNNs](https://arxiv.org/abs/1906.06423).\n\n![Flamingo's Perceiver Resampler](/assets/pics/multimodal/16-flamingo-perceiver-resampler.png)\n\n#### GATED XATTN-DENSE layers\n\nGATED XATTN-DENSE layers are inserted between existing and frozen LM layers to allow the language model to attend more efficiently to the visual tokens when generating text tokens. Without these layers, Flamingo authors noted a drop of 4.2% in the overall score.\n\n![Flamingo's GATED ATTN-DENSE layers](/assets/pics/multimodal/17-gated xattn-dense.png)\n\n#### Loss function\n\nFlamingo computes the likelihood of text \\\\(y\\\\) conditioned on the interleaved images and videos \\\\(x\\\\).\n\n\\\\[p(y|x) = \\prod_{l=1}^N p(y_l|y_{<l}, x_{\\leq l})\\\\] \n\nThe training loss function was a weighted sum of expected negative log-likelihoods of generated text across all 4 datasets, with \\\\(\\lambda_m\\\\) being the training weight of dataset \\\\(m\\\\).\n\n\\\\[\\sum_{m=1}^M \\lambda_m E_{(x, y)\\sim D_m} [ -\\sum_{l=1}^L \\log p(y|x)]\\\\] \n\n#### Training\n\nWhile the Chinchilla LM layers are finetuned and frozen, the additional components are trained from scratch, using all 4 Flamingo datasets, with different weights. _Finding the right per-dataset weights was key to performance._ The weight for each dataset is in the **Training weight** column in the dataset table above.\n\nVTP’s weight is much smaller than other datasets (0.03 compared to 0.2 and 1), so its contribution to the training should be minimal. However, the authors noted that removing this dataset negatively affects performance on all video tasks.\n\nWhile Flamingo isn’t open-sourced, there are many open-source replications of Flamingo.\n\n  * [IDEFICS](https://huggingface.co/spaces/HuggingFaceM4/idefics_playground) (HuggingFace)\n  * [mlfoundations/open_flamingo](https://github.com/mlfoundations/open_flamingo/issues)\n\n\n\n## TL;DR: CLIP vs. Flamingo\n\n![Flamingo's 4 datasets](/assets/pics/multimodal/18-clip-flamingo.png)\n\n## Part 3. Research Directions for LMMs\n\nCLIP is 3 years old and Flamingo is almost 2. While their architectures serve as a good foundation for us to understand how LMMs are built, there have been many new progresses in the space.\n\nHere are a few directions that I’m excited about. This is far from an exhaustive list, both because this post has been long and because I’m still learning about the space too. If you have any pointers or suggestions, please let me know!\n\n## Incorporating more data modalities\n\nToday, most multimodal systems work with text and images. It’s only a matter of time before we need systems that can incorporate other modalities such as videos, music, and 3D. Wouldn’t it be amazing to have one shared embedding space for ALL data modalities?\n\nExamples of works in this space:\n\n  * [ULIP: Learning a Unified Representation of Language, Images, and Point Clouds for 3D Understanding](https://arxiv.org/abs/2212.05171) (Xue et al., Dec 2022)\n  * [ImageBind: One Embedding Space To Bind Them All](https://browse.arxiv.org/abs/2305.05665) (Girdhar et al., May 2023)\n  * [NExT-GPT: Any-to-Any Multimodal Large Language Model](https://next-gpt.github.io/) (Wu et al., Sep 2023)\n  * Jeff Dean’s ambitious [Pathways](https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/) project (2021): its vision is to “ _enable multimodal models that encompass vision, auditory, and language understanding simultaneously_.”\n\n![Imagebind](/assets/pics/multimodal/19-imagebind.png)\n\n## Multimodal systems for instruction-following\n\nFlamingo was trained for completion, but not for dialogue or for following instructions. (If you’re not familiar with completion vs. dialogue, check out my post on [RLHF](https://huyenchip.com/2023/05/02/rlhf.html)). Many people are working on building LMMs that can follow instructions and have conversations, such as:\n\n  * [MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning](https://arxiv.org/abs/2212.10773) (Xu et al., Dec 2022)\n  * [LLaVA: Visual Instruction Tuning](https://arxiv.org/abs/2304.08485) (Liu et al., Apr 28, 2023)\n  * [InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning](https://arxiv.org/abs/2305.06500) (Salesforce, May 11, 2023)\n  * LaVIN: [Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models](https://arxiv.org/abs/2305.15023) (Luo et al., May 24, 2023)\n\n![LaVIN](/assets/pics/multimodal/20-LaVIN.png) Examples of LaVIN's outputs compared to other LMMs, shown in LaVIN's paper \n\n## Adapters for more efficient multimodal training\n\nWhile Flamingo used 9 pretrained and frozen layers from Chinchilla, it had to pretrain its vision encoder, Perceiver Resampler, and GATED XATTN-DENSE layers from scratch. These train-from-scratch modules could be compute-intensive. Many works focus on more efficient ways to bootstrap multimodal systems using less training from scratch.\n\nSome works are quite promising. BLIP-2, for example, outperformed Flamingo-80B by 8.7% on zero-shot VQA-v2 with 54x fewer trainable parameters.\n\nWorks in this space include:\n\n  * [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597)\n  * [LAVIN] [Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models](https://arxiv.org/abs/2305.15023)\n  * [LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model](https://arxiv.org/abs/2304.15010)\n\n\n\nThe two images below are from Chunyuan Li’s [Large Multimodal Models](https://datarelease.blob.core.windows.net/tutorial/vision_foundation_models_2023/slides/Chunyuan_cvpr2023_tutorial_lmm.pdf) tutorial at CVPR 2023, which is, btw, an excellent tutorial.\n\n![Adapters for LMMs](/assets/pics/multimodal/21-adapters-1.png) ![Adapters for LMMs](/assets/pics/multimodal/22-adapters-2.png)\n\n## Generating multimodal outputs\n\nWhile models that can process multimodal inputs are becoming the norm, multimodal output is still lagging. Many use cases require multimodal outputs. For example, if we ask ChatGPT to explain RLHF, an effective explanation might require graphs, equations, and even simple animations.\n\nTo generate multimodal outputs, a model would first need to generate a shared intermediate output. One key question is what the intermediate output would look like.\n\nOne option for intermediate output is text, which will then be used to generate/synthesize other actions.\n\nFor example, [CM3](https://arxiv.org/abs/2201.07520) (Aghajanyan et al., 2022) outputs HTML markup which can be compiled into webpages that contain not only text but also formattings, links, and images. GPT-4V generates Latex code, which can then be reconstructed as data tables.\n\n![CM3](/assets/pics/multimodal/23-cm3.png) Sampled outputs from CM3  ![GPT-4V generating LaTeX](/assets/pics/multimodal/24-gpt-4v-latex.png) GPT-4V generates Latex code, which can then be reconstructed as a data table \n\nAnother option for intermediate output would be multimodal tokens. This is the option that [Caiming Xiong](https://www.linkedin.com/in/caiming-xiong-150a1417), whose team at Salesforce has done a lot of awesome work on multimodality, told me. Each token will have a tag to denote whether it’s a text token or an image token. Image tokens will then be input into an image model like Diffusion to generate images. Text tokens will then be input into a language model.\n\n[Generating Images with Multimodal Language Models](https://arxiv.org/abs/2305.17216) (Koh et al., Jun 2023) is an awesome paper that shows how LMMs can generate and retrieve images together with generating texts. See below.\n\n![LMMs generating text and images](/assets/pics/multimodal/27-lmms-generating-images.png)\n\n## Conclusion\n\nIt’s been a lot of fun going over so many multimodal papers as well as talking to people doing awesome work and trying to summarize the key patterns in one blog post. There’s so much about multimodality that I’m sure there are many things that I’ve missed, but I hope that this post provides the core patterns that will help you develop multimodal systems and apply them to your work.\n\nAs you see in part 3 of this post, we’re still in the early days of multimodal systems (so early that a friend told me he’s not sure if the LMM abbreviation would catch on). Yes, in most of my conversations, there’s little doubt that multimodal systems in general, and LMMs in particular, will be even more impactful than large language models. However, keep in mind that LMMs do not make LLMs obsolete. As LMMs extend upon LLMs, the performance of an LMM relies on the performance of its base LLM. Many labs that work on multimodal systems work on LLMs in parallel.\n\n## Early reviewers\n\nI’d like to thank the amazing early reviewers who gave me plenty of pointers and suggestions to make this post better: [Han-chung Lee](https://www.linkedin.com/in/hanchunglee/), [Sam Reiswig](https://www.linkedin.com/in/samreiswig/), and [Luke Metz](https://twitter.com/Luke_Metz).\n\n## Resources\n\n### Models\n\nAn incomplete list of multimodal systems by time to give you a sense of how fast the space is moving!\n\n  * [Microsoft COCO Captions: Data Collection and Evaluation Server](https://arxiv.org/abs/1504.00325) (Apr 2015)\n  * [VQA: Visual Question Answering](https://arxiv.org/abs/1505.00468) (May 2015)\n  * [VideoBERT: A Joint Model for Video and Language Representation Learning](https://arxiv.org/abs/1904.01766) (Google, Apr 3, 2019)\n  * [LXMERT: Learning Cross-Modality Encoder Representations from Transformers](https://arxiv.org/abs/1908.07490) (UNC Chapel Hill, Aug 20, 2019)\n  * [[CLIP] Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) (OpenAI, 2021)\n  * [Unifying Vision-and-Language Tasks via Text Generation](https://arxiv.org/abs/2102.02779) (UNC Chapel Hill, May 2021)\n  * [BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://arxiv.org/abs/2201.12086) (Salesforce, Jan 28, 2022)\n  * [Flamingo: a Visual Language Model for Few-Shot Learning](https://arxiv.org/abs/2204.14198) (DeepMind, April 29, 2022)\n  * [GIT: A Generative Image-to-text Transformer for Vision and Language](https://arxiv.org/abs/2205.14100) (Microsoft, May 2, 2022)\n  * [MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning](https://arxiv.org/abs/2212.10773) (Xu et al., Dec 2022)\n  * [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597) (Salesforce, Jan 30, 2023)\n  * [Cross-Modal Fine-Tuning: Align then Refine](https://arxiv.org/abs/2302.05738) (Shen et al., Feb 11, 2023)\n  * [KOSMOS-1: Language Is Not All You Need: Aligning Perception with Language Models](https://arxiv.org/abs/2302.14045) (Microsoft, Feb 27, 2023)\n  * [PaLM-E: An Embodied Multimodal Language Model](https://arxiv.org/abs/2303.03378) (Google, Mar 10, 2023)\n  * [LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention](https://arxiv.org/abs/2303.16199) (Zhang et al., Mar 28, 2023)\n  * [mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality](https://arxiv.org/abs/2304.14178) (Ye et al., Apr 2, 2023)\n  * [LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model](https://arxiv.org/abs/2304.15010) (Gao et al., Apr 28, 2023)\n  * [LLaVA: Visual Instruction Tuning](https://arxiv.org/abs/2304.08485) (Liu et al., Apr 28, 2023)\n  * [X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages](https://arxiv.org/abs/2305.04160) (Chen et al., May 7, 2023)\n  * [InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning](https://arxiv.org/abs/2305.06500) (Salesforce, May 11, 2023)\n  * [Towards Expert-Level Medical Question Answering with Large Language Models](https://arxiv.org/abs/2305.09617) (Singhal et al., May 16, 2023)\n  * [Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models](https://arxiv.org/abs/2305.15023) (Luo et al., May 24, 2023)\n  * [Shikra: Unleashing Multimodal LLM’s Referential Dialogue Magic](https://arxiv.org/abs/2306.15195) (SenseTime, Jun 3, 2023)\n  * [Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration](https://arxiv.org/abs/2306.09093) (Tencent, Jun 15, 2023)\n\n\n\n### Other resources\n\n  * [CVPR2023 Tutorial Talk] [Large Multimodal Models: Towards Building and Surpassing Multimodal GPT-4](https://www.youtube.com/watch?v=mkI7EPD1vp8)\n    * Slides: [Large Multimodal Models](https://datarelease.blob.core.windows.net/tutorial/vision_foundation_models_2023/slides/Chunyuan_cvpr2023_tutorial_lmm.pdf)\n  * [CMU course] [11-777 MMML](https://cmu-multicomp-lab.github.io/mmml-course/fall2022/)\n  * [Open source] [Salesforce’s LAVIS](https://github.com/salesforce/LAVIS)\n\n\n\n  * chip@[thiswebsite]\n  * [chiphuyen](https://github.com/chiphuyen)\n  * [chipro](https://twitter.com/chipro)\n  * [chipiscrazy](https://facebook.com/chipiscrazy)\n  * [huyenchip19](https://instagram.com/huyenchip19)\n  * [chiphuyen](https://www.linkedin.com/in/chiphuyen)\n\n\n\nI help companies deploy machine learning into production. I write about AI applications, tooling, and best practices. \n\n![]()\n",
    "content_quality_score": 0.9,
    "summary": null,
    "child_urls": [
        "https://huyenchip.com/",
        "https://huyenchip.com/blog/",
        "https://huyenchip.com/books/",
        "https://huyenchip.com/events/",
        "https://huyenchip.com/mlops/",
        "https://huyenchip.com/llama-police",
        "https://huyenchip.com/ml-interviews-book/",
        "https://huyenchip.com/list-100/",
        "https://huyenchip.com/vn/",
        "https://huyenchip.com/2023/10/10/multimodal.html#part_1_understanding_multimodal",
        "https://huyenchip.com/2023/10/10/multimodal.html#why_multimodal",
        "https://huyenchip.com/2023/10/10/multimodal.html#data_modalities",
        "https://huyenchip.com/2023/10/10/multimodal.html#multimodal_tasks",
        "https://huyenchip.com/2023/10/10/multimodal.html#generation",
        "https://huyenchip.com/2023/10/10/multimodal.html#vision_language_understanding",
        "https://huyenchip.com/2023/10/10/multimodal.html#part_2_multimodal_training",
        "https://huyenchip.com/2023/10/10/multimodal.html#clip",
        "https://huyenchip.com/2023/10/10/multimodal.html#clip_s_high_level_architecture",
        "https://huyenchip.com/2023/10/10/multimodal.html#natural_language_supervision",
        "https://huyenchip.com/2023/10/10/multimodal.html#contrastive_learning",
        "https://huyenchip.com/2023/10/10/multimodal.html#classifier_objective",
        "https://huyenchip.com/2023/10/10/multimodal.html#lm_objective",
        "https://huyenchip.com/2023/10/10/multimodal.html#contrastive_objective",
        "https://huyenchip.com/2023/10/10/multimodal.html#clip_applications",
        "https://huyenchip.com/2023/10/10/multimodal.html#flamingo",
        "https://huyenchip.com/2023/10/10/multimodal.html#flamingo_s_high_level_architecture",
        "https://huyenchip.com/2023/10/10/multimodal.html#data",
        "https://huyenchip.com/2023/10/10/multimodal.html#flamingo_s_vision_encoder",
        "https://huyenchip.com/2023/10/10/multimodal.html#flamingo_s_language_model",
        "https://huyenchip.com/2023/10/10/multimodal.html#clip_vs_flamingo",
        "https://huyenchip.com/2023/10/10/multimodal.html#part_3_research_directions_for_lmms",
        "https://huyenchip.com/2023/10/10/multimodal.html#incorporating_more_data_modalities",
        "https://huyenchip.com/2023/10/10/multimodal.html#multimodal_systems_for_instruction_following",
        "https://huyenchip.com/2023/10/10/multimodal.html#adapters_for_more_efficient_multimodal_training",
        "https://huyenchip.com/2023/10/10/multimodal.html#generating_multimodal_outputs",
        "https://huyenchip.com/2023/10/10/multimodal.html#conclusion",
        "https://huyenchip.com/2023/10/10/multimodal.html#resources",
        "https://huyenchip.com/2023/05/02/rlhf.html",
        "https://cdn.openai.com/papers/GPTV_System_Card.pdf",
        "https://www.kantar.com/uki/inspiration/advertising-media/the-power-of-tiktok",
        "https://huggingface.co/spaces/facebook/MusicGen",
        "https://arxiv.org/abs/2103.00020",
        "https://arxiv.org/abs/2204.14198",
        "https://arxiv.org/abs/2201.12086",
        "https://arxiv.org/abs/2304.08485",
        "https://openai.com/research/better-language-models",
        "https://www.image-net.org/challenges/LSVRC/2012/",
        "https://arxiv.org/abs/1707.02968",
        "https://arxiv.org/abs/1509.04942",
        "https://github.com/rom1504/clip-retrieval",
        "https://openai.com/research/dall-e",
        "https://openai.com/research/hierarchical-text-conditional-image-generation-with-clip-latents",
        "https://arxiv.org/abs/1906.06423",
        "https://huggingface.co/spaces/HuggingFaceM4/idefics_playground",
        "https://github.com/mlfoundations/open_flamingo/issues",
        "https://arxiv.org/abs/2212.05171",
        "https://browse.arxiv.org/abs/2305.05665",
        "https://next-gpt.github.io/",
        "https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/",
        "https://arxiv.org/abs/2212.10773",
        "https://arxiv.org/abs/2305.06500",
        "https://arxiv.org/abs/2305.15023",
        "https://arxiv.org/abs/2301.12597",
        "https://arxiv.org/abs/2304.15010",
        "https://datarelease.blob.core.windows.net/tutorial/vision_foundation_models_2023/slides/Chunyuan_cvpr2023_tutorial_lmm.pdf",
        "https://arxiv.org/abs/2201.07520",
        "https://www.linkedin.com/in/caiming-xiong-150a1417",
        "https://arxiv.org/abs/2305.17216",
        "https://www.linkedin.com/in/hanchunglee/",
        "https://www.linkedin.com/in/samreiswig/",
        "https://twitter.com/Luke_Metz",
        "https://arxiv.org/abs/1504.00325",
        "https://arxiv.org/abs/1505.00468",
        "https://arxiv.org/abs/1904.01766",
        "https://arxiv.org/abs/1908.07490",
        "https://arxiv.org/abs/2102.02779",
        "https://arxiv.org/abs/2205.14100",
        "https://arxiv.org/abs/2302.05738",
        "https://arxiv.org/abs/2302.14045",
        "https://arxiv.org/abs/2303.03378",
        "https://arxiv.org/abs/2303.16199",
        "https://arxiv.org/abs/2304.14178",
        "https://arxiv.org/abs/2305.04160",
        "https://arxiv.org/abs/2305.09617",
        "https://arxiv.org/abs/2306.15195",
        "https://arxiv.org/abs/2306.09093",
        "https://www.youtube.com/watch?v=mkI7EPD1vp8",
        "https://cmu-multicomp-lab.github.io/mmml-course/fall2022/",
        "https://github.com/salesforce/LAVIS",
        "https://github.com/chiphuyen",
        "https://twitter.com/chipro",
        "https://facebook.com/chipiscrazy",
        "https://instagram.com/huyenchip19",
        "https://www.linkedin.com/in/chiphuyen"
    ]
}
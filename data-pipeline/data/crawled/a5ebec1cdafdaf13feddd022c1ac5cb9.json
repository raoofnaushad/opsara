{
    "id": "a5ebec1cdafdaf13feddd022c1ac5cb9",
    "metadata": {
        "id": "a5ebec1cdafdaf13feddd022c1ac5cb9",
        "url": "https://arxiv.org/abs/2203.02155/",
        "title": "[2203.02155] Training language models to follow instructions with human feedback",
        "properties": {
            "description": "Abstract page for arXiv paper 2203.02155: Training language models to follow instructions with human feedback",
            "keywords": null,
            "author": null,
            "og:type": "website",
            "og:site_name": "arXiv.org",
            "og:title": "Training language models to follow instructions with human feedback",
            "og:url": "https://arxiv.org/abs/2203.02155v1",
            "og:image": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
            "og:image:secure_url": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
            "og:image:width": "1200",
            "og:image:height": "700",
            "og:image:alt": "arXiv logo",
            "og:description": "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.",
            "twitter:site": "@arxiv",
            "twitter:card": "summary",
            "twitter:title": "Training language models to follow instructions with human feedback",
            "twitter:description": "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not...",
            "twitter:image": "https://static.arxiv.org/icons/twitter/arxiv-logo-twitter-square.png",
            "twitter:image:alt": "arXiv logo"
        }
    },
    "parent_metadata": {
        "id": "b21d158c4a556c2e0060dd35cb6c7bee",
        "url": "https://www.notion.so/LLMs-b21d158c4a556c2e0060dd35cb6c7bee",
        "title": "LLMs",
        "properties": {
            "Type": "Leaf"
        }
    },
    "content": "[Skip to main content](#content)\n\n[![Cornell University](/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n\nIn just 3 minutes help us improve arXiv:\n\n[Annual Global Survey](https://cornell.ca1.qualtrics.com/jfe/form/SV_6m22mbqW9GQ3pQO)\n\nWe gratefully acknowledge support from the Simons Foundation, [member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors. [Donate](https://info.arxiv.org/about/donate.html)\n\n[](/IgnoreMe)\n\n[![arxiv logo](/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](/) > [cs](/list/cs/recent) > arXiv:2203.02155 \n\n[Help](https://info.arxiv.org/help) | [Advanced Search](https://arxiv.org/search/advanced)\n\nAll fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text\n\nSearch\n\n[![arXiv logo](/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n\n[ ![Cornell University Logo](/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg) ](https://www.cornell.edu/)\n\nopen search\n\nGO\n\nopen navigation menu\n\n## quick links\n\n  * [Login](https://arxiv.org/login)\n  * [Help Pages](https://info.arxiv.org/help)\n  * [About](https://info.arxiv.org/about)\n\n\n\n# Computer Science > Computation and Language\n\n**arXiv:2203.02155** (cs) \n\n[Submitted on 4 Mar 2022]\n\n# Title:Training language models to follow instructions with human feedback\n\nAuthors:[Long Ouyang](https://arxiv.org/search/cs?searchtype=author&query=Ouyang,+L), [Jeff Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu,+J), [Xu Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang,+X), [Diogo Almeida](https://arxiv.org/search/cs?searchtype=author&query=Almeida,+D), [Carroll L. Wainwright](https://arxiv.org/search/cs?searchtype=author&query=Wainwright,+C+L), [Pamela Mishkin](https://arxiv.org/search/cs?searchtype=author&query=Mishkin,+P), [Chong Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+C), [Sandhini Agarwal](https://arxiv.org/search/cs?searchtype=author&query=Agarwal,+S), [Katarina Slama](https://arxiv.org/search/cs?searchtype=author&query=Slama,+K), [Alex Ray](https://arxiv.org/search/cs?searchtype=author&query=Ray,+A), [John Schulman](https://arxiv.org/search/cs?searchtype=author&query=Schulman,+J), [Jacob Hilton](https://arxiv.org/search/cs?searchtype=author&query=Hilton,+J), [Fraser Kelton](https://arxiv.org/search/cs?searchtype=author&query=Kelton,+F), [Luke Miller](https://arxiv.org/search/cs?searchtype=author&query=Miller,+L), [Maddie Simens](https://arxiv.org/search/cs?searchtype=author&query=Simens,+M), [Amanda Askell](https://arxiv.org/search/cs?searchtype=author&query=Askell,+A), [Peter Welinder](https://arxiv.org/search/cs?searchtype=author&query=Welinder,+P), [Paul Christiano](https://arxiv.org/search/cs?searchtype=author&query=Christiano,+P), [Jan Leike](https://arxiv.org/search/cs?searchtype=author&query=Leike,+J), [Ryan Lowe](https://arxiv.org/search/cs?searchtype=author&query=Lowe,+R)\n\nView a PDF of the paper titled Training language models to follow instructions with human feedback, by Long Ouyang and 19 other authors\n\n[View PDF](/pdf/2203.02155)\n\n> Abstract:Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent. \n\nSubjects: |  Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)  \n---|---  \nCite as: | [arXiv:2203.02155](https://arxiv.org/abs/2203.02155) [cs.CL]  \n(or  [arXiv:2203.02155v1](https://arxiv.org/abs/2203.02155v1) [cs.CL] for this version)   \n<https://doi.org/10.48550/arXiv.2203.02155> Focus to learn more arXiv-issued DOI via DataCite  \n  \n## Submission history\n\nFrom: Jan Leike [[view email](/show-email/f3b1678a/2203.02155)] **[v1]** Fri, 4 Mar 2022 07:04:42 UTC (1,047 KB) \n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Training language models to follow instructions with human feedback, by Long Ouyang and 19 other authors\n\n  * [View PDF](/pdf/2203.02155)\n  * [TeX Source](/src/2203.02155)\n  * [Other Formats](/format/2203.02155)\n\n\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/ \"Rights to this article\")\n\nCurrent browse context: \n\ncs.CL\n\n[< prev](/prevnext?id=2203.02155&function=prev&context=cs.CL \"previous in cs.CL \\(accesskey p\\)\") |  [next >](/prevnext?id=2203.02155&function=next&context=cs.CL \"next in cs.CL \\(accesskey n\\)\")\n\n[new](/list/cs.CL/new) |  [recent](/list/cs.CL/recent) | [2022-03](/list/cs.CL/2022-03)\n\nChange to browse by: \n\n[cs](/abs/2203.02155?context=cs) [cs.AI](/abs/2203.02155?context=cs.AI) [cs.LG](/abs/2203.02155?context=cs.LG)\n\n### References & Citations\n\n  * [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2203.02155)\n  * [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2203.02155)\n  * [Semantic Scholar](https://api.semanticscholar.org/arXiv:2203.02155)\n\n\n\n### [ 13 blog links](/tb/2203.02155)\n\n([what is this?](https://info.arxiv.org/help/trackback.html)) \n\n[a](/static/browse/0.3.4/css/cite.css) export BibTeX citation Loading...\n\n## BibTeX formatted citation\n\n√ó\n\nloading...\n\nData provided by: \n\n### Bookmark\n\n[ ![BibSonomy logo](/static/browse/0.3.4/images/icons/social/bibsonomy.png) ](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2203.02155&description=Training language models to follow instructions with human feedback \"Bookmark on BibSonomy\") [ ![Reddit logo](/static/browse/0.3.4/images/icons/social/reddit.png) ](https://reddit.com/submit?url=https://arxiv.org/abs/2203.02155&title=Training language models to follow instructions with human feedback \"Bookmark on Reddit\")\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _([What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _([What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _([What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _([What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _([What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _([What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _([What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _([What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _([What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _([What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _([What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _([What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _([What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _([What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _([What is CORE?](https://core.ac.uk/services/recommender))_\n\n  * Author\n  * Venue\n  * Institution\n  * Topic\n\n\n\nAbout arXivLabs \n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](/auth/show-endorsers/2203.02155) | [Disable MathJax](javascript:setMathjaxCookie\\(\\)) ([What is MathJax?](https://info.arxiv.org/help/mathjax.html)) \n\n  * [About](https://info.arxiv.org/about)\n  * [Help](https://info.arxiv.org/help)\n\n\n\n  * contact arXivClick here to contact arXiv [ Contact](https://info.arxiv.org/help/contact.html)\n  * subscribe to arXiv mailingsClick here to subscribe [ Subscribe](https://info.arxiv.org/help/subscribe)\n\n\n\n  * [Copyright](https://info.arxiv.org/help/license/index.html)\n  * [Privacy Policy](https://info.arxiv.org/help/policies/privacy_policy.html)\n\n\n\n  * [Web Accessibility Assistance](https://info.arxiv.org/help/web_accessibility.html)\n  * [arXiv Operational Status ](https://status.arxiv.org) Get status notifications via [email](https://subscribe.sorryapp.com/24846f03/email/new) or [slack](https://subscribe.sorryapp.com/24846f03/slack/new)\n\n\n\n",
    "content_quality_score": 0.9,
    "summary": null,
    "child_urls": [
        "https://arxiv.org/abs/2203.02155/#content",
        "https://info.arxiv.org/about/ourmembers.html",
        "https://info.arxiv.org/about/donate.html",
        "https://arxiv.org/IgnoreMe",
        "https://arxiv.org/",
        "https://arxiv.org/list/cs/recent",
        "https://info.arxiv.org/help",
        "https://arxiv.org/search/advanced",
        "https://arxiv.org/login",
        "https://info.arxiv.org/about",
        "https://arxiv.org/search/cs?searchtype=author&query=Ouyang,+L",
        "https://arxiv.org/search/cs?searchtype=author&query=Wu,+J",
        "https://arxiv.org/search/cs?searchtype=author&query=Jiang,+X",
        "https://arxiv.org/search/cs?searchtype=author&query=Almeida,+D",
        "https://arxiv.org/search/cs?searchtype=author&query=Wainwright,+C+L",
        "https://arxiv.org/search/cs?searchtype=author&query=Mishkin,+P",
        "https://arxiv.org/search/cs?searchtype=author&query=Zhang,+C",
        "https://arxiv.org/search/cs?searchtype=author&query=Agarwal,+S",
        "https://arxiv.org/search/cs?searchtype=author&query=Slama,+K",
        "https://arxiv.org/search/cs?searchtype=author&query=Ray,+A",
        "https://arxiv.org/search/cs?searchtype=author&query=Schulman,+J",
        "https://arxiv.org/search/cs?searchtype=author&query=Hilton,+J",
        "https://arxiv.org/search/cs?searchtype=author&query=Kelton,+F",
        "https://arxiv.org/search/cs?searchtype=author&query=Miller,+L",
        "https://arxiv.org/search/cs?searchtype=author&query=Simens,+M",
        "https://arxiv.org/search/cs?searchtype=author&query=Askell,+A",
        "https://arxiv.org/search/cs?searchtype=author&query=Welinder,+P",
        "https://arxiv.org/search/cs?searchtype=author&query=Christiano,+P",
        "https://arxiv.org/search/cs?searchtype=author&query=Leike,+J",
        "https://arxiv.org/search/cs?searchtype=author&query=Lowe,+R",
        "https://arxiv.org/pdf/2203.02155",
        "https://arxiv.org/abs/2203.02155",
        "https://arxiv.org/abs/2203.02155v1",
        "https://arxiv.org/show-email/f3b1678a/2203.02155",
        "https://arxiv.org/src/2203.02155",
        "https://arxiv.org/format/2203.02155",
        "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "https://arxiv.org/prevnext?id=2203.02155&function=prev&context=cs.CL",
        "https://arxiv.org/prevnext?id=2203.02155&function=next&context=cs.CL",
        "https://arxiv.org/list/cs.CL/new",
        "https://arxiv.org/list/cs.CL/recent",
        "https://arxiv.org/list/cs.CL/2022-03",
        "https://arxiv.org/abs/2203.02155?context=cs",
        "https://arxiv.org/abs/2203.02155?context=cs.AI",
        "https://arxiv.org/abs/2203.02155?context=cs.LG",
        "https://arxiv.org/tb/2203.02155",
        "https://info.arxiv.org/help/trackback.html",
        "https://arxiv.org/static/browse/0.3.4/css/cite.css",
        "https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer",
        "https://info.arxiv.org/labs/index.html",
        "https://arxiv.org/auth/show-endorsers/2203.02155",
        "https://info.arxiv.org/help/mathjax.html",
        "https://info.arxiv.org/help/contact.html",
        "https://info.arxiv.org/help/subscribe",
        "https://info.arxiv.org/help/license/index.html",
        "https://info.arxiv.org/help/policies/privacy_policy.html",
        "https://info.arxiv.org/help/web_accessibility.html",
        "https://status.arxiv.org",
        "https://www.cornell.edu/",
        "https://cornell.ca1.qualtrics.com/jfe/form/SV_6m22mbqW9GQ3pQO",
        "https://doi.org/10.48550/arXiv.2203.02155",
        "https://ui.adsabs.harvard.edu/abs/arXiv:2203.02155",
        "https://scholar.google.com/scholar_lookup?arxiv_id=2203.02155",
        "https://api.semanticscholar.org/arXiv:2203.02155",
        "http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2203.02155&description=Training language models to follow instructions with human feedback",
        "https://reddit.com/submit?url=https://arxiv.org/abs/2203.02155&title=Training language models to follow instructions with human feedback",
        "https://www.connectedpapers.com/about",
        "https://www.litmaps.co/",
        "https://www.scite.ai/",
        "https://alphaxiv.org/",
        "https://www.catalyzex.com",
        "https://dagshub.com/",
        "http://gotit.pub/faq",
        "https://huggingface.co/huggingface",
        "https://paperswithcode.com/",
        "https://sciencecast.org/welcome",
        "https://replicate.com/docs/arxiv/about",
        "https://huggingface.co/docs/hub/spaces",
        "https://txyz.ai",
        "https://influencemap.cmlab.dev/",
        "https://core.ac.uk/services/recommender",
        "javascript:setMathjaxCookie()",
        "https://subscribe.sorryapp.com/24846f03/email/new",
        "https://subscribe.sorryapp.com/24846f03/slack/new"
    ]
}
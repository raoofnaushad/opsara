[Skip to content](#start-of-content)

## Navigation Menu

Toggle navigation

[ ](/)

[ Sign in ](/login?return_to=https%3A%2F%2Fgithub.com%2Fvllm-project%2Fvllm%2F)

  * Product 

    * [ GitHub Copilot Write better code with AI  ](https://github.com/features/copilot)
    * [ Security Find and fix vulnerabilities  ](https://github.com/features/security)
    * [ Actions Automate any workflow  ](https://github.com/features/actions)
    * [ Codespaces Instant dev environments  ](https://github.com/features/codespaces)
    * [ Issues Plan and track work  ](https://github.com/features/issues)
    * [ Code Review Manage code changes  ](https://github.com/features/code-review)
    * [ Discussions Collaborate outside of code  ](https://github.com/features/discussions)
    * [ Code Search Find more, search less  ](https://github.com/features/code-search)

Explore
    * [ All features ](https://github.com/features)
    * [ Documentation ](https://docs.github.com)
    * [ GitHub Skills ](https://skills.github.com)
    * [ Blog ](https://github.blog)

  * Solutions 

By company size
    * [ Enterprises ](https://github.com/enterprise)
    * [ Small and medium teams ](https://github.com/team)
    * [ Startups ](https://github.com/enterprise/startups)
    * [ Nonprofits ](/solutions/industry/nonprofits)

By use case
    * [ DevSecOps ](/solutions/use-case/devsecops)
    * [ DevOps ](/solutions/use-case/devops)
    * [ CI/CD ](/solutions/use-case/ci-cd)
    * [ View all use cases ](/solutions/use-case)

By industry
    * [ Healthcare ](/solutions/industry/healthcare)
    * [ Financial services ](/solutions/industry/financial-services)
    * [ Manufacturing ](/solutions/industry/manufacturing)
    * [ Government ](/solutions/industry/government)
    * [ View all industries ](/solutions/industry)

[ View all solutions ](/solutions)

  * Resources 

Topics
    * [ AI ](/resources/articles/ai)
    * [ DevOps ](/resources/articles/devops)
    * [ Security ](/resources/articles/security)
    * [ Software Development ](/resources/articles/software-development)
    * [ View all ](/resources/articles)

Explore
    * [ Learning Pathways ](https://resources.github.com/learn/pathways)
    * [ White papers, Ebooks, Webinars ](https://resources.github.com)
    * [ Customer Stories ](https://github.com/customer-stories)
    * [ Partners ](https://partner.github.com)
    * [ Executive Insights ](https://github.com/solutions/executive-insights)

  * Open Source 

    * [ GitHub Sponsors Fund open source developers  ](/sponsors)

    * [ The ReadME Project GitHub community articles  ](https://github.com/readme)

Repositories
    * [ Topics ](https://github.com/topics)
    * [ Trending ](https://github.com/trending)
    * [ Collections ](https://github.com/collections)

  * Enterprise 

    * [ Enterprise platform AI-powered developer platform  ](/enterprise)

Available add-ons
    * [ Advanced Security Enterprise-grade security features  ](https://github.com/enterprise/advanced-security)
    * [ GitHub Copilot Enterprise-grade AI features  ](/features/copilot#enterprise)
    * [ Premium Support Enterprise-grade 24/7 support  ](/premium-support)

  * [Pricing](https://github.com/pricing)



Search or jump to...

# Search code, repositories, users, issues, pull requests...

Search 

Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

#  Provide feedback 

We read every piece of feedback, and take your input very seriously.

Include my email address so I can be contacted

Cancel  Submit feedback 

#  Saved searches 

## Use saved searches to filter your results more quickly

Name

Query

To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax). 

Cancel  Create saved search 

[ Sign in ](/login?return_to=https%3A%2F%2Fgithub.com%2Fvllm-project%2Fvllm%2F)

[ Sign up ](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=vllm-project%2Fvllm) Reseting focus

You signed in with another tab or window. [Reload]() to refresh your session. You signed out in another tab or window. [Reload]() to refresh your session. You switched accounts on another tab or window. [Reload]() to refresh your session. Dismiss alert

{{ message }}

[ vllm-project ](/vllm-project) / **[vllm](/vllm-project/vllm) ** Public

  * Sponsor

#  Sponsor vllm-project/vllm 

  * [ Notifications ](/login?return_to=%2Fvllm-project%2Fvllm) You must be signed in to change notification settings
  * [ Fork 5.2k ](/login?return_to=%2Fvllm-project%2Fvllm)
  * [ Star  34k ](/login?return_to=%2Fvllm-project%2Fvllm)




A high-throughput and memory-efficient inference and serving engine for LLMs 

[docs.vllm.ai](https://docs.vllm.ai "https://docs.vllm.ai")

### License

[ Apache-2.0 license ](/vllm-project/vllm/blob/main/LICENSE)

[ 34k stars ](/vllm-project/vllm/stargazers) [ 5.2k forks ](/vllm-project/vllm/forks) [ Branches ](/vllm-project/vllm/branches) [ Tags ](/vllm-project/vllm/tags) [ Activity ](/vllm-project/vllm/activity)

[ Star  ](/login?return_to=%2Fvllm-project%2Fvllm)

[ Notifications ](/login?return_to=%2Fvllm-project%2Fvllm) You must be signed in to change notification settings

  * [ Code ](/vllm-project/vllm)
  * [ Issues 1.2k ](/vllm-project/vllm/issues)
  * [ Pull requests 458 ](/vllm-project/vllm/pulls)
  * [ Discussions ](/vllm-project/vllm/discussions)
  * [ Actions ](/vllm-project/vllm/actions)
  * [ Security ](/vllm-project/vllm/security)
  * [ Insights ](/vllm-project/vllm/pulse)



Additional navigation options

  * [ Code  ](/vllm-project/vllm)
  * [ Issues  ](/vllm-project/vllm/issues)
  * [ Pull requests  ](/vllm-project/vllm/pulls)
  * [ Discussions  ](/vllm-project/vllm/discussions)
  * [ Actions  ](/vllm-project/vllm/actions)
  * [ Security  ](/vllm-project/vllm/security)
  * [ Insights  ](/vllm-project/vllm/pulse)



# vllm-project/vllm

main

[**39** Branches](/vllm-project/vllm/branches)[**47** Tags](/vllm-project/vllm/tags)

[](/vllm-project/vllm/branches)[](/vllm-project/vllm/tags)

Go to file

Code

## Folders and files

Name| Name| Last commit message| Last commit date  
---|---|---|---  
  
## Latest commit

## History

[4,220 Commits](/vllm-project/vllm/commits/main/)[](/vllm-project/vllm/commits/main/)  
[.buildkite](/vllm-project/vllm/tree/main/.buildkite ".buildkite")| [.buildkite](/vllm-project/vllm/tree/main/.buildkite ".buildkite")  
[.github](/vllm-project/vllm/tree/main/.github ".github")| [.github](/vllm-project/vllm/tree/main/.github ".github")  
[benchmarks](/vllm-project/vllm/tree/main/benchmarks "benchmarks")| [benchmarks](/vllm-project/vllm/tree/main/benchmarks "benchmarks")  
[cmake](/vllm-project/vllm/tree/main/cmake "cmake")| [cmake](/vllm-project/vllm/tree/main/cmake "cmake")  
[csrc](/vllm-project/vllm/tree/main/csrc "csrc")| [csrc](/vllm-project/vllm/tree/main/csrc "csrc")  
[docs](/vllm-project/vllm/tree/main/docs "docs")| [docs](/vllm-project/vllm/tree/main/docs "docs")  
[examples](/vllm-project/vllm/tree/main/examples "examples")| [examples](/vllm-project/vllm/tree/main/examples "examples")  
[tests](/vllm-project/vllm/tree/main/tests "tests")| [tests](/vllm-project/vllm/tree/main/tests "tests")  
[tools](/vllm-project/vllm/tree/main/tools "tools")| [tools](/vllm-project/vllm/tree/main/tools "tools")  
[vllm](/vllm-project/vllm/tree/main/vllm "vllm")| [vllm](/vllm-project/vllm/tree/main/vllm "vllm")  
[.clang-format](/vllm-project/vllm/blob/main/.clang-format ".clang-format")| [.clang-format](/vllm-project/vllm/blob/main/.clang-format ".clang-format")  
[.dockerignore](/vllm-project/vllm/blob/main/.dockerignore ".dockerignore")| [.dockerignore](/vllm-project/vllm/blob/main/.dockerignore ".dockerignore")  
[.gitignore](/vllm-project/vllm/blob/main/.gitignore ".gitignore")| [.gitignore](/vllm-project/vllm/blob/main/.gitignore ".gitignore")  
[.pre-commit-config.yaml](/vllm-project/vllm/blob/main/.pre-commit-config.yaml ".pre-commit-config.yaml")| [.pre-commit-config.yaml](/vllm-project/vllm/blob/main/.pre-commit-config.yaml ".pre-commit-config.yaml")  
[.readthedocs.yaml](/vllm-project/vllm/blob/main/.readthedocs.yaml ".readthedocs.yaml")| [.readthedocs.yaml](/vllm-project/vllm/blob/main/.readthedocs.yaml ".readthedocs.yaml")  
[.shellcheckrc](/vllm-project/vllm/blob/main/.shellcheckrc ".shellcheckrc")| [.shellcheckrc](/vllm-project/vllm/blob/main/.shellcheckrc ".shellcheckrc")  
[.yapfignore](/vllm-project/vllm/blob/main/.yapfignore ".yapfignore")| [.yapfignore](/vllm-project/vllm/blob/main/.yapfignore ".yapfignore")  
[CMakeLists.txt](/vllm-project/vllm/blob/main/CMakeLists.txt "CMakeLists.txt")| [CMakeLists.txt](/vllm-project/vllm/blob/main/CMakeLists.txt "CMakeLists.txt")  
[CODE_OF_CONDUCT.md](/vllm-project/vllm/blob/main/CODE_OF_CONDUCT.md "CODE_OF_CONDUCT.md")| [CODE_OF_CONDUCT.md](/vllm-project/vllm/blob/main/CODE_OF_CONDUCT.md "CODE_OF_CONDUCT.md")  
[CONTRIBUTING.md](/vllm-project/vllm/blob/main/CONTRIBUTING.md "CONTRIBUTING.md")| [CONTRIBUTING.md](/vllm-project/vllm/blob/main/CONTRIBUTING.md "CONTRIBUTING.md")  
[DCO](/vllm-project/vllm/blob/main/DCO "DCO")| [DCO](/vllm-project/vllm/blob/main/DCO "DCO")  
[Dockerfile](/vllm-project/vllm/blob/main/Dockerfile "Dockerfile")| [Dockerfile](/vllm-project/vllm/blob/main/Dockerfile "Dockerfile")  
[Dockerfile.arm](/vllm-project/vllm/blob/main/Dockerfile.arm "Dockerfile.arm")| [Dockerfile.arm](/vllm-project/vllm/blob/main/Dockerfile.arm "Dockerfile.arm")  
[Dockerfile.cpu](/vllm-project/vllm/blob/main/Dockerfile.cpu "Dockerfile.cpu")| [Dockerfile.cpu](/vllm-project/vllm/blob/main/Dockerfile.cpu "Dockerfile.cpu")  
[Dockerfile.hpu](/vllm-project/vllm/blob/main/Dockerfile.hpu "Dockerfile.hpu")| [Dockerfile.hpu](/vllm-project/vllm/blob/main/Dockerfile.hpu "Dockerfile.hpu")  
[Dockerfile.neuron](/vllm-project/vllm/blob/main/Dockerfile.neuron "Dockerfile.neuron")| [Dockerfile.neuron](/vllm-project/vllm/blob/main/Dockerfile.neuron "Dockerfile.neuron")  
[Dockerfile.openvino](/vllm-project/vllm/blob/main/Dockerfile.openvino "Dockerfile.openvino")| [Dockerfile.openvino](/vllm-project/vllm/blob/main/Dockerfile.openvino "Dockerfile.openvino")  
[Dockerfile.ppc64le](/vllm-project/vllm/blob/main/Dockerfile.ppc64le "Dockerfile.ppc64le")| [Dockerfile.ppc64le](/vllm-project/vllm/blob/main/Dockerfile.ppc64le "Dockerfile.ppc64le")  
[Dockerfile.rocm](/vllm-project/vllm/blob/main/Dockerfile.rocm "Dockerfile.rocm")| [Dockerfile.rocm](/vllm-project/vllm/blob/main/Dockerfile.rocm "Dockerfile.rocm")  
[Dockerfile.tpu](/vllm-project/vllm/blob/main/Dockerfile.tpu "Dockerfile.tpu")| [Dockerfile.tpu](/vllm-project/vllm/blob/main/Dockerfile.tpu "Dockerfile.tpu")  
[Dockerfile.xpu](/vllm-project/vllm/blob/main/Dockerfile.xpu "Dockerfile.xpu")| [Dockerfile.xpu](/vllm-project/vllm/blob/main/Dockerfile.xpu "Dockerfile.xpu")  
[LICENSE](/vllm-project/vllm/blob/main/LICENSE "LICENSE")| [LICENSE](/vllm-project/vllm/blob/main/LICENSE "LICENSE")  
[MANIFEST.in](/vllm-project/vllm/blob/main/MANIFEST.in "MANIFEST.in")| [MANIFEST.in](/vllm-project/vllm/blob/main/MANIFEST.in "MANIFEST.in")  
[README.md](/vllm-project/vllm/blob/main/README.md "README.md")| [README.md](/vllm-project/vllm/blob/main/README.md "README.md")  
[SECURITY.md](/vllm-project/vllm/blob/main/SECURITY.md "SECURITY.md")| [SECURITY.md](/vllm-project/vllm/blob/main/SECURITY.md "SECURITY.md")  
[collect_env.py](/vllm-project/vllm/blob/main/collect_env.py "collect_env.py")| [collect_env.py](/vllm-project/vllm/blob/main/collect_env.py "collect_env.py")  
[find_cuda_init.py](/vllm-project/vllm/blob/main/find_cuda_init.py "find_cuda_init.py")| [find_cuda_init.py](/vllm-project/vllm/blob/main/find_cuda_init.py "find_cuda_init.py")  
[format.sh](/vllm-project/vllm/blob/main/format.sh "format.sh")| [format.sh](/vllm-project/vllm/blob/main/format.sh "format.sh")  
[pyproject.toml](/vllm-project/vllm/blob/main/pyproject.toml "pyproject.toml")| [pyproject.toml](/vllm-project/vllm/blob/main/pyproject.toml "pyproject.toml")  
[python_only_dev.py](/vllm-project/vllm/blob/main/python_only_dev.py "python_only_dev.py")| [python_only_dev.py](/vllm-project/vllm/blob/main/python_only_dev.py "python_only_dev.py")  
[requirements-build.txt](/vllm-project/vllm/blob/main/requirements-build.txt "requirements-build.txt")| [requirements-build.txt](/vllm-project/vllm/blob/main/requirements-build.txt "requirements-build.txt")  
[requirements-common.txt](/vllm-project/vllm/blob/main/requirements-common.txt "requirements-common.txt")| [requirements-common.txt](/vllm-project/vllm/blob/main/requirements-common.txt "requirements-common.txt")  
[requirements-cpu.txt](/vllm-project/vllm/blob/main/requirements-cpu.txt "requirements-cpu.txt")| [requirements-cpu.txt](/vllm-project/vllm/blob/main/requirements-cpu.txt "requirements-cpu.txt")  
[requirements-cuda.txt](/vllm-project/vllm/blob/main/requirements-cuda.txt "requirements-cuda.txt")| [requirements-cuda.txt](/vllm-project/vllm/blob/main/requirements-cuda.txt "requirements-cuda.txt")  
[requirements-dev.txt](/vllm-project/vllm/blob/main/requirements-dev.txt "requirements-dev.txt")| [requirements-dev.txt](/vllm-project/vllm/blob/main/requirements-dev.txt "requirements-dev.txt")  
[requirements-hpu.txt](/vllm-project/vllm/blob/main/requirements-hpu.txt "requirements-hpu.txt")| [requirements-hpu.txt](/vllm-project/vllm/blob/main/requirements-hpu.txt "requirements-hpu.txt")  
[requirements-lint.txt](/vllm-project/vllm/blob/main/requirements-lint.txt "requirements-lint.txt")| [requirements-lint.txt](/vllm-project/vllm/blob/main/requirements-lint.txt "requirements-lint.txt")  
[requirements-neuron.txt](/vllm-project/vllm/blob/main/requirements-neuron.txt "requirements-neuron.txt")| [requirements-neuron.txt](/vllm-project/vllm/blob/main/requirements-neuron.txt "requirements-neuron.txt")  
[requirements-openvino.txt](/vllm-project/vllm/blob/main/requirements-openvino.txt "requirements-openvino.txt")| [requirements-openvino.txt](/vllm-project/vllm/blob/main/requirements-openvino.txt "requirements-openvino.txt")  
[requirements-rocm.txt](/vllm-project/vllm/blob/main/requirements-rocm.txt "requirements-rocm.txt")| [requirements-rocm.txt](/vllm-project/vllm/blob/main/requirements-rocm.txt "requirements-rocm.txt")  
[requirements-test.in](/vllm-project/vllm/blob/main/requirements-test.in "requirements-test.in")| [requirements-test.in](/vllm-project/vllm/blob/main/requirements-test.in "requirements-test.in")  
[requirements-test.txt](/vllm-project/vllm/blob/main/requirements-test.txt "requirements-test.txt")| [requirements-test.txt](/vllm-project/vllm/blob/main/requirements-test.txt "requirements-test.txt")  
[requirements-tpu.txt](/vllm-project/vllm/blob/main/requirements-tpu.txt "requirements-tpu.txt")| [requirements-tpu.txt](/vllm-project/vllm/blob/main/requirements-tpu.txt "requirements-tpu.txt")  
[requirements-xpu.txt](/vllm-project/vllm/blob/main/requirements-xpu.txt "requirements-xpu.txt")| [requirements-xpu.txt](/vllm-project/vllm/blob/main/requirements-xpu.txt "requirements-xpu.txt")  
[setup.py](/vllm-project/vllm/blob/main/setup.py "setup.py")| [setup.py](/vllm-project/vllm/blob/main/setup.py "setup.py")  
[use_existing_torch.py](/vllm-project/vllm/blob/main/use_existing_torch.py "use_existing_torch.py")| [use_existing_torch.py](/vllm-project/vllm/blob/main/use_existing_torch.py "use_existing_torch.py")  
View all files  
  
## Repository files navigation

  * [README](#)
  * [Code of conduct](#)
  * [Apache-2.0 license](#)
  * [Security](#)



![vLLM](https://raw.githubusercontent.com/vllm-project/vllm/main/docs/source/assets/logos/vllm-logo-text-light.png)

###  Easy, fast, and cheap LLM serving for everyone 

[](#easy-fast-and-cheap-llm-serving-for-everyone)

| [**Documentation**](https://docs.vllm.ai) | [**Blog**](https://vllm.ai) | [**Paper**](https://arxiv.org/abs/2309.06180) | [**Discord**](https://discord.gg/jz7wjKhh6g) | [**Twitter/X**](https://x.com/vllm_project) | [**Developer Slack**](https://slack.vllm.ai) | 

The first vLLM meetup in 2025 is happening on January 22nd, Wednesday, with Google Cloud in San Francisco! We will talk about vLLM's performant V1 architecture, Q1 roadmap, Google Cloud's innovation around vLLM: networking, Cloud Run, Vertex, and TPU! [Register Now](https://lu.ma/zep56hui)

_Latest News_ ðŸ”¥

  * [2024/12] vLLM joins [pytorch ecosystem](https://pytorch.org/blog/vllm-joins-pytorch)! Easy, Fast, and Cheap LLM Serving for Everyone!
  * [2024/11] We hosted [the seventh vLLM meetup](https://lu.ma/h0qvrajz) with Snowflake! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1e3CxQBV3JsfGp30SwyvS3eM_tW-ghOhJ9PAJGK6KR54/edit?usp=sharing), and Snowflake team [here](https://docs.google.com/presentation/d/1qF3RkDAbOULwz9WK5TOltt2fE9t6uIc_hVNLFAaQX6A/edit?usp=sharing).
  * [2024/10] We have just created a developer slack ([slack.vllm.ai](https://slack.vllm.ai)) focusing on coordinating contributions and discussing features. Please feel free to join us there!
  * [2024/10] Ray Summit 2024 held a special track for vLLM! Please find the opening talk slides from the vLLM team [here](https://docs.google.com/presentation/d/1B_KQxpHBTRa_mDF-tR6i8rWdOU5QoTZNcEg2MKZxEHM/edit?usp=sharing). Learn more from the [talks](https://www.youtube.com/playlist?list=PLzTswPQNepXl6AQwifuwUImLPFRVpksjR) from other vLLM contributors and users!
  * [2024/09] We hosted [the sixth vLLM meetup](https://lu.ma/87q3nvnh) with NVIDIA! Please find the meetup slides [here](https://docs.google.com/presentation/d/1wrLGwytQfaOTd5wCGSPNhoaW3nq0E-9wqyP7ny93xRs/edit?usp=sharing).
  * [2024/07] We hosted [the fifth vLLM meetup](https://lu.ma/lp0gyjqr) with AWS! Please find the meetup slides [here](https://docs.google.com/presentation/d/1RgUD8aCfcHocghoP3zmXzck9vX3RCI9yfUAB2Bbcl4Y/edit?usp=sharing).
  * [2024/07] In partnership with Meta, vLLM officially supports Llama 3.1 with FP8 quantization and pipeline parallelism! Please check out our blog post [here](https://blog.vllm.ai/2024/07/23/llama31.html).
  * [2024/06] We hosted [the fourth vLLM meetup](https://lu.ma/agivllm) with Cloudflare and BentoML! Please find the meetup slides [here](https://docs.google.com/presentation/d/1iJ8o7V2bQEi0BFEljLTwc5G1S10_Rhv3beed5oB0NJ4/edit?usp=sharing).
  * [2024/04] We hosted [the third vLLM meetup](https://robloxandvllmmeetup2024.splashthat.com/) with Roblox! Please find the meetup slides [here](https://docs.google.com/presentation/d/1A--47JAK4BJ39t954HyTkvtfwn0fkqtsL8NGFuslReM/edit?usp=sharing).
  * [2024/01] We hosted [the second vLLM meetup](https://lu.ma/ygxbpzhl) with IBM! Please find the meetup slides [here](https://docs.google.com/presentation/d/12mI2sKABnUw5RBWXDYY-HtHth4iMSNcEoQ10jDQbxgA/edit?usp=sharing).
  * [2023/10] We hosted [the first vLLM meetup](https://lu.ma/first-vllm-meetup) with a16z! Please find the meetup slides [here](https://docs.google.com/presentation/d/1QL-XPFXiFpDBh86DbEegFXBXFXjix4v032GhShbKf3s/edit?usp=sharing).
  * [2023/08] We would like to express our sincere gratitude to [Andreessen Horowitz](https://a16z.com/2023/08/30/supporting-the-open-source-ai-community/) (a16z) for providing a generous grant to support the open-source development and research of vLLM.
  * [2023/06] We officially released vLLM! FastChat-vLLM integration has powered [LMSYS Vicuna and Chatbot Arena](https://chat.lmsys.org) since mid-April. Check out our [blog post](https://vllm.ai).



## About

[](#about)

vLLM is a fast and easy-to-use library for LLM inference and serving.

Originally developed in the [Sky Computing Lab](https://sky.cs.berkeley.edu) at UC Berkeley, vLLM has evloved into a community-driven project with contributions from both academia and industry.

vLLM is fast with:

  * State-of-the-art serving throughput
  * Efficient management of attention key and value memory with [**PagedAttention**](https://blog.vllm.ai/2023/06/20/vllm.html)
  * Continuous batching of incoming requests
  * Fast model execution with CUDA/HIP graph
  * Quantizations: [GPTQ](https://arxiv.org/abs/2210.17323), [AWQ](https://arxiv.org/abs/2306.00978), INT4, INT8, and FP8.
  * Optimized CUDA kernels, including integration with FlashAttention and FlashInfer.
  * Speculative decoding
  * Chunked prefill



**Performance benchmark** : We include a performance benchmark at the end of [our blog post](https://blog.vllm.ai/2024/09/05/perf-update.html). It compares the performance of vLLM against other LLM serving engines ([TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM), [SGLang](https://github.com/sgl-project/sglang) and [LMDeploy](https://github.com/InternLM/lmdeploy)). The implementation is under [nightly-benchmarks folder](/vllm-project/vllm/blob/main/.buildkite/nightly-benchmarks) and you can [reproduce](https://github.com/vllm-project/vllm/issues/8176) this benchmark using our one-click runnable script.

vLLM is flexible and easy to use with:

  * Seamless integration with popular Hugging Face models
  * High-throughput serving with various decoding algorithms, including _parallel sampling_ , _beam search_ , and more
  * Tensor parallelism and pipeline parallelism support for distributed inference
  * Streaming outputs
  * OpenAI-compatible API server
  * Support NVIDIA GPUs, AMD CPUs and GPUs, Intel CPUs and GPUs, PowerPC CPUs, TPU, and AWS Neuron.
  * Prefix caching support
  * Multi-lora support



vLLM seamlessly supports most popular open-source models on HuggingFace, including:

  * Transformer-like LLMs (e.g., Llama)
  * Mixture-of-Expert LLMs (e.g., Mixtral, Deepseek-V2 and V3)
  * Embedding Models (e.g. E5-Mistral)
  * Multi-modal LLMs (e.g., LLaVA)



Find the full list of supported models [here](https://docs.vllm.ai/en/latest/models/supported_models.html).

## Getting Started

[](#getting-started)

Install vLLM with `pip` or [from source](https://docs.vllm.ai/en/latest/getting_started/installation/gpu/index.html#build-wheel-from-source):

```
pip install vllm
```

Visit our [documentation](https://docs.vllm.ai/en/latest/) to learn more.

  * [Installation](https://docs.vllm.ai/en/latest/getting_started/installation/index.html)
  * [Quickstart](https://docs.vllm.ai/en/latest/getting_started/quickstart.html)
  * [List of Supported Models](https://docs.vllm.ai/en/latest/models/supported_models.html)



## Contributing

[](#contributing)

We welcome and value any contributions and collaborations. Please check out [CONTRIBUTING.md](/vllm-project/vllm/blob/main/CONTRIBUTING.md) for how to get involved.

## Sponsors

[](#sponsors)

vLLM is a community project. Our compute resources for development and testing are supported by the following organizations. Thank you for your support!

Cash Donations:

  * a16z
  * Dropbox
  * Sequoia Capital
  * Skywork AI
  * ZhenFund



Compute Resources:

  * AMD
  * Anyscale
  * AWS
  * Crusoe Cloud
  * Databricks
  * DeepInfra
  * Google Cloud
  * Lambda Lab
  * Nebius
  * Novita AI
  * NVIDIA
  * Replicate
  * Roblox
  * RunPod
  * Trainy
  * UC Berkeley
  * UC San Diego



Slack Sponsor: Anyscale

We also have an official fundraising venue through [OpenCollective](https://opencollective.com/vllm). We plan to use the fund to support the development, maintenance, and adoption of vLLM.

## Citation

[](#citation)

If you use vLLM for your research, please cite our [paper](https://arxiv.org/abs/2309.06180):

```
@inproceedings{kwon2023efficient, title={Efficient Memory Management for Large Language Model Serving with PagedAttention}, author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica}, booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles}, year={2023} }
```

## Contact Us

[](#contact-us)

  * For technical questions and feature requests, please use Github issues or discussions.
  * For discussing with fellow users, please use Discord.
  * For coordinating contributions and development, please use Slack.
  * For security disclosures, please use Github's security advisory feature.
  * For collaborations and partnerships, please contact us at vllm-questions AT lists.berkeley.edu.



## Media Kit

[](#media-kit)

  * If you wish to use vLLM's logo, please refer to [our media kit repo](https://github.com/vllm-project/media-kit).



## About

A high-throughput and memory-efficient inference and serving engine for LLMs 

[docs.vllm.ai](https://docs.vllm.ai "https://docs.vllm.ai")

### Topics

[ amd ](/topics/amd "Topic: amd") [ cuda ](/topics/cuda "Topic: cuda") [ inference ](/topics/inference "Topic: inference") [ pytorch ](/topics/pytorch "Topic: pytorch") [ transformer ](/topics/transformer "Topic: transformer") [ llama ](/topics/llama "Topic: llama") [ gpt ](/topics/gpt "Topic: gpt") [ rocm ](/topics/rocm "Topic: rocm") [ model-serving ](/topics/model-serving "Topic: model-serving") [ tpu ](/topics/tpu "Topic: tpu") [ hpu ](/topics/hpu "Topic: hpu") [ mlops ](/topics/mlops "Topic: mlops") [ xpu ](/topics/xpu "Topic: xpu") [ llm ](/topics/llm "Topic: llm") [ inferentia ](/topics/inferentia "Topic: inferentia") [ llmops ](/topics/llmops "Topic: llmops") [ llm-serving ](/topics/llm-serving "Topic: llm-serving") [ trainium ](/topics/trainium "Topic: trainium")

### Resources

[ Readme ](#readme-ov-file)

### License

[ Apache-2.0 license ](#Apache-2.0-1-ov-file)

### Code of conduct

[ Code of conduct ](#coc-ov-file)

### Security policy

[ Security policy ](#security-ov-file)

[ Activity](/vllm-project/vllm/activity)

[ Custom properties](/vllm-project/vllm/custom-properties)

### Stars

[ **34k** stars](/vllm-project/vllm/stargazers)

### Watchers

[ **280** watching](/vllm-project/vllm/watchers)

### Forks

[ **5.2k** forks](/vllm-project/vllm/forks)

[ Report repository ](/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fvllm-project%2Fvllm&report=vllm-project+%28user%29)

##  [Releases 46](/vllm-project/vllm/releases)

[ v0.6.6.post1 Latest  Dec 27, 2024 ](/vllm-project/vllm/releases/tag/v0.6.6.post1)

[+ 45 releases](/vllm-project/vllm/releases)

## Sponsor this project

  * [ ![@vllm-project](https://avatars.githubusercontent.com/u/136984999?s=64&v=4) ](/vllm-project) [ **vllm-project** vLLM ](/vllm-project) [ ](/sponsors/vllm-project)


  * ![open_collective](https://github.githubassets.com/assets/open_collective-0a706523753d.svg) [opencollective.com/**vllm**](https://opencollective.com/vllm)



[Learn more about GitHub Sponsors](/sponsors)

##  [Used by 2.8k](/vllm-project/vllm/network/dependents)

[

  * ![@Indoxer](https://avatars.githubusercontent.com/u/77686332?s=64&v=4)
  * ![@microsoft](https://avatars.githubusercontent.com/u/6154722?s=64&v=4)
  * ![@niminim](https://avatars.githubusercontent.com/u/12481619?s=64&v=4)
  * ![@lanad01](https://avatars.githubusercontent.com/u/62043299?s=64&v=4)
  * ![@narenp12](https://avatars.githubusercontent.com/u/146764727?s=64&v=4)
  * ![@mmrech](https://avatars.githubusercontent.com/u/137358704?s=64&v=4)
  * ![@mmrech](https://avatars.githubusercontent.com/u/137358704?s=64&v=4)
  * ![@SilexDataTeam](https://avatars.githubusercontent.com/u/109674383?s=64&v=4)

+ 2,816  ](/vllm-project/vllm/network/dependents)

##  [Contributors 788](/vllm-project/vllm/graphs/contributors)

  * [ ![@WoosukKwon](https://avatars.githubusercontent.com/u/46394894?s=64&v=4) ](https://github.com/WoosukKwon)
  * [ ![@youkaichao](https://avatars.githubusercontent.com/u/23236638?s=64&v=4) ](https://github.com/youkaichao)
  * [ ![@DarkLight1337](https://avatars.githubusercontent.com/u/44970335?s=64&v=4) ](https://github.com/DarkLight1337)
  * [ ![@mgoin](https://avatars.githubusercontent.com/u/3195154?s=64&v=4) ](https://github.com/mgoin)
  * [ ![@simon-mo](https://avatars.githubusercontent.com/u/21118851?s=64&v=4) ](https://github.com/simon-mo)
  * [ ![@ywang96](https://avatars.githubusercontent.com/u/136131678?s=64&v=4) ](https://github.com/ywang96)
  * [ ![@zhuohan123](https://avatars.githubusercontent.com/u/17310766?s=64&v=4) ](https://github.com/zhuohan123)
  * [ ![@Isotr0py](https://avatars.githubusercontent.com/u/41363108?s=64&v=4) ](https://github.com/Isotr0py)
  * [ ![@njhill](https://avatars.githubusercontent.com/u/16958488?s=64&v=4) ](https://github.com/njhill)
  * [ ![@robertgshaw2-redhat](https://avatars.githubusercontent.com/u/114415538?s=64&v=4) ](https://github.com/robertgshaw2-redhat)
  * [ ![@Yard1](https://avatars.githubusercontent.com/u/10364161?s=64&v=4) ](https://github.com/Yard1)
  * [ ![@tlrmchlsmth](https://avatars.githubusercontent.com/u/1236979?s=64&v=4) ](https://github.com/tlrmchlsmth)
  * [ ![@jeejeelee](https://avatars.githubusercontent.com/u/19733142?s=64&v=4) ](https://github.com/jeejeelee)
  * [ ![@comaniac](https://avatars.githubusercontent.com/u/8262694?s=64&v=4) ](https://github.com/comaniac)



[+ 774 contributors](/vllm-project/vllm/graphs/contributors)

## Languages

  * [ Python 84.2% ](/vllm-project/vllm/search?l=python)
  * [ Cuda 10.8% ](/vllm-project/vllm/search?l=cuda)
  * [ C++ 3.1% ](/vllm-project/vllm/search?l=c%2B%2B)
  * [ C 0.8% ](/vllm-project/vllm/search?l=c)
  * [ Shell 0.6% ](/vllm-project/vllm/search?l=shell)
  * [ CMake 0.4% ](/vllm-project/vllm/search?l=cmake)
  * [ Dockerfile 0.1% ](/vllm-project/vllm/search?l=dockerfile)



## Footer

[ ](https://github.com "GitHub") Â© 2025 GitHub, Inc. 

### Footer navigation

  * [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
  * [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
  * [Security](https://github.com/security)
  * [Status](https://www.githubstatus.com/)
  * [Docs](https://docs.github.com/)
  * [Contact](https://support.github.com?tags=dotcom-footer)
  * Manage cookies 
  * Do not share my personal information 



You canâ€™t perform that action at this time. 

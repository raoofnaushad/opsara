{
    "id": "fb6fef6b4b4ecccffdae4d1abea359ac",
    "metadata": {
        "id": "fb6fef6b4b4ecccffdae4d1abea359ac",
        "url": "https://github.com/vllm-project/vllm/",
        "title": "GitHub - vllm-project/vllm: A high-throughput and memory-efficient inference and serving engine for LLMs",
        "properties": {
            "description": "A high-throughput and memory-efficient inference and serving engine for LLMs - vllm-project/vllm",
            "keywords": null,
            "author": null,
            "og:image": "https://opengraph.githubassets.com/3ed7bd5168bbc18103aa738a6ea6161e3659a4409a5739bb4c527ca32e37d4c3/vllm-project/vllm",
            "og:image:alt": "A high-throughput and memory-efficient inference and serving engine for LLMs - vllm-project/vllm",
            "og:image:width": "1200",
            "og:image:height": "600",
            "og:site_name": "GitHub",
            "og:type": "object",
            "og:title": "GitHub - vllm-project/vllm: A high-throughput and memory-efficient inference and serving engine for LLMs",
            "og:url": "https://github.com/vllm-project/vllm",
            "og:description": "A high-throughput and memory-efficient inference and serving engine for LLMs - vllm-project/vllm",
            "twitter:image": "https://opengraph.githubassets.com/3ed7bd5168bbc18103aa738a6ea6161e3659a4409a5739bb4c527ca32e37d4c3/vllm-project/vllm",
            "twitter:site": "@github",
            "twitter:card": "summary_large_image",
            "twitter:title": "GitHub - vllm-project/vllm: A high-throughput and memory-efficient inference and serving engine for LLMs",
            "twitter:description": "A high-throughput and memory-efficient inference and serving engine for LLMs - vllm-project/vllm"
        }
    },
    "parent_metadata": {
        "id": "bfecefaaad83e3f79ba0efc2df4ef6e7",
        "url": "https://www.notion.so/Inference-Engines-bfecefaaad83e3f79ba0efc2df4ef6e7",
        "title": "Inference Engines",
        "properties": {
            "Type": "Leaf"
        }
    },
    "content": "[Skip to content](#start-of-content)\n\n## Navigation Menu\n\nToggle navigation\n\n[ ](/)\n\n[ Sign in ](/login?return_to=https%3A%2F%2Fgithub.com%2Fvllm-project%2Fvllm%2F)\n\n  * Product \n\n    * [ GitHub Copilot Write better code with AI  ](https://github.com/features/copilot)\n    * [ Security Find and fix vulnerabilities  ](https://github.com/features/security)\n    * [ Actions Automate any workflow  ](https://github.com/features/actions)\n    * [ Codespaces Instant dev environments  ](https://github.com/features/codespaces)\n    * [ Issues Plan and track work  ](https://github.com/features/issues)\n    * [ Code Review Manage code changes  ](https://github.com/features/code-review)\n    * [ Discussions Collaborate outside of code  ](https://github.com/features/discussions)\n    * [ Code Search Find more, search less  ](https://github.com/features/code-search)\n\nExplore\n    * [ All features ](https://github.com/features)\n    * [ Documentation ](https://docs.github.com)\n    * [ GitHub Skills ](https://skills.github.com)\n    * [ Blog ](https://github.blog)\n\n  * Solutions \n\nBy company size\n    * [ Enterprises ](https://github.com/enterprise)\n    * [ Small and medium teams ](https://github.com/team)\n    * [ Startups ](https://github.com/enterprise/startups)\n    * [ Nonprofits ](/solutions/industry/nonprofits)\n\nBy use case\n    * [ DevSecOps ](/solutions/use-case/devsecops)\n    * [ DevOps ](/solutions/use-case/devops)\n    * [ CI/CD ](/solutions/use-case/ci-cd)\n    * [ View all use cases ](/solutions/use-case)\n\nBy industry\n    * [ Healthcare ](/solutions/industry/healthcare)\n    * [ Financial services ](/solutions/industry/financial-services)\n    * [ Manufacturing ](/solutions/industry/manufacturing)\n    * [ Government ](/solutions/industry/government)\n    * [ View all industries ](/solutions/industry)\n\n[ View all solutions ](/solutions)\n\n  * Resources \n\nTopics\n    * [ AI ](/resources/articles/ai)\n    * [ DevOps ](/resources/articles/devops)\n    * [ Security ](/resources/articles/security)\n    * [ Software Development ](/resources/articles/software-development)\n    * [ View all ](/resources/articles)\n\nExplore\n    * [ Learning Pathways ](https://resources.github.com/learn/pathways)\n    * [ White papers, Ebooks, Webinars ](https://resources.github.com)\n    * [ Customer Stories ](https://github.com/customer-stories)\n    * [ Partners ](https://partner.github.com)\n    * [ Executive Insights ](https://github.com/solutions/executive-insights)\n\n  * Open Source \n\n    * [ GitHub Sponsors Fund open source developers  ](/sponsors)\n\n    * [ The ReadME Project GitHub community articles  ](https://github.com/readme)\n\nRepositories\n    * [ Topics ](https://github.com/topics)\n    * [ Trending ](https://github.com/trending)\n    * [ Collections ](https://github.com/collections)\n\n  * Enterprise \n\n    * [ Enterprise platform AI-powered developer platform  ](/enterprise)\n\nAvailable add-ons\n    * [ Advanced Security Enterprise-grade security features  ](https://github.com/enterprise/advanced-security)\n    * [ GitHub Copilot Enterprise-grade AI features  ](/features/copilot#enterprise)\n    * [ Premium Support Enterprise-grade 24/7 support  ](/premium-support)\n\n  * [Pricing](https://github.com/pricing)\n\n\n\nSearch or jump to...\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch \n\nClear\n\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n\n#  Provide feedback \n\nWe read every piece of feedback, and take your input very seriously.\n\nInclude my email address so I can be contacted\n\nCancel  Submit feedback \n\n#  Saved searches \n\n## Use saved searches to filter your results more quickly\n\nName\n\nQuery\n\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax). \n\nCancel  Create saved search \n\n[ Sign in ](/login?return_to=https%3A%2F%2Fgithub.com%2Fvllm-project%2Fvllm%2F)\n\n[ Sign up ](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=vllm-project%2Fvllm) Reseting focus\n\nYou signed in with another tab or window. [Reload]() to refresh your session. You signed out in another tab or window. [Reload]() to refresh your session. You switched accounts on another tab or window. [Reload]() to refresh your session. Dismiss alert\n\n{{ message }}\n\n[ vllm-project ](/vllm-project) / **[vllm](/vllm-project/vllm) ** Public\n\n  * Sponsor\n\n#  Sponsor vllm-project/vllm \n\n  * [ Notifications ](/login?return_to=%2Fvllm-project%2Fvllm) You must be signed in to change notification settings\n  * [ Fork 5.2k ](/login?return_to=%2Fvllm-project%2Fvllm)\n  * [ Star  34k ](/login?return_to=%2Fvllm-project%2Fvllm)\n\n\n\n\nA high-throughput and memory-efficient inference and serving engine for LLMs \n\n[docs.vllm.ai](https://docs.vllm.ai \"https://docs.vllm.ai\")\n\n### License\n\n[ Apache-2.0 license ](/vllm-project/vllm/blob/main/LICENSE)\n\n[ 34k stars ](/vllm-project/vllm/stargazers) [ 5.2k forks ](/vllm-project/vllm/forks) [ Branches ](/vllm-project/vllm/branches) [ Tags ](/vllm-project/vllm/tags) [ Activity ](/vllm-project/vllm/activity)\n\n[ Star  ](/login?return_to=%2Fvllm-project%2Fvllm)\n\n[ Notifications ](/login?return_to=%2Fvllm-project%2Fvllm) You must be signed in to change notification settings\n\n  * [ Code ](/vllm-project/vllm)\n  * [ Issues 1.2k ](/vllm-project/vllm/issues)\n  * [ Pull requests 458 ](/vllm-project/vllm/pulls)\n  * [ Discussions ](/vllm-project/vllm/discussions)\n  * [ Actions ](/vllm-project/vllm/actions)\n  * [ Security ](/vllm-project/vllm/security)\n  * [ Insights ](/vllm-project/vllm/pulse)\n\n\n\nAdditional navigation options\n\n  * [ Code  ](/vllm-project/vllm)\n  * [ Issues  ](/vllm-project/vllm/issues)\n  * [ Pull requests  ](/vllm-project/vllm/pulls)\n  * [ Discussions  ](/vllm-project/vllm/discussions)\n  * [ Actions  ](/vllm-project/vllm/actions)\n  * [ Security  ](/vllm-project/vllm/security)\n  * [ Insights  ](/vllm-project/vllm/pulse)\n\n\n\n# vllm-project/vllm\n\nmain\n\n[**39** Branches](/vllm-project/vllm/branches)[**47** Tags](/vllm-project/vllm/tags)\n\n[](/vllm-project/vllm/branches)[](/vllm-project/vllm/tags)\n\nGo to file\n\nCode\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\n## History\n\n[4,220 Commits](/vllm-project/vllm/commits/main/)[](/vllm-project/vllm/commits/main/)  \n[.buildkite](/vllm-project/vllm/tree/main/.buildkite \".buildkite\")| [.buildkite](/vllm-project/vllm/tree/main/.buildkite \".buildkite\")  \n[.github](/vllm-project/vllm/tree/main/.github \".github\")| [.github](/vllm-project/vllm/tree/main/.github \".github\")  \n[benchmarks](/vllm-project/vllm/tree/main/benchmarks \"benchmarks\")| [benchmarks](/vllm-project/vllm/tree/main/benchmarks \"benchmarks\")  \n[cmake](/vllm-project/vllm/tree/main/cmake \"cmake\")| [cmake](/vllm-project/vllm/tree/main/cmake \"cmake\")  \n[csrc](/vllm-project/vllm/tree/main/csrc \"csrc\")| [csrc](/vllm-project/vllm/tree/main/csrc \"csrc\")  \n[docs](/vllm-project/vllm/tree/main/docs \"docs\")| [docs](/vllm-project/vllm/tree/main/docs \"docs\")  \n[examples](/vllm-project/vllm/tree/main/examples \"examples\")| [examples](/vllm-project/vllm/tree/main/examples \"examples\")  \n[tests](/vllm-project/vllm/tree/main/tests \"tests\")| [tests](/vllm-project/vllm/tree/main/tests \"tests\")  \n[tools](/vllm-project/vllm/tree/main/tools \"tools\")| [tools](/vllm-project/vllm/tree/main/tools \"tools\")  \n[vllm](/vllm-project/vllm/tree/main/vllm \"vllm\")| [vllm](/vllm-project/vllm/tree/main/vllm \"vllm\")  \n[.clang-format](/vllm-project/vllm/blob/main/.clang-format \".clang-format\")| [.clang-format](/vllm-project/vllm/blob/main/.clang-format \".clang-format\")  \n[.dockerignore](/vllm-project/vllm/blob/main/.dockerignore \".dockerignore\")| [.dockerignore](/vllm-project/vllm/blob/main/.dockerignore \".dockerignore\")  \n[.gitignore](/vllm-project/vllm/blob/main/.gitignore \".gitignore\")| [.gitignore](/vllm-project/vllm/blob/main/.gitignore \".gitignore\")  \n[.pre-commit-config.yaml](/vllm-project/vllm/blob/main/.pre-commit-config.yaml \".pre-commit-config.yaml\")| [.pre-commit-config.yaml](/vllm-project/vllm/blob/main/.pre-commit-config.yaml \".pre-commit-config.yaml\")  \n[.readthedocs.yaml](/vllm-project/vllm/blob/main/.readthedocs.yaml \".readthedocs.yaml\")| [.readthedocs.yaml](/vllm-project/vllm/blob/main/.readthedocs.yaml \".readthedocs.yaml\")  \n[.shellcheckrc](/vllm-project/vllm/blob/main/.shellcheckrc \".shellcheckrc\")| [.shellcheckrc](/vllm-project/vllm/blob/main/.shellcheckrc \".shellcheckrc\")  \n[.yapfignore](/vllm-project/vllm/blob/main/.yapfignore \".yapfignore\")| [.yapfignore](/vllm-project/vllm/blob/main/.yapfignore \".yapfignore\")  \n[CMakeLists.txt](/vllm-project/vllm/blob/main/CMakeLists.txt \"CMakeLists.txt\")| [CMakeLists.txt](/vllm-project/vllm/blob/main/CMakeLists.txt \"CMakeLists.txt\")  \n[CODE_OF_CONDUCT.md](/vllm-project/vllm/blob/main/CODE_OF_CONDUCT.md \"CODE_OF_CONDUCT.md\")| [CODE_OF_CONDUCT.md](/vllm-project/vllm/blob/main/CODE_OF_CONDUCT.md \"CODE_OF_CONDUCT.md\")  \n[CONTRIBUTING.md](/vllm-project/vllm/blob/main/CONTRIBUTING.md \"CONTRIBUTING.md\")| [CONTRIBUTING.md](/vllm-project/vllm/blob/main/CONTRIBUTING.md \"CONTRIBUTING.md\")  \n[DCO](/vllm-project/vllm/blob/main/DCO \"DCO\")| [DCO](/vllm-project/vllm/blob/main/DCO \"DCO\")  \n[Dockerfile](/vllm-project/vllm/blob/main/Dockerfile \"Dockerfile\")| [Dockerfile](/vllm-project/vllm/blob/main/Dockerfile \"Dockerfile\")  \n[Dockerfile.arm](/vllm-project/vllm/blob/main/Dockerfile.arm \"Dockerfile.arm\")| [Dockerfile.arm](/vllm-project/vllm/blob/main/Dockerfile.arm \"Dockerfile.arm\")  \n[Dockerfile.cpu](/vllm-project/vllm/blob/main/Dockerfile.cpu \"Dockerfile.cpu\")| [Dockerfile.cpu](/vllm-project/vllm/blob/main/Dockerfile.cpu \"Dockerfile.cpu\")  \n[Dockerfile.hpu](/vllm-project/vllm/blob/main/Dockerfile.hpu \"Dockerfile.hpu\")| [Dockerfile.hpu](/vllm-project/vllm/blob/main/Dockerfile.hpu \"Dockerfile.hpu\")  \n[Dockerfile.neuron](/vllm-project/vllm/blob/main/Dockerfile.neuron \"Dockerfile.neuron\")| [Dockerfile.neuron](/vllm-project/vllm/blob/main/Dockerfile.neuron \"Dockerfile.neuron\")  \n[Dockerfile.openvino](/vllm-project/vllm/blob/main/Dockerfile.openvino \"Dockerfile.openvino\")| [Dockerfile.openvino](/vllm-project/vllm/blob/main/Dockerfile.openvino \"Dockerfile.openvino\")  \n[Dockerfile.ppc64le](/vllm-project/vllm/blob/main/Dockerfile.ppc64le \"Dockerfile.ppc64le\")| [Dockerfile.ppc64le](/vllm-project/vllm/blob/main/Dockerfile.ppc64le \"Dockerfile.ppc64le\")  \n[Dockerfile.rocm](/vllm-project/vllm/blob/main/Dockerfile.rocm \"Dockerfile.rocm\")| [Dockerfile.rocm](/vllm-project/vllm/blob/main/Dockerfile.rocm \"Dockerfile.rocm\")  \n[Dockerfile.tpu](/vllm-project/vllm/blob/main/Dockerfile.tpu \"Dockerfile.tpu\")| [Dockerfile.tpu](/vllm-project/vllm/blob/main/Dockerfile.tpu \"Dockerfile.tpu\")  \n[Dockerfile.xpu](/vllm-project/vllm/blob/main/Dockerfile.xpu \"Dockerfile.xpu\")| [Dockerfile.xpu](/vllm-project/vllm/blob/main/Dockerfile.xpu \"Dockerfile.xpu\")  \n[LICENSE](/vllm-project/vllm/blob/main/LICENSE \"LICENSE\")| [LICENSE](/vllm-project/vllm/blob/main/LICENSE \"LICENSE\")  \n[MANIFEST.in](/vllm-project/vllm/blob/main/MANIFEST.in \"MANIFEST.in\")| [MANIFEST.in](/vllm-project/vllm/blob/main/MANIFEST.in \"MANIFEST.in\")  \n[README.md](/vllm-project/vllm/blob/main/README.md \"README.md\")| [README.md](/vllm-project/vllm/blob/main/README.md \"README.md\")  \n[SECURITY.md](/vllm-project/vllm/blob/main/SECURITY.md \"SECURITY.md\")| [SECURITY.md](/vllm-project/vllm/blob/main/SECURITY.md \"SECURITY.md\")  \n[collect_env.py](/vllm-project/vllm/blob/main/collect_env.py \"collect_env.py\")| [collect_env.py](/vllm-project/vllm/blob/main/collect_env.py \"collect_env.py\")  \n[find_cuda_init.py](/vllm-project/vllm/blob/main/find_cuda_init.py \"find_cuda_init.py\")| [find_cuda_init.py](/vllm-project/vllm/blob/main/find_cuda_init.py \"find_cuda_init.py\")  \n[format.sh](/vllm-project/vllm/blob/main/format.sh \"format.sh\")| [format.sh](/vllm-project/vllm/blob/main/format.sh \"format.sh\")  \n[pyproject.toml](/vllm-project/vllm/blob/main/pyproject.toml \"pyproject.toml\")| [pyproject.toml](/vllm-project/vllm/blob/main/pyproject.toml \"pyproject.toml\")  \n[python_only_dev.py](/vllm-project/vllm/blob/main/python_only_dev.py \"python_only_dev.py\")| [python_only_dev.py](/vllm-project/vllm/blob/main/python_only_dev.py \"python_only_dev.py\")  \n[requirements-build.txt](/vllm-project/vllm/blob/main/requirements-build.txt \"requirements-build.txt\")| [requirements-build.txt](/vllm-project/vllm/blob/main/requirements-build.txt \"requirements-build.txt\")  \n[requirements-common.txt](/vllm-project/vllm/blob/main/requirements-common.txt \"requirements-common.txt\")| [requirements-common.txt](/vllm-project/vllm/blob/main/requirements-common.txt \"requirements-common.txt\")  \n[requirements-cpu.txt](/vllm-project/vllm/blob/main/requirements-cpu.txt \"requirements-cpu.txt\")| [requirements-cpu.txt](/vllm-project/vllm/blob/main/requirements-cpu.txt \"requirements-cpu.txt\")  \n[requirements-cuda.txt](/vllm-project/vllm/blob/main/requirements-cuda.txt \"requirements-cuda.txt\")| [requirements-cuda.txt](/vllm-project/vllm/blob/main/requirements-cuda.txt \"requirements-cuda.txt\")  \n[requirements-dev.txt](/vllm-project/vllm/blob/main/requirements-dev.txt \"requirements-dev.txt\")| [requirements-dev.txt](/vllm-project/vllm/blob/main/requirements-dev.txt \"requirements-dev.txt\")  \n[requirements-hpu.txt](/vllm-project/vllm/blob/main/requirements-hpu.txt \"requirements-hpu.txt\")| [requirements-hpu.txt](/vllm-project/vllm/blob/main/requirements-hpu.txt \"requirements-hpu.txt\")  \n[requirements-lint.txt](/vllm-project/vllm/blob/main/requirements-lint.txt \"requirements-lint.txt\")| [requirements-lint.txt](/vllm-project/vllm/blob/main/requirements-lint.txt \"requirements-lint.txt\")  \n[requirements-neuron.txt](/vllm-project/vllm/blob/main/requirements-neuron.txt \"requirements-neuron.txt\")| [requirements-neuron.txt](/vllm-project/vllm/blob/main/requirements-neuron.txt \"requirements-neuron.txt\")  \n[requirements-openvino.txt](/vllm-project/vllm/blob/main/requirements-openvino.txt \"requirements-openvino.txt\")| [requirements-openvino.txt](/vllm-project/vllm/blob/main/requirements-openvino.txt \"requirements-openvino.txt\")  \n[requirements-rocm.txt](/vllm-project/vllm/blob/main/requirements-rocm.txt \"requirements-rocm.txt\")| [requirements-rocm.txt](/vllm-project/vllm/blob/main/requirements-rocm.txt \"requirements-rocm.txt\")  \n[requirements-test.in](/vllm-project/vllm/blob/main/requirements-test.in \"requirements-test.in\")| [requirements-test.in](/vllm-project/vllm/blob/main/requirements-test.in \"requirements-test.in\")  \n[requirements-test.txt](/vllm-project/vllm/blob/main/requirements-test.txt \"requirements-test.txt\")| [requirements-test.txt](/vllm-project/vllm/blob/main/requirements-test.txt \"requirements-test.txt\")  \n[requirements-tpu.txt](/vllm-project/vllm/blob/main/requirements-tpu.txt \"requirements-tpu.txt\")| [requirements-tpu.txt](/vllm-project/vllm/blob/main/requirements-tpu.txt \"requirements-tpu.txt\")  \n[requirements-xpu.txt](/vllm-project/vllm/blob/main/requirements-xpu.txt \"requirements-xpu.txt\")| [requirements-xpu.txt](/vllm-project/vllm/blob/main/requirements-xpu.txt \"requirements-xpu.txt\")  \n[setup.py](/vllm-project/vllm/blob/main/setup.py \"setup.py\")| [setup.py](/vllm-project/vllm/blob/main/setup.py \"setup.py\")  \n[use_existing_torch.py](/vllm-project/vllm/blob/main/use_existing_torch.py \"use_existing_torch.py\")| [use_existing_torch.py](/vllm-project/vllm/blob/main/use_existing_torch.py \"use_existing_torch.py\")  \nView all files  \n  \n## Repository files navigation\n\n  * [README](#)\n  * [Code of conduct](#)\n  * [Apache-2.0 license](#)\n  * [Security](#)\n\n\n\n![vLLM](https://raw.githubusercontent.com/vllm-project/vllm/main/docs/source/assets/logos/vllm-logo-text-light.png)\n\n###  Easy, fast, and cheap LLM serving for everyone \n\n[](#easy-fast-and-cheap-llm-serving-for-everyone)\n\n| [**Documentation**](https://docs.vllm.ai) | [**Blog**](https://vllm.ai) | [**Paper**](https://arxiv.org/abs/2309.06180) | [**Discord**](https://discord.gg/jz7wjKhh6g) | [**Twitter/X**](https://x.com/vllm_project) | [**Developer Slack**](https://slack.vllm.ai) | \n\nThe first vLLM meetup in 2025 is happening on January 22nd, Wednesday, with Google Cloud in San Francisco! We will talk about vLLM's performant V1 architecture, Q1 roadmap, Google Cloud's innovation around vLLM: networking, Cloud Run, Vertex, and TPU! [Register Now](https://lu.ma/zep56hui)\n\n_Latest News_ ðŸ”¥\n\n  * [2024/12] vLLM joins [pytorch ecosystem](https://pytorch.org/blog/vllm-joins-pytorch)! Easy, Fast, and Cheap LLM Serving for Everyone!\n  * [2024/11] We hosted [the seventh vLLM meetup](https://lu.ma/h0qvrajz) with Snowflake! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1e3CxQBV3JsfGp30SwyvS3eM_tW-ghOhJ9PAJGK6KR54/edit?usp=sharing), and Snowflake team [here](https://docs.google.com/presentation/d/1qF3RkDAbOULwz9WK5TOltt2fE9t6uIc_hVNLFAaQX6A/edit?usp=sharing).\n  * [2024/10] We have just created a developer slack ([slack.vllm.ai](https://slack.vllm.ai)) focusing on coordinating contributions and discussing features. Please feel free to join us there!\n  * [2024/10] Ray Summit 2024 held a special track for vLLM! Please find the opening talk slides from the vLLM team [here](https://docs.google.com/presentation/d/1B_KQxpHBTRa_mDF-tR6i8rWdOU5QoTZNcEg2MKZxEHM/edit?usp=sharing). Learn more from the [talks](https://www.youtube.com/playlist?list=PLzTswPQNepXl6AQwifuwUImLPFRVpksjR) from other vLLM contributors and users!\n  * [2024/09] We hosted [the sixth vLLM meetup](https://lu.ma/87q3nvnh) with NVIDIA! Please find the meetup slides [here](https://docs.google.com/presentation/d/1wrLGwytQfaOTd5wCGSPNhoaW3nq0E-9wqyP7ny93xRs/edit?usp=sharing).\n  * [2024/07] We hosted [the fifth vLLM meetup](https://lu.ma/lp0gyjqr) with AWS! Please find the meetup slides [here](https://docs.google.com/presentation/d/1RgUD8aCfcHocghoP3zmXzck9vX3RCI9yfUAB2Bbcl4Y/edit?usp=sharing).\n  * [2024/07] In partnership with Meta, vLLM officially supports Llama 3.1 with FP8 quantization and pipeline parallelism! Please check out our blog post [here](https://blog.vllm.ai/2024/07/23/llama31.html).\n  * [2024/06] We hosted [the fourth vLLM meetup](https://lu.ma/agivllm) with Cloudflare and BentoML! Please find the meetup slides [here](https://docs.google.com/presentation/d/1iJ8o7V2bQEi0BFEljLTwc5G1S10_Rhv3beed5oB0NJ4/edit?usp=sharing).\n  * [2024/04] We hosted [the third vLLM meetup](https://robloxandvllmmeetup2024.splashthat.com/) with Roblox! Please find the meetup slides [here](https://docs.google.com/presentation/d/1A--47JAK4BJ39t954HyTkvtfwn0fkqtsL8NGFuslReM/edit?usp=sharing).\n  * [2024/01] We hosted [the second vLLM meetup](https://lu.ma/ygxbpzhl) with IBM! Please find the meetup slides [here](https://docs.google.com/presentation/d/12mI2sKABnUw5RBWXDYY-HtHth4iMSNcEoQ10jDQbxgA/edit?usp=sharing).\n  * [2023/10] We hosted [the first vLLM meetup](https://lu.ma/first-vllm-meetup) with a16z! Please find the meetup slides [here](https://docs.google.com/presentation/d/1QL-XPFXiFpDBh86DbEegFXBXFXjix4v032GhShbKf3s/edit?usp=sharing).\n  * [2023/08] We would like to express our sincere gratitude to [Andreessen Horowitz](https://a16z.com/2023/08/30/supporting-the-open-source-ai-community/) (a16z) for providing a generous grant to support the open-source development and research of vLLM.\n  * [2023/06] We officially released vLLM! FastChat-vLLM integration has powered [LMSYS Vicuna and Chatbot Arena](https://chat.lmsys.org) since mid-April. Check out our [blog post](https://vllm.ai).\n\n\n\n## About\n\n[](#about)\n\nvLLM is a fast and easy-to-use library for LLM inference and serving.\n\nOriginally developed in the [Sky Computing Lab](https://sky.cs.berkeley.edu) at UC Berkeley, vLLM has evloved into a community-driven project with contributions from both academia and industry.\n\nvLLM is fast with:\n\n  * State-of-the-art serving throughput\n  * Efficient management of attention key and value memory with [**PagedAttention**](https://blog.vllm.ai/2023/06/20/vllm.html)\n  * Continuous batching of incoming requests\n  * Fast model execution with CUDA/HIP graph\n  * Quantizations: [GPTQ](https://arxiv.org/abs/2210.17323), [AWQ](https://arxiv.org/abs/2306.00978), INT4, INT8, and FP8.\n  * Optimized CUDA kernels, including integration with FlashAttention and FlashInfer.\n  * Speculative decoding\n  * Chunked prefill\n\n\n\n**Performance benchmark** : We include a performance benchmark at the end of [our blog post](https://blog.vllm.ai/2024/09/05/perf-update.html). It compares the performance of vLLM against other LLM serving engines ([TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM), [SGLang](https://github.com/sgl-project/sglang) and [LMDeploy](https://github.com/InternLM/lmdeploy)). The implementation is under [nightly-benchmarks folder](/vllm-project/vllm/blob/main/.buildkite/nightly-benchmarks) and you can [reproduce](https://github.com/vllm-project/vllm/issues/8176) this benchmark using our one-click runnable script.\n\nvLLM is flexible and easy to use with:\n\n  * Seamless integration with popular Hugging Face models\n  * High-throughput serving with various decoding algorithms, including _parallel sampling_ , _beam search_ , and more\n  * Tensor parallelism and pipeline parallelism support for distributed inference\n  * Streaming outputs\n  * OpenAI-compatible API server\n  * Support NVIDIA GPUs, AMD CPUs and GPUs, Intel CPUs and GPUs, PowerPC CPUs, TPU, and AWS Neuron.\n  * Prefix caching support\n  * Multi-lora support\n\n\n\nvLLM seamlessly supports most popular open-source models on HuggingFace, including:\n\n  * Transformer-like LLMs (e.g., Llama)\n  * Mixture-of-Expert LLMs (e.g., Mixtral, Deepseek-V2 and V3)\n  * Embedding Models (e.g. E5-Mistral)\n  * Multi-modal LLMs (e.g., LLaVA)\n\n\n\nFind the full list of supported models [here](https://docs.vllm.ai/en/latest/models/supported_models.html).\n\n## Getting Started\n\n[](#getting-started)\n\nInstall vLLM with `pip` or [from source](https://docs.vllm.ai/en/latest/getting_started/installation/gpu/index.html#build-wheel-from-source):\n\n```\npip install vllm\n```\n\nVisit our [documentation](https://docs.vllm.ai/en/latest/) to learn more.\n\n  * [Installation](https://docs.vllm.ai/en/latest/getting_started/installation/index.html)\n  * [Quickstart](https://docs.vllm.ai/en/latest/getting_started/quickstart.html)\n  * [List of Supported Models](https://docs.vllm.ai/en/latest/models/supported_models.html)\n\n\n\n## Contributing\n\n[](#contributing)\n\nWe welcome and value any contributions and collaborations. Please check out [CONTRIBUTING.md](/vllm-project/vllm/blob/main/CONTRIBUTING.md) for how to get involved.\n\n## Sponsors\n\n[](#sponsors)\n\nvLLM is a community project. Our compute resources for development and testing are supported by the following organizations. Thank you for your support!\n\nCash Donations:\n\n  * a16z\n  * Dropbox\n  * Sequoia Capital\n  * Skywork AI\n  * ZhenFund\n\n\n\nCompute Resources:\n\n  * AMD\n  * Anyscale\n  * AWS\n  * Crusoe Cloud\n  * Databricks\n  * DeepInfra\n  * Google Cloud\n  * Lambda Lab\n  * Nebius\n  * Novita AI\n  * NVIDIA\n  * Replicate\n  * Roblox\n  * RunPod\n  * Trainy\n  * UC Berkeley\n  * UC San Diego\n\n\n\nSlack Sponsor: Anyscale\n\nWe also have an official fundraising venue through [OpenCollective](https://opencollective.com/vllm). We plan to use the fund to support the development, maintenance, and adoption of vLLM.\n\n## Citation\n\n[](#citation)\n\nIf you use vLLM for your research, please cite our [paper](https://arxiv.org/abs/2309.06180):\n\n```\n@inproceedings{kwon2023efficient, title={Efficient Memory Management for Large Language Model Serving with PagedAttention}, author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica}, booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles}, year={2023} }\n```\n\n## Contact Us\n\n[](#contact-us)\n\n  * For technical questions and feature requests, please use Github issues or discussions.\n  * For discussing with fellow users, please use Discord.\n  * For coordinating contributions and development, please use Slack.\n  * For security disclosures, please use Github's security advisory feature.\n  * For collaborations and partnerships, please contact us at vllm-questions AT lists.berkeley.edu.\n\n\n\n## Media Kit\n\n[](#media-kit)\n\n  * If you wish to use vLLM's logo, please refer to [our media kit repo](https://github.com/vllm-project/media-kit).\n\n\n\n## About\n\nA high-throughput and memory-efficient inference and serving engine for LLMs \n\n[docs.vllm.ai](https://docs.vllm.ai \"https://docs.vllm.ai\")\n\n### Topics\n\n[ amd ](/topics/amd \"Topic: amd\") [ cuda ](/topics/cuda \"Topic: cuda\") [ inference ](/topics/inference \"Topic: inference\") [ pytorch ](/topics/pytorch \"Topic: pytorch\") [ transformer ](/topics/transformer \"Topic: transformer\") [ llama ](/topics/llama \"Topic: llama\") [ gpt ](/topics/gpt \"Topic: gpt\") [ rocm ](/topics/rocm \"Topic: rocm\") [ model-serving ](/topics/model-serving \"Topic: model-serving\") [ tpu ](/topics/tpu \"Topic: tpu\") [ hpu ](/topics/hpu \"Topic: hpu\") [ mlops ](/topics/mlops \"Topic: mlops\") [ xpu ](/topics/xpu \"Topic: xpu\") [ llm ](/topics/llm \"Topic: llm\") [ inferentia ](/topics/inferentia \"Topic: inferentia\") [ llmops ](/topics/llmops \"Topic: llmops\") [ llm-serving ](/topics/llm-serving \"Topic: llm-serving\") [ trainium ](/topics/trainium \"Topic: trainium\")\n\n### Resources\n\n[ Readme ](#readme-ov-file)\n\n### License\n\n[ Apache-2.0 license ](#Apache-2.0-1-ov-file)\n\n### Code of conduct\n\n[ Code of conduct ](#coc-ov-file)\n\n### Security policy\n\n[ Security policy ](#security-ov-file)\n\n[ Activity](/vllm-project/vllm/activity)\n\n[ Custom properties](/vllm-project/vllm/custom-properties)\n\n### Stars\n\n[ **34k** stars](/vllm-project/vllm/stargazers)\n\n### Watchers\n\n[ **280** watching](/vllm-project/vllm/watchers)\n\n### Forks\n\n[ **5.2k** forks](/vllm-project/vllm/forks)\n\n[ Report repository ](/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fvllm-project%2Fvllm&report=vllm-project+%28user%29)\n\n##  [Releases 46](/vllm-project/vllm/releases)\n\n[ v0.6.6.post1 Latest  Dec 27, 2024 ](/vllm-project/vllm/releases/tag/v0.6.6.post1)\n\n[+ 45 releases](/vllm-project/vllm/releases)\n\n## Sponsor this project\n\n  * [ ![@vllm-project](https://avatars.githubusercontent.com/u/136984999?s=64&v=4) ](/vllm-project) [ **vllm-project** vLLM ](/vllm-project) [ ](/sponsors/vllm-project)\n\n\n  * ![open_collective](https://github.githubassets.com/assets/open_collective-0a706523753d.svg) [opencollective.com/**vllm**](https://opencollective.com/vllm)\n\n\n\n[Learn more about GitHub Sponsors](/sponsors)\n\n##  [Used by 2.8k](/vllm-project/vllm/network/dependents)\n\n[\n\n  * ![@Indoxer](https://avatars.githubusercontent.com/u/77686332?s=64&v=4)\n  * ![@microsoft](https://avatars.githubusercontent.com/u/6154722?s=64&v=4)\n  * ![@niminim](https://avatars.githubusercontent.com/u/12481619?s=64&v=4)\n  * ![@lanad01](https://avatars.githubusercontent.com/u/62043299?s=64&v=4)\n  * ![@narenp12](https://avatars.githubusercontent.com/u/146764727?s=64&v=4)\n  * ![@mmrech](https://avatars.githubusercontent.com/u/137358704?s=64&v=4)\n  * ![@mmrech](https://avatars.githubusercontent.com/u/137358704?s=64&v=4)\n  * ![@SilexDataTeam](https://avatars.githubusercontent.com/u/109674383?s=64&v=4)\n\n+ 2,816  ](/vllm-project/vllm/network/dependents)\n\n##  [Contributors 788](/vllm-project/vllm/graphs/contributors)\n\n  * [ ![@WoosukKwon](https://avatars.githubusercontent.com/u/46394894?s=64&v=4) ](https://github.com/WoosukKwon)\n  * [ ![@youkaichao](https://avatars.githubusercontent.com/u/23236638?s=64&v=4) ](https://github.com/youkaichao)\n  * [ ![@DarkLight1337](https://avatars.githubusercontent.com/u/44970335?s=64&v=4) ](https://github.com/DarkLight1337)\n  * [ ![@mgoin](https://avatars.githubusercontent.com/u/3195154?s=64&v=4) ](https://github.com/mgoin)\n  * [ ![@simon-mo](https://avatars.githubusercontent.com/u/21118851?s=64&v=4) ](https://github.com/simon-mo)\n  * [ ![@ywang96](https://avatars.githubusercontent.com/u/136131678?s=64&v=4) ](https://github.com/ywang96)\n  * [ ![@zhuohan123](https://avatars.githubusercontent.com/u/17310766?s=64&v=4) ](https://github.com/zhuohan123)\n  * [ ![@Isotr0py](https://avatars.githubusercontent.com/u/41363108?s=64&v=4) ](https://github.com/Isotr0py)\n  * [ ![@njhill](https://avatars.githubusercontent.com/u/16958488?s=64&v=4) ](https://github.com/njhill)\n  * [ ![@robertgshaw2-redhat](https://avatars.githubusercontent.com/u/114415538?s=64&v=4) ](https://github.com/robertgshaw2-redhat)\n  * [ ![@Yard1](https://avatars.githubusercontent.com/u/10364161?s=64&v=4) ](https://github.com/Yard1)\n  * [ ![@tlrmchlsmth](https://avatars.githubusercontent.com/u/1236979?s=64&v=4) ](https://github.com/tlrmchlsmth)\n  * [ ![@jeejeelee](https://avatars.githubusercontent.com/u/19733142?s=64&v=4) ](https://github.com/jeejeelee)\n  * [ ![@comaniac](https://avatars.githubusercontent.com/u/8262694?s=64&v=4) ](https://github.com/comaniac)\n\n\n\n[+ 774 contributors](/vllm-project/vllm/graphs/contributors)\n\n## Languages\n\n  * [ Python 84.2% ](/vllm-project/vllm/search?l=python)\n  * [ Cuda 10.8% ](/vllm-project/vllm/search?l=cuda)\n  * [ C++ 3.1% ](/vllm-project/vllm/search?l=c%2B%2B)\n  * [ C 0.8% ](/vllm-project/vllm/search?l=c)\n  * [ Shell 0.6% ](/vllm-project/vllm/search?l=shell)\n  * [ CMake 0.4% ](/vllm-project/vllm/search?l=cmake)\n  * [ Dockerfile 0.1% ](/vllm-project/vllm/search?l=dockerfile)\n\n\n\n## Footer\n\n[ ](https://github.com \"GitHub\") Â© 2025 GitHub, Inc. \n\n### Footer navigation\n\n  * [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)\n  * [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)\n  * [Security](https://github.com/security)\n  * [Status](https://www.githubstatus.com/)\n  * [Docs](https://docs.github.com/)\n  * [Contact](https://support.github.com?tags=dotcom-footer)\n  * Manage cookies \n  * Do not share my personal information \n\n\n\nYou canâ€™t perform that action at this time. \n",
    "content_quality_score": 0.4,
    "summary": null,
    "child_urls": [
        "https://github.com/vllm-project/vllm/#start-of-content",
        "https://github.com/",
        "https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fvllm-project%2Fvllm%2F",
        "https://github.com/features/copilot",
        "https://github.com/features/security",
        "https://github.com/features/actions",
        "https://github.com/features/codespaces",
        "https://github.com/features/issues",
        "https://github.com/features/code-review",
        "https://github.com/features/discussions",
        "https://github.com/features/code-search",
        "https://github.com/features",
        "https://docs.github.com",
        "https://skills.github.com",
        "https://github.com/enterprise",
        "https://github.com/team",
        "https://github.com/enterprise/startups",
        "https://github.com/solutions/industry/nonprofits",
        "https://github.com/solutions/use-case/devsecops",
        "https://github.com/solutions/use-case/devops",
        "https://github.com/solutions/use-case/ci-cd",
        "https://github.com/solutions/use-case",
        "https://github.com/solutions/industry/healthcare",
        "https://github.com/solutions/industry/financial-services",
        "https://github.com/solutions/industry/manufacturing",
        "https://github.com/solutions/industry/government",
        "https://github.com/solutions/industry",
        "https://github.com/solutions",
        "https://github.com/resources/articles/ai",
        "https://github.com/resources/articles/devops",
        "https://github.com/resources/articles/security",
        "https://github.com/resources/articles/software-development",
        "https://github.com/resources/articles",
        "https://resources.github.com/learn/pathways",
        "https://resources.github.com",
        "https://github.com/customer-stories",
        "https://partner.github.com",
        "https://github.com/solutions/executive-insights",
        "https://github.com/sponsors",
        "https://github.com/readme",
        "https://github.com/topics",
        "https://github.com/trending",
        "https://github.com/collections",
        "https://github.com/enterprise/advanced-security",
        "https://github.com/features/copilot#enterprise",
        "https://github.com/premium-support",
        "https://github.com/pricing",
        "https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax",
        "https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=vllm-project%2Fvllm",
        "https://github.com/vllm-project",
        "https://github.com/vllm-project/vllm",
        "https://github.com/login?return_to=%2Fvllm-project%2Fvllm",
        "https://github.com/vllm-project/vllm/blob/main/LICENSE",
        "https://github.com/vllm-project/vllm/stargazers",
        "https://github.com/vllm-project/vllm/forks",
        "https://github.com/vllm-project/vllm/branches",
        "https://github.com/vllm-project/vllm/tags",
        "https://github.com/vllm-project/vllm/activity",
        "https://github.com/vllm-project/vllm/issues",
        "https://github.com/vllm-project/vllm/pulls",
        "https://github.com/vllm-project/vllm/discussions",
        "https://github.com/vllm-project/vllm/actions",
        "https://github.com/vllm-project/vllm/security",
        "https://github.com/vllm-project/vllm/pulse",
        "https://github.com/vllm-project/vllm/commits/main/",
        "https://github.com/vllm-project/vllm/tree/main/.buildkite",
        "https://github.com/vllm-project/vllm/tree/main/.github",
        "https://github.com/vllm-project/vllm/tree/main/benchmarks",
        "https://github.com/vllm-project/vllm/tree/main/cmake",
        "https://github.com/vllm-project/vllm/tree/main/csrc",
        "https://github.com/vllm-project/vllm/tree/main/docs",
        "https://github.com/vllm-project/vllm/tree/main/examples",
        "https://github.com/vllm-project/vllm/tree/main/tests",
        "https://github.com/vllm-project/vllm/tree/main/tools",
        "https://github.com/vllm-project/vllm/tree/main/vllm",
        "https://github.com/vllm-project/vllm/blob/main/.clang-format",
        "https://github.com/vllm-project/vllm/blob/main/.dockerignore",
        "https://github.com/vllm-project/vllm/blob/main/.gitignore",
        "https://github.com/vllm-project/vllm/blob/main/.pre-commit-config.yaml",
        "https://github.com/vllm-project/vllm/blob/main/.readthedocs.yaml",
        "https://github.com/vllm-project/vllm/blob/main/.shellcheckrc",
        "https://github.com/vllm-project/vllm/blob/main/.yapfignore",
        "https://github.com/vllm-project/vllm/blob/main/CMakeLists.txt",
        "https://github.com/vllm-project/vllm/blob/main/CODE_OF_CONDUCT.md",
        "https://github.com/vllm-project/vllm/blob/main/CONTRIBUTING.md",
        "https://github.com/vllm-project/vllm/blob/main/DCO",
        "https://github.com/vllm-project/vllm/blob/main/Dockerfile",
        "https://github.com/vllm-project/vllm/blob/main/Dockerfile.arm",
        "https://github.com/vllm-project/vllm/blob/main/Dockerfile.cpu",
        "https://github.com/vllm-project/vllm/blob/main/Dockerfile.hpu",
        "https://github.com/vllm-project/vllm/blob/main/Dockerfile.neuron",
        "https://github.com/vllm-project/vllm/blob/main/Dockerfile.openvino",
        "https://github.com/vllm-project/vllm/blob/main/Dockerfile.ppc64le",
        "https://github.com/vllm-project/vllm/blob/main/Dockerfile.rocm",
        "https://github.com/vllm-project/vllm/blob/main/Dockerfile.tpu",
        "https://github.com/vllm-project/vllm/blob/main/Dockerfile.xpu",
        "https://github.com/vllm-project/vllm/blob/main/MANIFEST.in",
        "https://github.com/vllm-project/vllm/blob/main/README.md",
        "https://github.com/vllm-project/vllm/blob/main/SECURITY.md",
        "https://github.com/vllm-project/vllm/blob/main/collect_env.py",
        "https://github.com/vllm-project/vllm/blob/main/find_cuda_init.py",
        "https://github.com/vllm-project/vllm/blob/main/format.sh",
        "https://github.com/vllm-project/vllm/blob/main/pyproject.toml",
        "https://github.com/vllm-project/vllm/blob/main/python_only_dev.py",
        "https://github.com/vllm-project/vllm/blob/main/requirements-build.txt",
        "https://github.com/vllm-project/vllm/blob/main/requirements-common.txt",
        "https://github.com/vllm-project/vllm/blob/main/requirements-cpu.txt",
        "https://github.com/vllm-project/vllm/blob/main/requirements-cuda.txt",
        "https://github.com/vllm-project/vllm/blob/main/requirements-dev.txt",
        "https://github.com/vllm-project/vllm/blob/main/requirements-hpu.txt",
        "https://github.com/vllm-project/vllm/blob/main/requirements-lint.txt",
        "https://github.com/vllm-project/vllm/blob/main/requirements-neuron.txt",
        "https://github.com/vllm-project/vllm/blob/main/requirements-openvino.txt",
        "https://github.com/vllm-project/vllm/blob/main/requirements-rocm.txt",
        "https://github.com/vllm-project/vllm/blob/main/requirements-test.in",
        "https://github.com/vllm-project/vllm/blob/main/requirements-test.txt",
        "https://github.com/vllm-project/vllm/blob/main/requirements-tpu.txt",
        "https://github.com/vllm-project/vllm/blob/main/requirements-xpu.txt",
        "https://github.com/vllm-project/vllm/blob/main/setup.py",
        "https://github.com/vllm-project/vllm/blob/main/use_existing_torch.py",
        "https://github.com/vllm-project/vllm/",
        "https://github.com/vllm-project/vllm/#easy-fast-and-cheap-llm-serving-for-everyone",
        "https://github.com/vllm-project/vllm/#about",
        "https://github.com/NVIDIA/TensorRT-LLM",
        "https://github.com/sgl-project/sglang",
        "https://github.com/InternLM/lmdeploy",
        "https://github.com/vllm-project/vllm/blob/main/.buildkite/nightly-benchmarks",
        "https://github.com/vllm-project/vllm/issues/8176",
        "https://github.com/vllm-project/vllm/#getting-started",
        "https://github.com/vllm-project/vllm/#contributing",
        "https://github.com/vllm-project/vllm/#sponsors",
        "https://github.com/vllm-project/vllm/#citation",
        "https://github.com/vllm-project/vllm/#contact-us",
        "https://github.com/vllm-project/vllm/#media-kit",
        "https://github.com/vllm-project/media-kit",
        "https://github.com/topics/amd",
        "https://github.com/topics/cuda",
        "https://github.com/topics/inference",
        "https://github.com/topics/pytorch",
        "https://github.com/topics/transformer",
        "https://github.com/topics/llama",
        "https://github.com/topics/gpt",
        "https://github.com/topics/rocm",
        "https://github.com/topics/model-serving",
        "https://github.com/topics/tpu",
        "https://github.com/topics/hpu",
        "https://github.com/topics/mlops",
        "https://github.com/topics/xpu",
        "https://github.com/topics/llm",
        "https://github.com/topics/inferentia",
        "https://github.com/topics/llmops",
        "https://github.com/topics/llm-serving",
        "https://github.com/topics/trainium",
        "https://github.com/vllm-project/vllm/#readme-ov-file",
        "https://github.com/vllm-project/vllm/#Apache-2.0-1-ov-file",
        "https://github.com/vllm-project/vllm/#coc-ov-file",
        "https://github.com/vllm-project/vllm/#security-ov-file",
        "https://github.com/vllm-project/vllm/custom-properties",
        "https://github.com/vllm-project/vllm/watchers",
        "https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fvllm-project%2Fvllm&report=vllm-project+%28user%29",
        "https://github.com/vllm-project/vllm/releases",
        "https://github.com/vllm-project/vllm/releases/tag/v0.6.6.post1",
        "https://github.com/sponsors/vllm-project",
        "https://github.com/vllm-project/vllm/network/dependents",
        "https://github.com/vllm-project/vllm/graphs/contributors",
        "https://github.com/WoosukKwon",
        "https://github.com/youkaichao",
        "https://github.com/DarkLight1337",
        "https://github.com/mgoin",
        "https://github.com/simon-mo",
        "https://github.com/ywang96",
        "https://github.com/zhuohan123",
        "https://github.com/Isotr0py",
        "https://github.com/njhill",
        "https://github.com/robertgshaw2-redhat",
        "https://github.com/Yard1",
        "https://github.com/tlrmchlsmth",
        "https://github.com/jeejeelee",
        "https://github.com/comaniac",
        "https://github.com/vllm-project/vllm/search?l=python",
        "https://github.com/vllm-project/vllm/search?l=cuda",
        "https://github.com/vllm-project/vllm/search?l=c%2B%2B",
        "https://github.com/vllm-project/vllm/search?l=c",
        "https://github.com/vllm-project/vllm/search?l=shell",
        "https://github.com/vllm-project/vllm/search?l=cmake",
        "https://github.com/vllm-project/vllm/search?l=dockerfile",
        "https://github.com",
        "https://docs.github.com/site-policy/github-terms/github-terms-of-service",
        "https://docs.github.com/site-policy/privacy-policies/github-privacy-statement",
        "https://github.com/security",
        "https://docs.github.com/",
        "https://support.github.com?tags=dotcom-footer",
        "https://github.blog",
        "https://docs.vllm.ai",
        "https://vllm.ai",
        "https://arxiv.org/abs/2309.06180",
        "https://discord.gg/jz7wjKhh6g",
        "https://x.com/vllm_project",
        "https://slack.vllm.ai",
        "https://lu.ma/zep56hui",
        "https://pytorch.org/blog/vllm-joins-pytorch",
        "https://lu.ma/h0qvrajz",
        "https://docs.google.com/presentation/d/1e3CxQBV3JsfGp30SwyvS3eM_tW-ghOhJ9PAJGK6KR54/edit?usp=sharing",
        "https://docs.google.com/presentation/d/1qF3RkDAbOULwz9WK5TOltt2fE9t6uIc_hVNLFAaQX6A/edit?usp=sharing",
        "https://docs.google.com/presentation/d/1B_KQxpHBTRa_mDF-tR6i8rWdOU5QoTZNcEg2MKZxEHM/edit?usp=sharing",
        "https://www.youtube.com/playlist?list=PLzTswPQNepXl6AQwifuwUImLPFRVpksjR",
        "https://lu.ma/87q3nvnh",
        "https://docs.google.com/presentation/d/1wrLGwytQfaOTd5wCGSPNhoaW3nq0E-9wqyP7ny93xRs/edit?usp=sharing",
        "https://lu.ma/lp0gyjqr",
        "https://docs.google.com/presentation/d/1RgUD8aCfcHocghoP3zmXzck9vX3RCI9yfUAB2Bbcl4Y/edit?usp=sharing",
        "https://blog.vllm.ai/2024/07/23/llama31.html",
        "https://lu.ma/agivllm",
        "https://docs.google.com/presentation/d/1iJ8o7V2bQEi0BFEljLTwc5G1S10_Rhv3beed5oB0NJ4/edit?usp=sharing",
        "https://robloxandvllmmeetup2024.splashthat.com/",
        "https://docs.google.com/presentation/d/1A--47JAK4BJ39t954HyTkvtfwn0fkqtsL8NGFuslReM/edit?usp=sharing",
        "https://lu.ma/ygxbpzhl",
        "https://docs.google.com/presentation/d/12mI2sKABnUw5RBWXDYY-HtHth4iMSNcEoQ10jDQbxgA/edit?usp=sharing",
        "https://lu.ma/first-vllm-meetup",
        "https://docs.google.com/presentation/d/1QL-XPFXiFpDBh86DbEegFXBXFXjix4v032GhShbKf3s/edit?usp=sharing",
        "https://a16z.com/2023/08/30/supporting-the-open-source-ai-community/",
        "https://chat.lmsys.org",
        "https://sky.cs.berkeley.edu",
        "https://blog.vllm.ai/2023/06/20/vllm.html",
        "https://arxiv.org/abs/2210.17323",
        "https://arxiv.org/abs/2306.00978",
        "https://blog.vllm.ai/2024/09/05/perf-update.html",
        "https://docs.vllm.ai/en/latest/models/supported_models.html",
        "https://docs.vllm.ai/en/latest/getting_started/installation/gpu/index.html#build-wheel-from-source",
        "https://docs.vllm.ai/en/latest/",
        "https://docs.vllm.ai/en/latest/getting_started/installation/index.html",
        "https://docs.vllm.ai/en/latest/getting_started/quickstart.html",
        "https://opencollective.com/vllm",
        "https://www.githubstatus.com/"
    ]
}
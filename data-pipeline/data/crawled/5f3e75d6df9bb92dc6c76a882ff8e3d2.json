{
    "id": "5f3e75d6df9bb92dc6c76a882ff8e3d2",
    "metadata": {
        "id": "5f3e75d6df9bb92dc6c76a882ff8e3d2",
        "url": "https://github.com/unslothai/unsloth/",
        "title": "GitHub - unslothai/unsloth: Finetune Llama 3.3, Mistral, Phi-4, Qwen 2.5 & Gemma LLMs 2-5x faster with 70% less memory",
        "properties": {
            "description": "Finetune Llama 3.3, Mistral, Phi-4, Qwen 2.5 & Gemma LLMs 2-5x faster with 70% less memory - unslothai/unsloth",
            "keywords": null,
            "author": null,
            "og:image": "https://repository-images.githubusercontent.com/725205304/5cddb831-01b1-4ec2-b45f-e1f17dde22ce",
            "og:image:alt": "Finetune Llama 3.3, Mistral, Phi-4, Qwen 2.5 & Gemma LLMs 2-5x faster with 70% less memory - unslothai/unsloth",
            "og:site_name": "GitHub",
            "og:type": "object",
            "og:title": "GitHub - unslothai/unsloth: Finetune Llama 3.3, Mistral, Phi-4, Qwen 2.5 & Gemma LLMs 2-5x faster with 70% less memory",
            "og:url": "https://github.com/unslothai/unsloth",
            "og:description": "Finetune Llama 3.3, Mistral, Phi-4, Qwen 2.5 & Gemma LLMs 2-5x faster with 70% less memory - unslothai/unsloth",
            "twitter:image": "https://repository-images.githubusercontent.com/725205304/5cddb831-01b1-4ec2-b45f-e1f17dde22ce",
            "twitter:site": "@github",
            "twitter:card": "summary_large_image",
            "twitter:title": "GitHub - unslothai/unsloth: Finetune Llama 3.3, Mistral, Phi-4, Qwen 2.5 & Gemma LLMs 2-5x faster with 70% less memory",
            "twitter:description": "Finetune Llama 3.3, Mistral, Phi-4, Qwen 2.5 & Gemma LLMs 2-5x faster with 70% less memory - unslothai/unsloth"
        }
    },
    "parent_metadata": {
        "id": "9fbbebfb0eeddb97df00c7cdfc1af2f0",
        "url": "https://www.notion.so/Training-Fine-tuning-LLMs-9fbbebfb0eeddb97df00c7cdfc1af2f0",
        "title": "Training & Fine-tuning LLMs",
        "properties": {
            "Type": "Leaf"
        }
    },
    "content": "[Skip to content](#start-of-content)\n\n## Navigation Menu\n\nToggle navigation\n\n[ ](/)\n\n[ Sign in ](/login?return_to=https%3A%2F%2Fgithub.com%2Funslothai%2Funsloth%2F)\n\n  * Product \n\n    * [ GitHub Copilot Write better code with AI  ](https://github.com/features/copilot)\n    * [ Security Find and fix vulnerabilities  ](https://github.com/features/security)\n    * [ Actions Automate any workflow  ](https://github.com/features/actions)\n    * [ Codespaces Instant dev environments  ](https://github.com/features/codespaces)\n    * [ Issues Plan and track work  ](https://github.com/features/issues)\n    * [ Code Review Manage code changes  ](https://github.com/features/code-review)\n    * [ Discussions Collaborate outside of code  ](https://github.com/features/discussions)\n    * [ Code Search Find more, search less  ](https://github.com/features/code-search)\n\nExplore\n    * [ All features ](https://github.com/features)\n    * [ Documentation ](https://docs.github.com)\n    * [ GitHub Skills ](https://skills.github.com)\n    * [ Blog ](https://github.blog)\n\n  * Solutions \n\nBy company size\n    * [ Enterprises ](https://github.com/enterprise)\n    * [ Small and medium teams ](https://github.com/team)\n    * [ Startups ](https://github.com/enterprise/startups)\n    * [ Nonprofits ](/solutions/industry/nonprofits)\n\nBy use case\n    * [ DevSecOps ](/solutions/use-case/devsecops)\n    * [ DevOps ](/solutions/use-case/devops)\n    * [ CI/CD ](/solutions/use-case/ci-cd)\n    * [ View all use cases ](/solutions/use-case)\n\nBy industry\n    * [ Healthcare ](/solutions/industry/healthcare)\n    * [ Financial services ](/solutions/industry/financial-services)\n    * [ Manufacturing ](/solutions/industry/manufacturing)\n    * [ Government ](/solutions/industry/government)\n    * [ View all industries ](/solutions/industry)\n\n[ View all solutions ](/solutions)\n\n  * Resources \n\nTopics\n    * [ AI ](/resources/articles/ai)\n    * [ DevOps ](/resources/articles/devops)\n    * [ Security ](/resources/articles/security)\n    * [ Software Development ](/resources/articles/software-development)\n    * [ View all ](/resources/articles)\n\nExplore\n    * [ Learning Pathways ](https://resources.github.com/learn/pathways)\n    * [ White papers, Ebooks, Webinars ](https://resources.github.com)\n    * [ Customer Stories ](https://github.com/customer-stories)\n    * [ Partners ](https://partner.github.com)\n    * [ Executive Insights ](https://github.com/solutions/executive-insights)\n\n  * Open Source \n\n    * [ GitHub Sponsors Fund open source developers  ](/sponsors)\n\n    * [ The ReadME Project GitHub community articles  ](https://github.com/readme)\n\nRepositories\n    * [ Topics ](https://github.com/topics)\n    * [ Trending ](https://github.com/trending)\n    * [ Collections ](https://github.com/collections)\n\n  * Enterprise \n\n    * [ Enterprise platform AI-powered developer platform  ](/enterprise)\n\nAvailable add-ons\n    * [ Advanced Security Enterprise-grade security features  ](https://github.com/enterprise/advanced-security)\n    * [ GitHub Copilot Enterprise-grade AI features  ](/features/copilot#enterprise)\n    * [ Premium Support Enterprise-grade 24/7 support  ](/premium-support)\n\n  * [Pricing](https://github.com/pricing)\n\n\n\nSearch or jump to...\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch \n\nClear\n\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n\n#  Provide feedback \n\nWe read every piece of feedback, and take your input very seriously.\n\nInclude my email address so I can be contacted\n\nCancel  Submit feedback \n\n#  Saved searches \n\n## Use saved searches to filter your results more quickly\n\nName\n\nQuery\n\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax). \n\nCancel  Create saved search \n\n[ Sign in ](/login?return_to=https%3A%2F%2Fgithub.com%2Funslothai%2Funsloth%2F)\n\n[ Sign up ](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=unslothai%2Funsloth) Reseting focus\n\nYou signed in with another tab or window. [Reload]() to refresh your session. You signed out in another tab or window. [Reload]() to refresh your session. You switched accounts on another tab or window. [Reload]() to refresh your session. Dismiss alert\n\n{{ message }}\n\n[ unslothai ](/unslothai) / **[unsloth](/unslothai/unsloth) ** Public\n\n  * Sponsor\n\n#  Sponsor unslothai/unsloth \n\n##### External links\n\n![ko_fi](https://github.githubassets.com/assets/ko_fi-53a60c17e75c.svg)\n\n[ko-fi.com/**unsloth**](https://ko-fi.com/unsloth)\n\n[Learn more about funding links in repositories](https://docs.github.com/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/displaying-a-sponsor-button-in-your-repository). \n\n[Report abuse](/contact/report-abuse?report=unslothai%2Funsloth+%28Repository+Funding+Links%29)\n\n  * [ Notifications ](/login?return_to=%2Funslothai%2Funsloth) You must be signed in to change notification settings\n  * [ Fork 1.5k ](/login?return_to=%2Funslothai%2Funsloth)\n  * [ Star  21k ](/login?return_to=%2Funslothai%2Funsloth)\n\n\n\n\nFinetune Llama 3.3, Mistral, Phi-4, Qwen 2.5 & Gemma LLMs 2-5x faster with 70% less memory \n\n[unsloth.ai](https://unsloth.ai \"https://unsloth.ai\")\n\n### License\n\n[ Apache-2.0 license ](/unslothai/unsloth/blob/main/LICENSE)\n\n[ 21k stars ](/unslothai/unsloth/stargazers) [ 1.5k forks ](/unslothai/unsloth/forks) [ Branches ](/unslothai/unsloth/branches) [ Tags ](/unslothai/unsloth/tags) [ Activity ](/unslothai/unsloth/activity)\n\n[ Star  ](/login?return_to=%2Funslothai%2Funsloth)\n\n[ Notifications ](/login?return_to=%2Funslothai%2Funsloth) You must be signed in to change notification settings\n\n  * [ Code ](/unslothai/unsloth)\n  * [ Issues 621 ](/unslothai/unsloth/issues)\n  * [ Pull requests 33 ](/unslothai/unsloth/pulls)\n  * [ Actions ](/unslothai/unsloth/actions)\n  * [ Wiki ](/unslothai/unsloth/wiki)\n  * [ Security ](/unslothai/unsloth/security)\n  * [ Insights ](/unslothai/unsloth/pulse)\n\n\n\nAdditional navigation options\n\n  * [ Code  ](/unslothai/unsloth)\n  * [ Issues  ](/unslothai/unsloth/issues)\n  * [ Pull requests  ](/unslothai/unsloth/pulls)\n  * [ Actions  ](/unslothai/unsloth/actions)\n  * [ Wiki  ](/unslothai/unsloth/wiki)\n  * [ Security  ](/unslothai/unsloth/security)\n  * [ Insights  ](/unslothai/unsloth/pulse)\n\n\n\n# unslothai/unsloth\n\nmain\n\n[**4** Branches](/unslothai/unsloth/branches)[**9** Tags](/unslothai/unsloth/tags)\n\n[](/unslothai/unsloth/branches)[](/unslothai/unsloth/tags)\n\nGo to file\n\nCode\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\n[![shimmyshimmer](https://avatars.githubusercontent.com/u/107991372?v=4&size=40)](/shimmyshimmer)[shimmyshimmer](/unslothai/unsloth/commits?author=shimmyshimmer)[Merge pull request](/unslothai/unsloth/commit/d802bbf4e298cb0da1e976ab9670fbc1cbe3514c) [#1569](https://github.com/unslothai/unsloth/pull/1569) [from unslothai/shimmyshimmer-patch-1](/unslothai/unsloth/commit/d802bbf4e298cb0da1e976ab9670fbc1cbe3514c)Jan 21, 2025[d802bbf](/unslothai/unsloth/commit/d802bbf4e298cb0da1e976ab9670fbc1cbe3514c) Â· Jan 21, 2025\n\n## History\n\n[1,054 Commits](/unslothai/unsloth/commits/main/)[](/unslothai/unsloth/commits/main/)  \n[.github](/unslothai/unsloth/tree/main/.github \".github\")| [.github](/unslothai/unsloth/tree/main/.github \".github\")| [Update issue templates](/unslothai/unsloth/commit/d6982c1fe6b814874f2ff989a69e485e6c13ab52 \"Update issue templates\")| Jan 17, 2025  \n[images](/unslothai/unsloth/tree/main/images \"images\")| [images](/unslothai/unsloth/tree/main/images \"images\")| [Vision (](/unslothai/unsloth/commit/d30c363d25668b8059237c58586d0f2d10903682 \"Vision \\(#1318\\)\n* Add files via upload\n* Add files via upload\n* Add files via upload\n* Add files via upload\n* Update README.md\n* Update README.md\n* Update README.md\n* Update README.md\n---------\nCo-authored-by: Michael <107991372+shimmyshimmer@users.noreply.github.com>\")[#1318](https://github.com/unslothai/unsloth/pull/1318)[)](/unslothai/unsloth/commit/d30c363d25668b8059237c58586d0f2d10903682 \"Vision \\(#1318\\)\n* Add files via upload\n* Add files via upload\n* Add files via upload\n* Add files via upload\n* Update README.md\n* Update README.md\n* Update README.md\n* Update README.md\n---------\nCo-authored-by: Michael <107991372+shimmyshimmer@users.noreply.github.com>\")| Nov 21, 2024  \n[unsloth](/unslothai/unsloth/tree/main/unsloth \"unsloth\")| [unsloth](/unslothai/unsloth/tree/main/unsloth \"unsloth\")| [Update mapper.py](/unslothai/unsloth/commit/fde26db11de0dc6b76a9c6ac21ce9c71d3593323 \"Update mapper.py\")| Jan 20, 2025  \n[CONTRIBUTING.md](/unslothai/unsloth/blob/main/CONTRIBUTING.md \"CONTRIBUTING.md\")| [CONTRIBUTING.md](/unslothai/unsloth/blob/main/CONTRIBUTING.md \"CONTRIBUTING.md\")| [Update CONTRIBUTING.md](/unslothai/unsloth/commit/3a0eb2bbf42016dfeec924b320c420dabbce168b \"Update CONTRIBUTING.md\nimproved sentence\")| Jan 5, 2025  \n[LICENSE](/unslothai/unsloth/blob/main/LICENSE \"LICENSE\")| [LICENSE](/unslothai/unsloth/blob/main/LICENSE \"LICENSE\")| [Auto Healing Tokenizer (](/unslothai/unsloth/commit/a68aebc1fa17755ffbcdafc9239e7ca37ab21657 \"Auto Healing Tokenizer \\(#283\\)\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* llama\n* Update llama.py\n* gemma\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update save.py\n* RoPE\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update gemma.py\n* correct_dtype\n* Update gemma.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Chat Templates\n* Update README.md\n* Update README.md\n* Update llama.py\n* DoRA\n* Update _utils.py\n* Update chat_templates.py\n* Update llama.py\n* Hotfix - fix DoRA, Gemma prompt template \\(#202\\) \\(#203\\)\n* Update save.py\n* saving\n* Update save.py\n* Update save.py\n* Update save.py\n* Update save.py\n* Update save.py\n* Update save.py\n* Update save.py\n* Update save.py\n* Update save.py\n* Update save.py\n* Update save.py\n* Update save.py\n* Update save.py\n* Update __init__.py\n* Update save.py\n* Update save.py\n* Update save.py\n* save\n* trainer\n* spaces\n* original\n* Gemma\n* Update pyproject.toml\n* Update mapper.py\n* Update fast_lora.py\n* FastGemmaModel\n* model_type\n* Update llama.py\n* Update llama.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update llama.py\n* Update llama.py\n* Update fast_lora.py\n* Update llama.py\n* Update llama.py\n* Update cross_entropy_loss.py\n* Update llama.py\n* Update llama.py\n* gemma\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update fast_lora.py\n* Update fast_lora.py\n* Fast CE Loss\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* CE\n* Update llama.py\n* Update llama.py\n* Update cross_entropy_loss.py\n* Update geglu.py\n* Update cross_entropy_loss.py\n* revert\n* Update llama.py\n* Update llama.py\n* norm\n* Update gemma.py\n* Update gemma.py\n* position_ids\n* Update gemma.py\n* Update gemma.py\n* pos\n* Update llama.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update cross_entropy_loss.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update llama.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update llama.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* revert\n* revert\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update llama.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update cross_entropy_loss.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* rope\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* llama\n* Update llama.py\n* gemma\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update save.py\n* RoPE\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update gemma.py\n* correct_dtype\n* Update gemma.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Chat Templates\n* Update README.md\n* Update README.md\n* Update llama.py\n* DoRA\n* Update _utils.py\n* Update chat_templates.py\n* Update pyproject.toml\n* Small fixes\n* Update pyproject.toml\n* Approx gelu\n* Update geglu.py\n* Approx gelu\n* Update llama.py\n* Update __init__.py\n* Update __init__.py\n* Update _utils.py\n* Update geglu.py\n* Update gemma.py\n* Update rms_layernorm.py\n* Update rms_layernorm.py\n* Update rms_layernorm.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Fix Gemma merging\n* Update rms_layernorm.py\n* Update gemma.py\n* Update pyproject.toml\n* Layernorms\n* Gemma precision\n* Update gemma.py\n* sqrt\n* Update gemma.py\n* Update save.py\n* RoPE and Gemma precision\n* Update rms_layernorm.py\n* Fix warning\n* Update chat_templates.py\n* Update chat_templates.py\n* Update save.py\n* Update save.py\n* Update save.py\n* Update chat_templates.py\n* Update llama.py\n* model_name\n* Update loader.py\n* Tokenizer overwritten\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update save.py\n* Accuracy\n* Revert\n* Update save.py\n* Update fast_lora.py\n* Update fast_lora.py\n* Update fast_lora.py\n* Update fast_lora.py\n* Update fast_lora.py\n* Update chat_templates.py\n* Update save.py\n* Update save.py\n* Update llama.py\n* Update llama.py\n* Account for DoRA\n* Update llama.py\n* Update save.py\n* GGUF incorrect\n* Update save.py\n* Update pyproject.toml\n* kaggle new\n* Update pyproject.toml\n* Update pyproject.toml\n* upcasting\n* Fix Colab\n* Update pyproject.toml\n* Update pyproject.toml\n* Update pyproject.toml\n* Update pyproject.toml\n* Update pyproject.toml\n* Update pyproject.toml\n* Update pyproject.toml\n* Update pyproject.toml\n* Update chat_templates.py\n* Update chat_templates.py\n* Update chat_templates.py\n* Update chat_templates.py\n* Update chat_templates.py\n* Update pyproject.toml\n* Update pyproject.toml\n* Update pyproject.toml\n* Update rope_embedding.py\n* Update rope_embedding.py\n* Fix bugs\n* Update fast_lora.py\n* Update fast_lora.py\n* Update README.md\n* Update README.md\n* GGUF\n* Update save.py\n* Update save.py\n* Update save.py\n* Update save.py\n* Update README.md\n* Update README.md\n* Bugs\n* Update fast_lora.py\n* Update pyproject.toml\n* Update fast_lora.py\n* Update __init__.py\n* Update fast_lora.py\n* dtype\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* dtype\n* Update mistral.py\n* trust_remote_code\n* lm_head\n* Update llama.py\n* save_pretrained_settings\n* Update save.py\n* Update save.py\n* Update save.py\n* Update save.py\n* Update save.py\n* Update save.py\n* Update save.py\n* Update save.py\n* Update save.py\n* Update save.py\n* Update save.py\n* Update save.py\n* state_dict\n* Update save.py\n* whoami\n* Update llama.py\n* Update save.py\n* Update llama.py\n* Patch tokenizer\n* Update chat_templates.py\n* Heal tokenizers\n* Update chat_templates.py\n* Update mapper.py\n* Update tokenizer_utils.py\n* Update tokenizer_utils.py\n* Update tokenizer_utils.py\n* Update tokenizer_utils.py\n* Update tokenizer_utils.py\n* Update chat_templates.py\n* tokenizer patching\n* patch_tokenizer\n* Update chat_templates.py\n* Update tokenizer_utils.py\n* Update chat_templates.py\n* Update chat_templates.py\n* Update chat_templates.py\n* Update tokenizer_utils.py\n* Edit\")[#283](https://github.com/unslothai/unsloth/pull/283)[)](/unslothai/unsloth/commit/a68aebc1fa17755ffbcdafc9239e7ca37ab21657 \"Auto Healing Tokenizer \\(#283\\)\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* llama\n* Update llama.py\n* gemma\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update save.py\n* RoPE\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update gemma.py\n* correct_dtype\n* Update gemma.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Chat Templates\n* Update README.md\n* Update README.md\n* Update llama.py\n* DoRA\n* Update _utils.py\n* Update chat_templates.py\n* Update llama.py\n* Hotfix - fix DoRA, Gemma prompt template \\(#202\\) \\(#203\\)\n* Update save.py\n* saving\n* Update save.py\n* Update save.py\n* Update save.py\n* Update save.py\n* Update save.py\n* Update save.py\n* Update save.py\n* Update save.py\n* Update save.py\n* Update save.py\n* Update save.py\n* Update save.py\n* Update save.py\n* Update __init__.py\n* Update save.py\n* Update save.py\n* Update save.py\n* save\n* trainer\n* spaces\n* original\n* Gemma\n* Update pyproject.toml\n* Update mapper.py\n* Update fast_lora.py\n* FastGemmaModel\n* model_type\n* Update llama.py\n* Update llama.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update llama.py\n* Update llama.py\n* Update fast_lora.py\n* Update llama.py\n* Update llama.py\n* Update cross_entropy_loss.py\n* Update llama.py\n* Update llama.py\n* gemma\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update fast_lora.py\n* Update fast_lora.py\n* Fast CE Loss\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* CE\n* Update llama.py\n* Update llama.py\n* Update cross_entropy_loss.py\n* Update geglu.py\n* Update cross_entropy_loss.py\n* revert\n* Update llama.py\n* Update llama.py\n* norm\n* Update gemma.py\n* Update gemma.py\n* position_ids\n* Update gemma.py\n* Update gemma.py\n* pos\n* Update llama.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update cross_entropy_loss.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update llama.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update llama.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* revert\n* revert\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update llama.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update cross_entropy_loss.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* rope\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* llama\n* Update llama.py\n* gemma\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update save.py\n* RoPE\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update gemma.py\n* correct_dtype\n* Update gemma.py\n* Update cross_entropy_loss.py\n* Update cross_entropy_loss.py\n* Chat Templates\n* Update README.md\n* Update README.md\n* Update llama.py\n* DoRA\n* Update _utils.py\n* Update chat_templates.py\n* Update pyproject.toml\n* Small fixes\n* Update pyproject.toml\n* Approx gelu\n* Update geglu.py\n* Approx gelu\n* Update llama.py\n* Update __init__.py\n* Update __init__.py\n* Update _utils.py\n* Update geglu.py\n* Update gemma.py\n* Update rms_layernorm.py\n* Update rms_layernorm.py\n* Update rms_layernorm.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Update gemma.py\n* Fix Gemma merging\n* Update rms_layernorm.py\n* Update gemma.py\n* Update pyproject.toml\n* Layernorms\n* Gemma precision\n* Update gemma.py\n* sqrt\n* Update gemma.py\n* Update save.py\n* RoPE and Gemma precision\n* Update rms_layernorm.py\n* Fix warning\n* Update chat_templates.py\n* Update chat_templates.py\n* Update save.py\n* Update save.py\n* Update save.py\n* Update chat_templates.py\n* Update llama.py\n* model_name\n* Update loader.py\n* Tokenizer overwritten\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update save.py\n* Accuracy\n* Revert\n* Update save.py\n* Update fast_lora.py\n* Update fast_lora.py\n* Update fast_lora.py\n* Update fast_lora.py\n* Update fast_lora.py\n* Update chat_templates.py\n* Update save.py\n* Update save.py\n* Update llama.py\n* Update llama.py\n* Account for DoRA\n* Update llama.py\n* Update save.py\n* GGUF incorrect\n* Update save.py\n* Update pyproject.toml\n* kaggle new\n* Update pyproject.toml\n* Update pyproject.toml\n* upcasting\n* Fix Colab\n* Update pyproject.toml\n* Update pyproject.toml\n* Update pyproject.toml\n* Update pyproject.toml\n* Update pyproject.toml\n* Update pyproject.toml\n* Update pyproject.toml\n* Update pyproject.toml\n* Update chat_templates.py\n* Update chat_templates.py\n* Update chat_templates.py\n* Update chat_templates.py\n* Update chat_templates.py\n* Update pyproject.toml\n* Update pyproject.toml\n* Update pyproject.toml\n* Update rope_embedding.py\n* Update rope_embedding.py\n* Fix bugs\n* Update fast_lora.py\n* Update fast_lora.py\n* Update README.md\n* Update README.md\n* GGUF\n* Update save.py\n* Update save.py\n* Update save.py\n* Update save.py\n* Update README.md\n* Update README.md\n* Bugs\n* Update fast_lora.py\n* Update pyproject.toml\n* Update fast_lora.py\n* Update __init__.py\n* Update fast_lora.py\n* dtype\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* dtype\n* Update mistral.py\n* trust_remote_code\n* lm_head\n* Update llama.py\n* save_pretrained_settings\n* Update save.py\n* Update save.py\n* Update save.py\n* Update save.py\n* Update save.py\n* Update save.py\n* Update save.py\n* Update save.py\n* Update save.py\n* Update save.py\n* Update save.py\n* Update save.py\n* state_dict\n* Update save.py\n* whoami\n* Update llama.py\n* Update save.py\n* Update llama.py\n* Patch tokenizer\n* Update chat_templates.py\n* Heal tokenizers\n* Update chat_templates.py\n* Update mapper.py\n* Update tokenizer_utils.py\n* Update tokenizer_utils.py\n* Update tokenizer_utils.py\n* Update tokenizer_utils.py\n* Update tokenizer_utils.py\n* Update chat_templates.py\n* tokenizer patching\n* patch_tokenizer\n* Update chat_templates.py\n* Update tokenizer_utils.py\n* Update chat_templates.py\n* Update chat_templates.py\n* Update chat_templates.py\n* Update tokenizer_utils.py\n* Edit\")| Mar 27, 2024  \n[README.md](/unslothai/unsloth/blob/main/README.md \"README.md\")| [README.md](/unslothai/unsloth/blob/main/README.md \"README.md\")| [Update README.md](/unslothai/unsloth/commit/0546d6793a45aff68c5a83c38bf6be003dd57e00 \"Update README.md\")| Jan 21, 2025  \n[pyproject.toml](/unslothai/unsloth/blob/main/pyproject.toml \"pyproject.toml\")| [pyproject.toml](/unslothai/unsloth/blob/main/pyproject.toml \"pyproject.toml\")| [Fix Mistral, Qwen (](/unslothai/unsloth/commit/d8c58fbbb7d59dfa13edba89c13301e60ccdbaf6 \"Fix Mistral, Qwen \\(#1565\\)\n* use exact model name\n* Update save.py\n* Update _utils.py\n* Update _utils.py\n* Update _utils.py\n* Update _utils.py\n* print\n* Update _utils.py\n* Update _utils.py\n* Update llama.py\n* Update _utils.py\n* Update vision.py\n* Update _utils.py\n* Update _utils.py\n* Update _utils.py\n* Update _utils.py\n* Update _utils.py\n* Update _utils.py\n* Update _utils.py\n* Update _utils.py\n* Update loader.py\n* accurate_accumulation\n* Update loader.py\n* Update loader.py\n* Update _utils.py\n* Update loader.py\n* Update loader.py\n* Update loader.py\n* Update loader.py\n* Update pyproject.toml\n* Update __init__.py\n* Update pyproject.toml\n* Update __init__.py\n* Update __init__.py\n* Fix Triton heuristics\nhttps://github.com/triton-lang/triton/issues/5224\n* Update __init__.py\n* Update __init__.py\n* Update __init__.py\n* Update __init__.py\n* Xformers\n* Update loader.py\n* Update loader.py\n* Rewind\n* Update _utils.py\n* Update _utils.py\n* requires grad\n* Update loader.py\n* Update _utils.py\n* Update loader.py\n* changing model to base_model if peft model is already used\n* Improve debugging experience \\(#1512\\)\n* Create CONTRIBUTING.md \\(#1472\\)\nCreating contributing guidelines\n* Update CONTRIBUTING.md\nimproved sentence\n* Improve logging control in `unsloth_compile_transformers` by conditionally redirecting stdout based on UNSLOTH_DISABLE_LOGGER environment variable\n---------\nCo-authored-by: Michael Han <107991372+shimmyshimmer@users.noreply.github.com>\nCo-authored-by: Nino Risteski <95188570+NinoRisteski@users.noreply.github.com>\n* Update loader.py\n* Update llama.py\n* Update llama.py\n* Revert \"Update llama.py\"\nThis reverts commit b7ddf962d2f398be0286602d0fbb5b11e317887b.\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Auto change is_bfloat16_supported\n* Update llama.py\n* Force data-type\n* Update llama.py\n* All attention refactor fix \\(#1491\\)\n* change initilization of n_heads, n_kv_heads, hidden_size in llama.py\n* do the same for cohere, mistral, gemma2, granite\n* do the same for flexattention,cohere, mistral, granite\n* Update llama.py\n* Update llama.py\n* Update granite to work with latest post_patch methods \\(#1502\\)\n* Update granite to work with latest post_patch methods\n* Pass position_embeddings for granite even if transformers<4.47\n* Update llama.py\n---------\nCo-authored-by: Daniel Han <danielhanchen@gmail.com>\n* Minor fixes for granite models \\(#1503\\)\n* Update granite.py\nGrab residual multiplier directly from layer\n* Update llama.py\nVersion should read >= 4.47.1 as that is the version requiring the changes\n* Update granite.py\n* Update llama.py\n---------\nCo-authored-by: Daniel Han <danielhanchen@gmail.com>\n* support modelscope models and datasets \\(#1481\\)\n* support modelscope\n* change modelscope args\n* remove useless import\n* remove useless import\n* fix\n* wip\n* fix\n* remove useless code\n* add readme\n* add some comments\n* change print to raise error\n* update comment\n* Update loader.py\n---------\nCo-authored-by: Daniel Han <danielhanchen@gmail.com>\n* Merge branch 'main' into nightly\n* Phi 4\n* Update llama.py\n* Torch.Cuda Is Available Condition and Warning \\(#1545\\)\n* check for torch.cuda and triton if available\non my machine\\(mac m3\\) the cuda were not available\n* Update pyproject.toml\n* Update __init__.py\n---------\nCo-authored-by: Daniel Han <danielhanchen@gmail.com>\n* Update mistral.py\n* Update mistral.py\n* Update _utils.py\n* Update _utils.py\n* Update _utils.py\n* Update _utils.py\n* Update _utils.py\n* Fix\n* Bug fixes\n* Update mapper.py\n* Add dropout to granite to match HF's implementation \\(#1557\\)\nSigned-off-by: datta0 <venkatadattasainimmaturi@gmail.com>\n* Update llama.py\n* Update llama.py\n* Bug fixes\n* fix: flash_attn_detection_error \\(#1556\\)\n* fix: flash_attn_detection_error\n* Update _utils.py\n---------\nCo-authored-by: Daniel Han <danielhanchen@gmail.com>\n---------\nSigned-off-by: datta0 <venkatadattasainimmaturi@gmail.com>\nCo-authored-by: Itsuro Tajima <tajima@georepublic.de>\nCo-authored-by: Muhammad Osama <muhammadosama1994@gmail.com>\nCo-authored-by: Edd <68678137+Erland366@users.noreply.github.com>\nCo-authored-by: Michael Han <107991372+shimmyshimmer@users.noreply.github.com>\nCo-authored-by: Nino Risteski <95188570+NinoRisteski@users.noreply.github.com>\nCo-authored-by: Kareem <81531392+KareemMusleh@users.noreply.github.com>\nCo-authored-by: Datta Nimmaturi <datta.nimmaturi@nutanix.com>\nCo-authored-by: Z <coffeevampirebusiness@gmail.com>\nCo-authored-by: tastelikefeet <58414341+tastelikefeet@users.noreply.github.com>\nCo-authored-by: AminWhat <88392440+aminwhat@users.noreply.github.com>\nCo-authored-by: Zhe Zhang <2631992879@qq.com>\")[#1565](https://github.com/unslothai/unsloth/pull/1565)[)](/unslothai/unsloth/commit/d8c58fbbb7d59dfa13edba89c13301e60ccdbaf6 \"Fix Mistral, Qwen \\(#1565\\)\n* use exact model name\n* Update save.py\n* Update _utils.py\n* Update _utils.py\n* Update _utils.py\n* Update _utils.py\n* print\n* Update _utils.py\n* Update _utils.py\n* Update llama.py\n* Update _utils.py\n* Update vision.py\n* Update _utils.py\n* Update _utils.py\n* Update _utils.py\n* Update _utils.py\n* Update _utils.py\n* Update _utils.py\n* Update _utils.py\n* Update _utils.py\n* Update loader.py\n* accurate_accumulation\n* Update loader.py\n* Update loader.py\n* Update _utils.py\n* Update loader.py\n* Update loader.py\n* Update loader.py\n* Update loader.py\n* Update pyproject.toml\n* Update __init__.py\n* Update pyproject.toml\n* Update __init__.py\n* Update __init__.py\n* Fix Triton heuristics\nhttps://github.com/triton-lang/triton/issues/5224\n* Update __init__.py\n* Update __init__.py\n* Update __init__.py\n* Update __init__.py\n* Xformers\n* Update loader.py\n* Update loader.py\n* Rewind\n* Update _utils.py\n* Update _utils.py\n* requires grad\n* Update loader.py\n* Update _utils.py\n* Update loader.py\n* changing model to base_model if peft model is already used\n* Improve debugging experience \\(#1512\\)\n* Create CONTRIBUTING.md \\(#1472\\)\nCreating contributing guidelines\n* Update CONTRIBUTING.md\nimproved sentence\n* Improve logging control in `unsloth_compile_transformers` by conditionally redirecting stdout based on UNSLOTH_DISABLE_LOGGER environment variable\n---------\nCo-authored-by: Michael Han <107991372+shimmyshimmer@users.noreply.github.com>\nCo-authored-by: Nino Risteski <95188570+NinoRisteski@users.noreply.github.com>\n* Update loader.py\n* Update llama.py\n* Update llama.py\n* Revert \"Update llama.py\"\nThis reverts commit b7ddf962d2f398be0286602d0fbb5b11e317887b.\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Auto change is_bfloat16_supported\n* Update llama.py\n* Force data-type\n* Update llama.py\n* All attention refactor fix \\(#1491\\)\n* change initilization of n_heads, n_kv_heads, hidden_size in llama.py\n* do the same for cohere, mistral, gemma2, granite\n* do the same for flexattention,cohere, mistral, granite\n* Update llama.py\n* Update llama.py\n* Update granite to work with latest post_patch methods \\(#1502\\)\n* Update granite to work with latest post_patch methods\n* Pass position_embeddings for granite even if transformers<4.47\n* Update llama.py\n---------\nCo-authored-by: Daniel Han <danielhanchen@gmail.com>\n* Minor fixes for granite models \\(#1503\\)\n* Update granite.py\nGrab residual multiplier directly from layer\n* Update llama.py\nVersion should read >= 4.47.1 as that is the version requiring the changes\n* Update granite.py\n* Update llama.py\n---------\nCo-authored-by: Daniel Han <danielhanchen@gmail.com>\n* support modelscope models and datasets \\(#1481\\)\n* support modelscope\n* change modelscope args\n* remove useless import\n* remove useless import\n* fix\n* wip\n* fix\n* remove useless code\n* add readme\n* add some comments\n* change print to raise error\n* update comment\n* Update loader.py\n---------\nCo-authored-by: Daniel Han <danielhanchen@gmail.com>\n* Merge branch 'main' into nightly\n* Phi 4\n* Update llama.py\n* Torch.Cuda Is Available Condition and Warning \\(#1545\\)\n* check for torch.cuda and triton if available\non my machine\\(mac m3\\) the cuda were not available\n* Update pyproject.toml\n* Update __init__.py\n---------\nCo-authored-by: Daniel Han <danielhanchen@gmail.com>\n* Update mistral.py\n* Update mistral.py\n* Update _utils.py\n* Update _utils.py\n* Update _utils.py\n* Update _utils.py\n* Update _utils.py\n* Fix\n* Bug fixes\n* Update mapper.py\n* Add dropout to granite to match HF's implementation \\(#1557\\)\nSigned-off-by: datta0 <venkatadattasainimmaturi@gmail.com>\n* Update llama.py\n* Update llama.py\n* Bug fixes\n* fix: flash_attn_detection_error \\(#1556\\)\n* fix: flash_attn_detection_error\n* Update _utils.py\n---------\nCo-authored-by: Daniel Han <danielhanchen@gmail.com>\n---------\nSigned-off-by: datta0 <venkatadattasainimmaturi@gmail.com>\nCo-authored-by: Itsuro Tajima <tajima@georepublic.de>\nCo-authored-by: Muhammad Osama <muhammadosama1994@gmail.com>\nCo-authored-by: Edd <68678137+Erland366@users.noreply.github.com>\nCo-authored-by: Michael Han <107991372+shimmyshimmer@users.noreply.github.com>\nCo-authored-by: Nino Risteski <95188570+NinoRisteski@users.noreply.github.com>\nCo-authored-by: Kareem <81531392+KareemMusleh@users.noreply.github.com>\nCo-authored-by: Datta Nimmaturi <datta.nimmaturi@nutanix.com>\nCo-authored-by: Z <coffeevampirebusiness@gmail.com>\nCo-authored-by: tastelikefeet <58414341+tastelikefeet@users.noreply.github.com>\nCo-authored-by: AminWhat <88392440+aminwhat@users.noreply.github.com>\nCo-authored-by: Zhe Zhang <2631992879@qq.com>\")| Jan 20, 2025  \n[unsloth-cli.py](/unslothai/unsloth/blob/main/unsloth-cli.py \"unsloth-cli.py\")| [unsloth-cli.py](/unslothai/unsloth/blob/main/unsloth-cli.py \"unsloth-cli.py\")| [Bug fixes (](/unslothai/unsloth/commit/c14046ea4a68f73dc3de29223b0d805382320e9f \"Bug fixes \\(#1516\\)\n* use exact model name\n* Update save.py\n* Update _utils.py\n* Update _utils.py\n* Update _utils.py\n* Update _utils.py\n* print\n* Update _utils.py\n* Update _utils.py\n* Update llama.py\n* Update _utils.py\n* Update vision.py\n* Update _utils.py\n* Update _utils.py\n* Update _utils.py\n* Update _utils.py\n* Update _utils.py\n* Update _utils.py\n* Update _utils.py\n* Update _utils.py\n* Update loader.py\n* accurate_accumulation\n* Update loader.py\n* Update loader.py\n* Update _utils.py\n* Update loader.py\n* Update loader.py\n* Update loader.py\n* Update loader.py\n* Update pyproject.toml\n* Update __init__.py\n* Update pyproject.toml\n* Update __init__.py\n* Update __init__.py\n* Fix Triton heuristics\nhttps://github.com/triton-lang/triton/issues/5224\n* Update __init__.py\n* Update __init__.py\n* Update __init__.py\n* Update __init__.py\n* Xformers\n* Update loader.py\n* Update loader.py\n* Rewind\n* Update _utils.py\n* Update _utils.py\n* requires grad\n* Update loader.py\n* Update _utils.py\n* Update loader.py\n* changing model to base_model if peft model is already used\n* Improve debugging experience \\(#1512\\)\n* Create CONTRIBUTING.md \\(#1472\\)\nCreating contributing guidelines\n* Update CONTRIBUTING.md\nimproved sentence\n* Improve logging control in `unsloth_compile_transformers` by conditionally redirecting stdout based on UNSLOTH_DISABLE_LOGGER environment variable\n---------\nCo-authored-by: Michael Han <107991372+shimmyshimmer@users.noreply.github.com>\nCo-authored-by: Nino Risteski <95188570+NinoRisteski@users.noreply.github.com>\n* Update loader.py\n* Update llama.py\n* Update llama.py\n* Revert \"Update llama.py\"\nThis reverts commit b7ddf962d2f398be0286602d0fbb5b11e317887b.\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Auto change is_bfloat16_supported\n* Update llama.py\n* Force data-type\n* Update llama.py\n* All attention refactor fix \\(#1491\\)\n* change initilization of n_heads, n_kv_heads, hidden_size in llama.py\n* do the same for cohere, mistral, gemma2, granite\n* do the same for flexattention,cohere, mistral, granite\n* Update llama.py\n* Update llama.py\n* Update granite to work with latest post_patch methods \\(#1502\\)\n* Update granite to work with latest post_patch methods\n* Pass position_embeddings for granite even if transformers<4.47\n* Update llama.py\n---------\nCo-authored-by: Daniel Han <danielhanchen@gmail.com>\n* Minor fixes for granite models \\(#1503\\)\n* Update granite.py\nGrab residual multiplier directly from layer\n* Update llama.py\nVersion should read >= 4.47.1 as that is the version requiring the changes\n* Update granite.py\n* Update llama.py\n---------\nCo-authored-by: Daniel Han <danielhanchen@gmail.com>\n* support modelscope models and datasets \\(#1481\\)\n* support modelscope\n* change modelscope args\n* remove useless import\n* remove useless import\n* fix\n* wip\n* fix\n* remove useless code\n* add readme\n* add some comments\n* change print to raise error\n* update comment\n* Update loader.py\n---------\nCo-authored-by: Daniel Han <danielhanchen@gmail.com>\n---------\nCo-authored-by: Itsuro Tajima <tajima@georepublic.de>\nCo-authored-by: Muhammad Osama <muhammadosama1994@gmail.com>\nCo-authored-by: Edd <68678137+Erland366@users.noreply.github.com>\nCo-authored-by: Michael Han <107991372+shimmyshimmer@users.noreply.github.com>\nCo-authored-by: Nino Risteski <95188570+NinoRisteski@users.noreply.github.com>\nCo-authored-by: Kareem <81531392+KareemMusleh@users.noreply.github.com>\nCo-authored-by: Datta Nimmaturi <datta.nimmaturi@nutanix.com>\nCo-authored-by: Z <coffeevampirebusiness@gmail.com>\nCo-authored-by: tastelikefeet <58414341+tastelikefeet@users.noreply.github.com>\")[#1516](https://github.com/unslothai/unsloth/pull/1516)[)](/unslothai/unsloth/commit/c14046ea4a68f73dc3de29223b0d805382320e9f \"Bug fixes \\(#1516\\)\n* use exact model name\n* Update save.py\n* Update _utils.py\n* Update _utils.py\n* Update _utils.py\n* Update _utils.py\n* print\n* Update _utils.py\n* Update _utils.py\n* Update llama.py\n* Update _utils.py\n* Update vision.py\n* Update _utils.py\n* Update _utils.py\n* Update _utils.py\n* Update _utils.py\n* Update _utils.py\n* Update _utils.py\n* Update _utils.py\n* Update _utils.py\n* Update loader.py\n* accurate_accumulation\n* Update loader.py\n* Update loader.py\n* Update _utils.py\n* Update loader.py\n* Update loader.py\n* Update loader.py\n* Update loader.py\n* Update pyproject.toml\n* Update __init__.py\n* Update pyproject.toml\n* Update __init__.py\n* Update __init__.py\n* Fix Triton heuristics\nhttps://github.com/triton-lang/triton/issues/5224\n* Update __init__.py\n* Update __init__.py\n* Update __init__.py\n* Update __init__.py\n* Xformers\n* Update loader.py\n* Update loader.py\n* Rewind\n* Update _utils.py\n* Update _utils.py\n* requires grad\n* Update loader.py\n* Update _utils.py\n* Update loader.py\n* changing model to base_model if peft model is already used\n* Improve debugging experience \\(#1512\\)\n* Create CONTRIBUTING.md \\(#1472\\)\nCreating contributing guidelines\n* Update CONTRIBUTING.md\nimproved sentence\n* Improve logging control in `unsloth_compile_transformers` by conditionally redirecting stdout based on UNSLOTH_DISABLE_LOGGER environment variable\n---------\nCo-authored-by: Michael Han <107991372+shimmyshimmer@users.noreply.github.com>\nCo-authored-by: Nino Risteski <95188570+NinoRisteski@users.noreply.github.com>\n* Update loader.py\n* Update llama.py\n* Update llama.py\n* Revert \"Update llama.py\"\nThis reverts commit b7ddf962d2f398be0286602d0fbb5b11e317887b.\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Update llama.py\n* Auto change is_bfloat16_supported\n* Update llama.py\n* Force data-type\n* Update llama.py\n* All attention refactor fix \\(#1491\\)\n* change initilization of n_heads, n_kv_heads, hidden_size in llama.py\n* do the same for cohere, mistral, gemma2, granite\n* do the same for flexattention,cohere, mistral, granite\n* Update llama.py\n* Update llama.py\n* Update granite to work with latest post_patch methods \\(#1502\\)\n* Update granite to work with latest post_patch methods\n* Pass position_embeddings for granite even if transformers<4.47\n* Update llama.py\n---------\nCo-authored-by: Daniel Han <danielhanchen@gmail.com>\n* Minor fixes for granite models \\(#1503\\)\n* Update granite.py\nGrab residual multiplier directly from layer\n* Update llama.py\nVersion should read >= 4.47.1 as that is the version requiring the changes\n* Update granite.py\n* Update llama.py\n---------\nCo-authored-by: Daniel Han <danielhanchen@gmail.com>\n* support modelscope models and datasets \\(#1481\\)\n* support modelscope\n* change modelscope args\n* remove useless import\n* remove useless import\n* fix\n* wip\n* fix\n* remove useless code\n* add readme\n* add some comments\n* change print to raise error\n* update comment\n* Update loader.py\n---------\nCo-authored-by: Daniel Han <danielhanchen@gmail.com>\n---------\nCo-authored-by: Itsuro Tajima <tajima@georepublic.de>\nCo-authored-by: Muhammad Osama <muhammadosama1994@gmail.com>\nCo-authored-by: Edd <68678137+Erland366@users.noreply.github.com>\nCo-authored-by: Michael Han <107991372+shimmyshimmer@users.noreply.github.com>\nCo-authored-by: Nino Risteski <95188570+NinoRisteski@users.noreply.github.com>\nCo-authored-by: Kareem <81531392+KareemMusleh@users.noreply.github.com>\nCo-authored-by: Datta Nimmaturi <datta.nimmaturi@nutanix.com>\nCo-authored-by: Z <coffeevampirebusiness@gmail.com>\nCo-authored-by: tastelikefeet <58414341+tastelikefeet@users.noreply.github.com>\")| Jan 7, 2025  \nView all files  \n  \n## Repository files navigation\n\n  * [README](#)\n  * [Apache-2.0 license](#)\n\n\n\n[ ![unsloth logo](https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png) ](https://unsloth.ai)\n\n[![](https://raw.githubusercontent.com/unslothai/unsloth/main/images/start free finetune button.png)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_\\(8B\\)-Alpaca.ipynb) [![](https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord button.png)](https://discord.gg/unsloth) [![](https://raw.githubusercontent.com/unslothai/unsloth/refs/heads/main/images/Documentation%20Button.png)](https://docs.unsloth.ai)\n\n### Finetune Llama 3.3, Mistral, Phi-4, Qwen 2.5 & Gemma 2-5x faster with 80% less memory!\n\n[](#finetune-llama-33-mistral-phi-4-qwen-25--gemma-2-5x-faster-with-80-less-memory)\n\n[![](https://camo.githubusercontent.com/e93226ff5ba5be911d9fa41864c58db621a09ba17e14fe8f98f63e124431c927/68747470733a2f2f692e6962622e636f2f734a37526847472f696d6167652d34312e706e67)](https://camo.githubusercontent.com/e93226ff5ba5be911d9fa41864c58db621a09ba17e14fe8f98f63e124431c927/68747470733a2f2f692e6962622e636f2f734a37526847472f696d6167652d34312e706e67)\n\n## â¨ Finetune for Free\n\n[](#-finetune-for-free)\n\nAll notebooks are **beginner friendly**! Add your dataset, click \"Run All\", and you'll get a 2x faster finetuned model which can be exported to GGUF, Ollama, vLLM or uploaded to Hugging Face.\n\nUnsloth supports | Free Notebooks | Performance | Memory use  \n---|---|---|---  \n**Llama 3.2 (3B)** | [â¶ï¸ Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_\\(1B_and_3B\\)-Conversational.ipynb) | 2x faster | 60% less  \n**Phi-4 (14B)** | [â¶ï¸ Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4-Conversational.ipynb) | 2x faster | 50% less  \n**Llama 3.2 Vision (11B)** | [â¶ï¸ Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_\\(11B\\)-Vision.ipynb) | 2x faster | 40% less  \n**Llama 3.1 (8B)** | [â¶ï¸ Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_\\(8B\\)-Alpaca.ipynb) | 2x faster | 60% less  \n**Gemma 2 (9B)** | [â¶ï¸ Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma2_\\(9B\\)-Alpaca.ipynb) | 2x faster | 63% less  \n**Qwen 2.5 (7B)** | [â¶ï¸ Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_\\(7B\\)-Alpaca.ipynb) | 2x faster | 63% less  \n**Mistral v0.3 (7B)** | [â¶ï¸ Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_\\(7B\\)-Conversational.ipynb) | 2.2x faster | 73% less  \n**Ollama** | [â¶ï¸ Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_\\(8B\\)-Ollama.ipynb) | 1.9x faster | 43% less  \n**ORPO** | [â¶ï¸ Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_\\(8B\\)-ORPO.ipynb) | 1.9x faster | 43% less  \n**DPO Zephyr** | [â¶ï¸ Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Zephyr_\\(7B\\)-DPO.ipynb) | 1.9x faster | 43% less  \n  \n  * See [all our notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks) and [all our models](https://docs.unsloth.ai/get-started/all-our-models)\n  * **Kaggle Notebooks** for [Llama 3.2 Kaggle notebook](https://www.kaggle.com/danielhanchen/kaggle-llama-3-2-1b-3b-unsloth-notebook), [Llama 3.1 (8B)](https://www.kaggle.com/danielhanchen/kaggle-llama-3-1-8b-unsloth-notebook), [Gemma 2 (9B)](https://www.kaggle.com/code/danielhanchen/kaggle-gemma-7b-unsloth-notebook/), [Mistral (7B)](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)\n  * Run notebooks for [Llama 3.2 conversational](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_\\(1B_and_3B\\)-Conversational.ipynb), [Llama 3.1 conversational](https://colab.research.google.com/drive/15OyFkGoCImV9dSsewU1wa2JuKB4-mDE_?usp=sharing) and [Mistral v0.3 ChatML](https://colab.research.google.com/drive/15F1xyn8497_dUbxZP4zWmPZ3PJx1Oymv?usp=sharing)\n  * This [text completion notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_\\(7B\\)-Text_Completion.ipynb) is for continued pretraining / raw text\n  * This [continued pretraining notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_\\(7B\\)-CPT.ipynb) is for learning another language\n  * Click [here](https://docs.unsloth.ai/) for detailed documentation for Unsloth.\n\n\n\n## ð¦¥ Unsloth.ai News\n\n[](#-unslothai-news)\n\n  * ð£ NEW! [DeepSeek-R1](https://unsloth.ai/blog/deepseek-r1) - the most powerful open reasoning models with Llama & Qwen distillations. Run or fine-tune them now! More details: [unsloth.ai/blog/deepseek-r1](https://unsloth.ai/blog/deepseek-r1). All model uploads: [here](https://huggingface.co/collections/unsloth/deepseek-r1-all-versions-678e1c48f5d2fce87892ace5).\n  * ð£ NEW! [Phi-4](https://unsloth.ai/blog/phi4) by Microsoft is now supported. We also [fixed bugs](https://unsloth.ai/blog/phi4) in Phi-4 and [uploaded GGUFs, 4-bit](https://huggingface.co/collections/unsloth/phi-4-all-versions-677eecf93784e61afe762afa). Try the [Phi-4 Colab notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4-Conversational.ipynb)\n  * ð£ NEW! [Llama 3.3 (70B)](https://huggingface.co/collections/unsloth/llama-33-all-versions-67535d7d994794b9d7cf5e9f), Meta's latest model is supported.\n  * ð£ NEW! We worked with Apple to add [Cut Cross Entropy](https://arxiv.org/abs/2411.09009). Unsloth now supports 89K context for Meta's Llama 3.3 (70B) on a 80GB GPU - 13x longer than HF+FA2. For Llama 3.1 (8B), Unsloth enables 342K context, surpassing its native 128K support.\n  * ð£ Introducing Unsloth [Dynamic 4-bit Quantization](https://unsloth.ai/blog/dynamic-4bit)! We dynamically opt not to quantize certain parameters and this greatly increases accuracy while only using <10% more VRAM than BnB 4-bit. See our collection on [Hugging Face here.](https://huggingface.co/collections/unsloth/unsloth-4-bit-dynamic-quants-67503bb873f89e15276c44e7)\n  * ð£ [Vision models](https://unsloth.ai/blog/vision) now supported! [Llama 3.2 Vision (11B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_\\(11B\\)-Vision.ipynb), [Qwen 2.5 VL (7B)](https://colab.research.google.com/drive/1whHb54GNZMrNxIsi2wm2EY_-Pvo2QyKh?usp=sharing) and [Pixtral (12B) 2409](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_VL_\\(7B\\)-Vision.ipynb)\n\nClick for more news\n\n  * ð£ We found and helped fix a [gradient accumulation bug](https://unsloth.ai/blog/gradient)! Please update Unsloth and transformers.\n  * ð£ Try out [Chat interface](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Unsloth_Studio.ipynb)!\n  * ð£ NEW! Qwen-2.5 including [Coder](https://unsloth.ai/blog/qwen-coder) models are now supported with bugfixes. 14b fits in a Colab GPU! [Qwen 2.5 conversational notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_Coder_\\(14B\\)-Conversational.ipynb)\n  * ð£ NEW! [Mistral Small 22b notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_Small_\\(22B\\)-Alpaca.ipynb) finetuning fits in under 16GB of VRAM!\n  * ð£ NEW! `pip install unsloth` now works! Head over to [pypi](https://pypi.org/project/unsloth/) to check it out! This allows non git pull installs. Use `pip install unsloth[colab-new]` for non dependency installs.\n  * ð£ NEW! Continued Pretraining [notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_\\(7B\\)-CPT.ipynb) for other languages like Korean!\n  * ð£ [2x faster inference](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_\\(8B\\)-Inference.ipynb) added for all our models\n  * ð£ We cut memory usage by a [further 30%](https://unsloth.ai/blog/long-context) and now support [4x longer context windows](https://unsloth.ai/blog/long-context)!\n\n\n\n## ð Links and Resources\n\n[](#-links-and-resources)\n\nType | Links  \n---|---  \nð **Documentation & Wiki** | [Read Our Docs](https://docs.unsloth.ai)  \n[![](https://camo.githubusercontent.com/2d318d2de87051f80a781377bcf094af5e9a9648beb1f93cb3471cf4d5da0af0/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f362f36662f4c6f676f5f6f665f547769747465722e737667)](https://camo.githubusercontent.com/2d318d2de87051f80a781377bcf094af5e9a9648beb1f93cb3471cf4d5da0af0/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f362f36662f4c6f676f5f6f665f547769747465722e737667) **Twitter (aka X)** | [Follow us on X](https://twitter.com/unslothai)  \nð¾ **Installation** | [unsloth/README.md](https://github.com/unslothai/unsloth/tree/main#-installation-instructions)  \nð¥ **Benchmarking** | [Performance Tables](https://github.com/unslothai/unsloth/tree/main#-performance-benchmarking)  \nð **Released Models** | [Unsloth Releases](https://docs.unsloth.ai/get-started/all-our-models)  \nâï¸ **Blog** | [Read our Blogs](https://unsloth.ai/blog)  \n[![](https://camo.githubusercontent.com/366c9dc2ccd72d171a31e5af3cdfb0b42ce61a6f22a20d3e53609ad9ccb47655/68747470733a2f2f726564646974696e632e636f6d2f68732d66732f68756266732f526564646974253230496e632f4272616e642f5265646469745f4c6f676f2e706e67)](https://camo.githubusercontent.com/366c9dc2ccd72d171a31e5af3cdfb0b42ce61a6f22a20d3e53609ad9ccb47655/68747470733a2f2f726564646974696e632e636f6d2f68732d66732f68756266732f526564646974253230496e632f4272616e642f5265646469745f4c6f676f2e706e67) **Reddit** | [Join our Reddit page](https://reddit.com/r/unsloth)  \n  \n## â­ Key Features\n\n[](#-key-features)\n\n  * All kernels written in [OpenAI's Triton](https://openai.com/research/triton) language. **Manual backprop engine**.\n  * **0% loss in accuracy** - no approximation methods - all exact.\n  * No change of hardware. Supports NVIDIA GPUs since 2018+. Minimum CUDA Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc) [Check your GPU!](https://developer.nvidia.com/cuda-gpus) GTX 1070, 1080 works, but is slow.\n  * Works on **Linux** and **Windows** via WSL.\n  * Supports 4bit and 16bit QLoRA / LoRA finetuning via [bitsandbytes](https://github.com/TimDettmers/bitsandbytes).\n  * Open source trains 5x faster - see [Unsloth Pro](https://unsloth.ai/) for up to **30x faster training**!\n  * If you trained a model with ð¦¥Unsloth, you can use this cool sticker! [![](https://raw.githubusercontent.com/unslothai/unsloth/main/images/made with unsloth.png)](https://raw.githubusercontent.com/unslothai/unsloth/main/images/made with unsloth.png)\n\n\n\n## ð¥ Performance Benchmarking\n\n[](#-performance-benchmarking)\n\n  * For our most detailed benchmarks, read our [Llama 3.3 Blog](https://unsloth.ai/blog/llama3-3).\n  * Benchmarking of Unsloth was also conducted by [ð¤Hugging Face](https://huggingface.co/blog/unsloth-trl).\n\n\n\nWe tested using the Alpaca Dataset, a batch size of 2, gradient accumulation steps of 4, rank = 32, and applied QLoRA on all linear layers (q, k, v, o, gate, up, down):\n\nModel | VRAM | ð¦¥ Unsloth speed | ð¦¥ VRAM reduction | ð¦¥ Longer context | ð Hugging Face + FA2  \n---|---|---|---|---|---  \nLlama 3.3 (70B) | 80GB | 2x | >75% | 13x longer | 1x  \nLlama 3.1 (8B) | 80GB | 2x | >70% | 12x longer | 1x  \n  \n[![](https://camo.githubusercontent.com/e93226ff5ba5be911d9fa41864c58db621a09ba17e14fe8f98f63e124431c927/68747470733a2f2f692e6962622e636f2f734a37526847472f696d6167652d34312e706e67)](https://camo.githubusercontent.com/e93226ff5ba5be911d9fa41864c58db621a09ba17e14fe8f98f63e124431c927/68747470733a2f2f692e6962622e636f2f734a37526847472f696d6167652d34312e706e67)\n\n## ð¾ Installation Instructions\n\n[](#-installation-instructions)\n\nFor stable releases, use `pip install unsloth`. We recommend `pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"` for most installations though.\n\n### Conda Installation\n\n[](#conda-installation)\n\n`â ï¸Only use Conda if you have it. If not, use Pip`. Select either `pytorch-cuda=11.8,12.1` for CUDA 11.8 or CUDA 12.1. We support `python=3.10,3.11,3.12`.\n\n```\nconda create --name unsloth_env \\ python=3.11 \\ pytorch-cuda=12.1 \\ pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers \\ -y conda activate unsloth_env pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" pip install --no-deps trl peft accelerate bitsandbytes\n```\n\nIf you're looking to install Conda in a Linux environment, [read here](https://docs.anaconda.com/miniconda/), or run the below ð½\n\n```\nmkdir -p ~/miniconda3 wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3 rm -rf ~/miniconda3/miniconda.sh ~/miniconda3/bin/conda init bash ~/miniconda3/bin/conda init zsh\n```\n\n### Pip Installation\n\n[](#pip-installation)\n\n`â ï¸Do **NOT** use this if you have Conda.` Pip is a bit more complex since there are dependency issues. The pip command is different for `torch 2.2,2.3,2.4,2.5` and CUDA versions.\n\nFor other torch versions, we support `torch211`, `torch212`, `torch220`, `torch230`, `torch240` and for CUDA versions, we support `cu118` and `cu121` and `cu124`. For Ampere devices (A100, H100, RTX3090) and above, use `cu118-ampere` or `cu121-ampere` or `cu124-ampere`.\n\nFor example, if you have `torch 2.4` and `CUDA 12.1`, use:\n\n```\npip install --upgrade pip pip install \"unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git\"\n```\n\nAnother example, if you have `torch 2.5` and `CUDA 12.4`, use:\n\n```\npip install --upgrade pip pip install \"unsloth[cu124-torch250] @ git+https://github.com/unslothai/unsloth.git\"\n```\n\nAnd other examples:\n\n```\npip install \"unsloth[cu121-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git\" pip install \"unsloth[cu118-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git\" pip install \"unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git\" pip install \"unsloth[cu118-torch240] @ git+https://github.com/unslothai/unsloth.git\" pip install \"unsloth[cu121-torch230] @ git+https://github.com/unslothai/unsloth.git\" pip install \"unsloth[cu121-ampere-torch230] @ git+https://github.com/unslothai/unsloth.git\" pip install \"unsloth[cu121-torch250] @ git+https://github.com/unslothai/unsloth.git\" pip install \"unsloth[cu124-ampere-torch250] @ git+https://github.com/unslothai/unsloth.git\"\n```\n\nOr, run the below in a terminal to get the **optimal** pip installation command:\n\n```\nwget -qO- https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/_auto_install.py | python -\n```\n\nOr, run the below manually in a Python REPL:\n\n```\ntry: import torch except: raise ImportError('Install torch via `pip install torch`') from packaging.version import Version as V v = V(torch.__version__) cuda = str(torch.version.cuda) is_ampere = torch.cuda.get_device_capability()[0] >= 8 if cuda != \"12.1\" and cuda != \"11.8\" and cuda != \"12.4\": raise RuntimeError(f\"CUDA = {cuda} not supported!\") if v <= V('2.1.0'): raise RuntimeError(f\"Torch = {v} too old!\") elif v <= V('2.1.1'): x = 'cu{}{}-torch211' elif v <= V('2.1.2'): x = 'cu{}{}-torch212' elif v < V('2.3.0'): x = 'cu{}{}-torch220' elif v < V('2.4.0'): x = 'cu{}{}-torch230' elif v < V('2.5.0'): x = 'cu{}{}-torch240' elif v < V('2.6.0'): x = 'cu{}{}-torch250' else: raise RuntimeError(f\"Torch = {v} too new!\") x = x.format(cuda.replace(\".\", \"\"), \"-ampere\" if is_ampere else \"\") print(f'pip install --upgrade pip && pip install \"unsloth[{x}] @ git+https://github.com/unslothai/unsloth.git\"')\n```\n\n### Windows Installation\n\n[](#windows-installation)\n\nTo run Unsloth directly on Windows:\n\n  * Install Triton from this Windows fork and follow the instructions: <https://github.com/woct0rdho/triton-windows>\n  * In the SFTTrainer, set `dataset_num_proc=1` to avoid a crashing issue:\n\n\n\n```\ntrainer = SFTTrainer( dataset_num_proc=1, ... )\n```\n\nFor **advanced installation instructions** or if you see weird errors during installations:\n\n  1. Install `torch` and `triton`. Go to <https://pytorch.org> to install it. For example `pip install torch torchvision torchaudio triton`\n  2. Confirm if CUDA is installated correctly. Try `nvcc`. If that fails, you need to install `cudatoolkit` or CUDA drivers.\n  3. Install `xformers` manually. You can try installing `vllm` and seeing if `vllm` succeeds. Check if `xformers` succeeded with `python -m xformers.info` Go to <https://github.com/facebookresearch/xformers>. Another option is to install `flash-attn` for Ampere GPUs.\n  4. Finally, install `bitsandbytes` and check it with `python -m bitsandbytes`\n\n\n\n## ð [Documentation](https://docs.unsloth.ai)\n\n[](#-documentation)\n\n  * Go to our official [Documentation](https://docs.unsloth.ai) for saving to GGUF, checkpointing, evaluation and more!\n  * We support Huggingface's TRL, Trainer, Seq2SeqTrainer or even Pytorch code!\n  * We're in ð¤Hugging Face's official docs! Check out the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!\n  * If you want to download models from the ModelScope community, please use an environment variable: `UNSLOTH_USE_MODELSCOPE=1`, and install the modelscope library by: `pip install modelscope -U`.\n\n\n\n> unsloth_cli.py also supports `UNSLOTH_USE_MODELSCOPE=1` to download models and datasets. please remember to use the model and dataset id in the ModelScope community.\n\n```\nfrom unsloth import FastLanguageModel from unsloth import is_bfloat16_supported import torch from trl import SFTTrainer from transformers import TrainingArguments from datasets import load_dataset max_seq_length = 2048 # Supports RoPE Scaling interally, so choose any! # Get LAION dataset url = \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\" dataset = load_dataset(\"json\", data_files = {\"train\" : url}, split = \"train\") # 4bit pre quantized models we support for 4x faster downloading + no OOMs. fourbit_models = [ \"unsloth/mistral-7b-v0.3-bnb-4bit\", # New Mistral v3 2x faster! \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\", \"unsloth/llama-3-8b-bnb-4bit\", # Llama-3 15 trillion tokens model 2x faster! \"unsloth/llama-3-8b-Instruct-bnb-4bit\", \"unsloth/llama-3-70b-bnb-4bit\", \"unsloth/Phi-3-mini-4k-instruct\", # Phi-3 2x faster! \"unsloth/Phi-3-medium-4k-instruct\", \"unsloth/mistral-7b-bnb-4bit\", \"unsloth/gemma-7b-bnb-4bit\", # Gemma 2.2x faster! ] # More models at https://huggingface.co/unsloth model, tokenizer = FastLanguageModel.from_pretrained( model_name = \"unsloth/llama-3-8b-bnb-4bit\", max_seq_length = max_seq_length, dtype = None, load_in_4bit = True, ) # Do model patching and add fast LoRA weights model = FastLanguageModel.get_peft_model( model, r = 16, target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\",], lora_alpha = 16, lora_dropout = 0, # Supports any, but = 0 is optimized bias = \"none\", # Supports any, but = \"none\" is optimized # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes! use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context random_state = 3407, max_seq_length = max_seq_length, use_rslora = False, # We support rank stabilized LoRA loftq_config = None, # And LoftQ ) trainer = SFTTrainer( model = model, train_dataset = dataset, dataset_text_field = \"text\", max_seq_length = max_seq_length, tokenizer = tokenizer, args = TrainingArguments( per_device_train_batch_size = 2, gradient_accumulation_steps = 4, warmup_steps = 10, max_steps = 60, fp16 = not is_bfloat16_supported(), bf16 = is_bfloat16_supported(), logging_steps = 1, output_dir = \"outputs\", optim = \"adamw_8bit\", seed = 3407, ), ) trainer.train() # Go to https://github.com/unslothai/unsloth/wiki for advanced tips like # (1) Saving to GGUF / merging to 16bit for vLLM # (2) Continued training from a saved LoRA adapter # (3) Adding an evaluation loop / OOMs # (4) Customized chat templates\n```\n\n## DPO Support\n\n[](#dpo-support)\n\nDPO (Direct Preference Optimization), PPO, Reward Modelling all seem to work as per 3rd party independent testing from [Llama-Factory](https://github.com/hiyouga/LLaMA-Factory). We have a preliminary Google Colab notebook for reproducing Zephyr on Tesla T4 here: [notebook](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing).\n\nWe're in ð¤Hugging Face's official docs! We're on the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and the [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!\n\n```\nimport os os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # Optional set GPU device ID from unsloth import FastLanguageModel, PatchDPOTrainer from unsloth import is_bfloat16_supported PatchDPOTrainer() import torch from transformers import TrainingArguments from trl import DPOTrainer model, tokenizer = FastLanguageModel.from_pretrained( model_name = \"unsloth/zephyr-sft-bnb-4bit\", max_seq_length = max_seq_length, dtype = None, load_in_4bit = True, ) # Do model patching and add fast LoRA weights model = FastLanguageModel.get_peft_model( model, r = 64, target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\",], lora_alpha = 64, lora_dropout = 0, # Supports any, but = 0 is optimized bias = \"none\", # Supports any, but = \"none\" is optimized # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes! use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context random_state = 3407, max_seq_length = max_seq_length, ) dpo_trainer = DPOTrainer( model = model, ref_model = None, args = TrainingArguments( per_device_train_batch_size = 4, gradient_accumulation_steps = 8, warmup_ratio = 0.1, num_train_epochs = 3, fp16 = not is_bfloat16_supported(), bf16 = is_bfloat16_supported(), logging_steps = 1, optim = \"adamw_8bit\", seed = 42, output_dir = \"outputs\", ), beta = 0.1, train_dataset = YOUR_DATASET_HERE, # eval_dataset = YOUR_DATASET_HERE, tokenizer = tokenizer, max_length = 1024, max_prompt_length = 512, ) dpo_trainer.train()\n```\n\n## ð¥ Detailed Benchmarking Tables\n\n[](#-detailed-benchmarking-tables)\n\n### Context length benchmarks\n\n[](#context-length-benchmarks)\n\n#### Llama 3.1 (8B) max. context length\n\n[](#llama-31-8b-max-context-length)\n\nWe tested Llama 3.1 (8B) Instruct and did 4bit QLoRA on all linear layers (Q, K, V, O, gate, up and down) with rank = 32 with a batch size of 1. We padded all sequences to a certain maximum sequence length to mimic long context finetuning workloads.\n\nGPU VRAM | ð¦¥Unsloth context length | Hugging Face + FA2  \n---|---|---  \n8 GB | 2,972 | OOM  \n12 GB | 21,848 | 932  \n16 GB | 40,724 | 2,551  \n24 GB | 78,475 | 5,789  \n40 GB | 153,977 | 12,264  \n48 GB | 191,728 | 15,502  \n80 GB | 342,733 | 28,454  \n  \n#### Llama 3.3 (70B) max. context length\n\n[](#llama-33-70b-max-context-length)\n\nWe tested Llama 3.3 (70B) Instruct on a 80GB A100 and did 4bit QLoRA on all linear layers (Q, K, V, O, gate, up and down) with rank = 32 with a batch size of 1. We padded all sequences to a certain maximum sequence length to mimic long context finetuning workloads.\n\nGPU VRAM | ð¦¥Unsloth context length | Hugging Face + FA2  \n---|---|---  \n48 GB | 12,106 | OOM  \n80 GB | 89,389 | 6,916  \n  \n[![](https://camo.githubusercontent.com/e93226ff5ba5be911d9fa41864c58db621a09ba17e14fe8f98f63e124431c927/68747470733a2f2f692e6962622e636f2f734a37526847472f696d6167652d34312e706e67)](https://camo.githubusercontent.com/e93226ff5ba5be911d9fa41864c58db621a09ba17e14fe8f98f63e124431c927/68747470733a2f2f692e6962622e636f2f734a37526847472f696d6167652d34312e706e67)\n\n### Citation\n\n[](#citation)\n\nYou can cite the Unsloth repo as follows:\n\n```\n@software{unsloth, author = {Daniel Han, Michael Han and Unsloth team}, title = {Unsloth}, url = {http://github.com/unslothai/unsloth}, year = {2023} }\n```\n\n### Thank You to\n\n[](#thank-you-to)\n\n  * [Erik](https://github.com/erikwijmans) for his help adding [Apple's ML Cross Entropy](https://github.com/apple/ml-cross-entropy) in Unsloth\n  * [HuyNguyen-hust](https://github.com/HuyNguyen-hust) for making [RoPE Embeddings 28% faster](https://github.com/unslothai/unsloth/pull/238)\n  * [RandomInternetPreson](https://github.com/RandomInternetPreson) for confirming WSL support\n  * [152334H](https://github.com/152334H) for experimental DPO support\n  * [atgctg](https://github.com/atgctg) for syntax highlighting\n\n\n\n## About\n\nFinetune Llama 3.3, Mistral, Phi-4, Qwen 2.5 & Gemma LLMs 2-5x faster with 70% less memory \n\n[unsloth.ai](https://unsloth.ai \"https://unsloth.ai\")\n\n### Topics\n\n[ llama ](/topics/llama \"Topic: llama\") [ lora ](/topics/lora \"Topic: lora\") [ gemma ](/topics/gemma \"Topic: gemma\") [ mistral ](/topics/mistral \"Topic: mistral\") [ fine-tuning ](/topics/fine-tuning \"Topic: fine-tuning\") [ finetuning ](/topics/finetuning \"Topic: finetuning\") [ llm ](/topics/llm \"Topic: llm\") [ llms ](/topics/llms \"Topic: llms\") [ qlora ](/topics/qlora \"Topic: qlora\") [ unsloth ](/topics/unsloth \"Topic: unsloth\") [ llama3 ](/topics/llama3 \"Topic: llama3\") [ phi3 ](/topics/phi3 \"Topic: phi3\") [ gemma2 ](/topics/gemma2 \"Topic: gemma2\")\n\n### Resources\n\n[ Readme ](#readme-ov-file)\n\n### License\n\n[ Apache-2.0 license ](#Apache-2.0-1-ov-file)\n\n[ Activity](/unslothai/unsloth/activity)\n\n[ Custom properties](/unslothai/unsloth/custom-properties)\n\n### Stars\n\n[ **21k** stars](/unslothai/unsloth/stargazers)\n\n### Watchers\n\n[ **139** watching](/unslothai/unsloth/watchers)\n\n### Forks\n\n[ **1.5k** forks](/unslothai/unsloth/forks)\n\n[ Report repository ](/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Funslothai%2Funsloth&report=unslothai+%28user%29)\n\n##  [Releases 9](/unslothai/unsloth/releases)\n\n[ Phi-4 & Bug Fixes Latest  Jan 10, 2025 ](/unslothai/unsloth/releases/tag/2025-01)\n\n[+ 8 releases](/unslothai/unsloth/releases)\n\n## Sponsor this project\n\n  * ![ko_fi](https://github.githubassets.com/assets/ko_fi-53a60c17e75c.svg) [ko-fi.com/**unsloth**](https://ko-fi.com/unsloth)\n\n\n\n##  [Packages 0](/orgs/unslothai/packages?repo_name=unsloth)\n\nNo packages published \n\n##  [Contributors 54](/unslothai/unsloth/graphs/contributors)\n\n  * [ ![@danielhanchen](https://avatars.githubusercontent.com/u/23090290?s=64&v=4) ](https://github.com/danielhanchen)\n  * [ ![@shimmyshimmer](https://avatars.githubusercontent.com/u/107991372?s=64&v=4) ](https://github.com/shimmyshimmer)\n  * [ ![@Erland366](https://avatars.githubusercontent.com/u/68678137?s=64&v=4) ](https://github.com/Erland366)\n  * [ ![@Datta0](https://avatars.githubusercontent.com/u/39181234?s=64&v=4) ](https://github.com/Datta0)\n  * [ ![@xyangk](https://avatars.githubusercontent.com/u/9495054?s=64&v=4) ](https://github.com/xyangk)\n  * [ ![@sebdg](https://avatars.githubusercontent.com/u/1187529?s=64&v=4) ](https://github.com/sebdg)\n  * [ ![@bet0x](https://avatars.githubusercontent.com/u/778862?s=64&v=4) ](https://github.com/bet0x)\n  * [ ![@neph1](https://avatars.githubusercontent.com/u/7988802?s=64&v=4) ](https://github.com/neph1)\n  * [ ![@t-vi](https://avatars.githubusercontent.com/u/20787943?s=64&v=4) ](https://github.com/t-vi)\n  * [ ![@Oseltamivir](https://avatars.githubusercontent.com/u/58582368?s=64&v=4) ](https://github.com/Oseltamivir)\n  * [ ![@chrehall68](https://avatars.githubusercontent.com/u/60240707?s=64&v=4) ](https://github.com/chrehall68)\n  * [ ![@mahiatlinux](https://avatars.githubusercontent.com/u/110882203?s=64&v=4) ](https://github.com/mahiatlinux)\n  * [ ![@shaper](https://avatars.githubusercontent.com/u/8998?s=64&v=4) ](https://github.com/shaper)\n  * [ ![@Rabbidon](https://avatars.githubusercontent.com/u/24900318?s=64&v=4) ](https://github.com/Rabbidon)\n\n\n\n[+ 40 contributors](/unslothai/unsloth/graphs/contributors)\n\n## Languages\n\n  * [ Python 100.0% ](/unslothai/unsloth/search?l=python)\n\n\n\n## Footer\n\n[ ](https://github.com \"GitHub\") Â© 2025 GitHub, Inc. \n\n### Footer navigation\n\n  * [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)\n  * [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)\n  * [Security](https://github.com/security)\n  * [Status](https://www.githubstatus.com/)\n  * [Docs](https://docs.github.com/)\n  * [Contact](https://support.github.com?tags=dotcom-footer)\n  * Manage cookies \n  * Do not share my personal information \n\n\n\nYou canât perform that action at this time. \n",
    "content_quality_score": null,
    "summary": null,
    "child_urls": [
        "https://github.com/unslothai/unsloth/#start-of-content",
        "https://github.com/",
        "https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Funslothai%2Funsloth%2F",
        "https://github.com/features/copilot",
        "https://github.com/features/security",
        "https://github.com/features/actions",
        "https://github.com/features/codespaces",
        "https://github.com/features/issues",
        "https://github.com/features/code-review",
        "https://github.com/features/discussions",
        "https://github.com/features/code-search",
        "https://github.com/features",
        "https://docs.github.com",
        "https://skills.github.com",
        "https://github.com/enterprise",
        "https://github.com/team",
        "https://github.com/enterprise/startups",
        "https://github.com/solutions/industry/nonprofits",
        "https://github.com/solutions/use-case/devsecops",
        "https://github.com/solutions/use-case/devops",
        "https://github.com/solutions/use-case/ci-cd",
        "https://github.com/solutions/use-case",
        "https://github.com/solutions/industry/healthcare",
        "https://github.com/solutions/industry/financial-services",
        "https://github.com/solutions/industry/manufacturing",
        "https://github.com/solutions/industry/government",
        "https://github.com/solutions/industry",
        "https://github.com/solutions",
        "https://github.com/resources/articles/ai",
        "https://github.com/resources/articles/devops",
        "https://github.com/resources/articles/security",
        "https://github.com/resources/articles/software-development",
        "https://github.com/resources/articles",
        "https://resources.github.com/learn/pathways",
        "https://resources.github.com",
        "https://github.com/customer-stories",
        "https://partner.github.com",
        "https://github.com/solutions/executive-insights",
        "https://github.com/sponsors",
        "https://github.com/readme",
        "https://github.com/topics",
        "https://github.com/trending",
        "https://github.com/collections",
        "https://github.com/enterprise/advanced-security",
        "https://github.com/features/copilot#enterprise",
        "https://github.com/premium-support",
        "https://github.com/pricing",
        "https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax",
        "https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=unslothai%2Funsloth",
        "https://github.com/unslothai",
        "https://github.com/unslothai/unsloth",
        "https://docs.github.com/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/displaying-a-sponsor-button-in-your-repository",
        "https://github.com/contact/report-abuse?report=unslothai%2Funsloth+%28Repository+Funding+Links%29",
        "https://github.com/login?return_to=%2Funslothai%2Funsloth",
        "https://github.com/unslothai/unsloth/blob/main/LICENSE",
        "https://github.com/unslothai/unsloth/stargazers",
        "https://github.com/unslothai/unsloth/forks",
        "https://github.com/unslothai/unsloth/branches",
        "https://github.com/unslothai/unsloth/tags",
        "https://github.com/unslothai/unsloth/activity",
        "https://github.com/unslothai/unsloth/issues",
        "https://github.com/unslothai/unsloth/pulls",
        "https://github.com/unslothai/unsloth/actions",
        "https://github.com/unslothai/unsloth/wiki",
        "https://github.com/unslothai/unsloth/security",
        "https://github.com/unslothai/unsloth/pulse",
        "https://github.com/shimmyshimmer",
        "https://github.com/unslothai/unsloth/commits?author=shimmyshimmer",
        "https://github.com/unslothai/unsloth/commit/d802bbf4e298cb0da1e976ab9670fbc1cbe3514c",
        "https://github.com/unslothai/unsloth/pull/1569",
        "https://github.com/unslothai/unsloth/commits/main/",
        "https://github.com/unslothai/unsloth/tree/main/.github",
        "https://github.com/unslothai/unsloth/commit/d6982c1fe6b814874f2ff989a69e485e6c13ab52",
        "https://github.com/unslothai/unsloth/tree/main/images",
        "https://github.com/unslothai/unsloth/commit/d30c363d25668b8059237c58586d0f2d10903682",
        "https://github.com/unslothai/unsloth/pull/1318",
        "https://github.com/unslothai/unsloth/tree/main/unsloth",
        "https://github.com/unslothai/unsloth/commit/fde26db11de0dc6b76a9c6ac21ce9c71d3593323",
        "https://github.com/unslothai/unsloth/blob/main/CONTRIBUTING.md",
        "https://github.com/unslothai/unsloth/commit/3a0eb2bbf42016dfeec924b320c420dabbce168b",
        "https://github.com/unslothai/unsloth/commit/a68aebc1fa17755ffbcdafc9239e7ca37ab21657",
        "https://github.com/unslothai/unsloth/pull/283",
        "https://github.com/unslothai/unsloth/blob/main/README.md",
        "https://github.com/unslothai/unsloth/commit/0546d6793a45aff68c5a83c38bf6be003dd57e00",
        "https://github.com/unslothai/unsloth/blob/main/pyproject.toml",
        "https://github.com/unslothai/unsloth/commit/d8c58fbbb7d59dfa13edba89c13301e60ccdbaf6",
        "https://github.com/unslothai/unsloth/pull/1565",
        "https://github.com/unslothai/unsloth/blob/main/unsloth-cli.py",
        "https://github.com/unslothai/unsloth/commit/c14046ea4a68f73dc3de29223b0d805382320e9f",
        "https://github.com/unslothai/unsloth/pull/1516",
        "https://github.com/unslothai/unsloth/",
        "https://github.com/unslothai/unsloth/#finetune-llama-33-mistral-phi-4-qwen-25--gemma-2-5x-faster-with-80-less-memory",
        "https://github.com/unslothai/unsloth/#-finetune-for-free",
        "https://github.com/unslothai/unsloth/#-unslothai-news",
        "https://github.com/unslothai/unsloth/#-links-and-resources",
        "https://github.com/unslothai/unsloth/tree/main#-installation-instructions",
        "https://github.com/unslothai/unsloth/tree/main#-performance-benchmarking",
        "https://github.com/unslothai/unsloth/#-key-features",
        "https://github.com/TimDettmers/bitsandbytes",
        "https://github.com/unslothai/unsloth/#-performance-benchmarking",
        "https://github.com/unslothai/unsloth/#-installation-instructions",
        "https://github.com/unslothai/unsloth/#conda-installation",
        "https://github.com/unslothai/unsloth/#pip-installation",
        "https://github.com/unslothai/unsloth/#windows-installation",
        "https://github.com/woct0rdho/triton-windows",
        "https://github.com/facebookresearch/xformers",
        "https://github.com/unslothai/unsloth/#-documentation",
        "https://github.com/unslothai/unsloth/#dpo-support",
        "https://github.com/hiyouga/LLaMA-Factory",
        "https://github.com/unslothai/unsloth/#-detailed-benchmarking-tables",
        "https://github.com/unslothai/unsloth/#context-length-benchmarks",
        "https://github.com/unslothai/unsloth/#llama-31-8b-max-context-length",
        "https://github.com/unslothai/unsloth/#llama-33-70b-max-context-length",
        "https://github.com/unslothai/unsloth/#citation",
        "https://github.com/unslothai/unsloth/#thank-you-to",
        "https://github.com/erikwijmans",
        "https://github.com/apple/ml-cross-entropy",
        "https://github.com/HuyNguyen-hust",
        "https://github.com/unslothai/unsloth/pull/238",
        "https://github.com/RandomInternetPreson",
        "https://github.com/152334H",
        "https://github.com/atgctg",
        "https://github.com/topics/llama",
        "https://github.com/topics/lora",
        "https://github.com/topics/gemma",
        "https://github.com/topics/mistral",
        "https://github.com/topics/fine-tuning",
        "https://github.com/topics/finetuning",
        "https://github.com/topics/llm",
        "https://github.com/topics/llms",
        "https://github.com/topics/qlora",
        "https://github.com/topics/unsloth",
        "https://github.com/topics/llama3",
        "https://github.com/topics/phi3",
        "https://github.com/topics/gemma2",
        "https://github.com/unslothai/unsloth/#readme-ov-file",
        "https://github.com/unslothai/unsloth/#Apache-2.0-1-ov-file",
        "https://github.com/unslothai/unsloth/custom-properties",
        "https://github.com/unslothai/unsloth/watchers",
        "https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Funslothai%2Funsloth&report=unslothai+%28user%29",
        "https://github.com/unslothai/unsloth/releases",
        "https://github.com/unslothai/unsloth/releases/tag/2025-01",
        "https://github.com/orgs/unslothai/packages?repo_name=unsloth",
        "https://github.com/unslothai/unsloth/graphs/contributors",
        "https://github.com/danielhanchen",
        "https://github.com/Erland366",
        "https://github.com/Datta0",
        "https://github.com/xyangk",
        "https://github.com/sebdg",
        "https://github.com/bet0x",
        "https://github.com/neph1",
        "https://github.com/t-vi",
        "https://github.com/Oseltamivir",
        "https://github.com/chrehall68",
        "https://github.com/mahiatlinux",
        "https://github.com/shaper",
        "https://github.com/Rabbidon",
        "https://github.com/unslothai/unsloth/search?l=python",
        "https://github.com",
        "https://docs.github.com/site-policy/github-terms/github-terms-of-service",
        "https://docs.github.com/site-policy/privacy-policies/github-privacy-statement",
        "https://github.com/security",
        "https://docs.github.com/",
        "https://support.github.com?tags=dotcom-footer",
        "https://github.blog",
        "https://ko-fi.com/unsloth",
        "https://unsloth.ai",
        "https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb",
        "https://discord.gg/unsloth",
        "https://docs.unsloth.ai",
        "https://camo.githubusercontent.com/e93226ff5ba5be911d9fa41864c58db621a09ba17e14fe8f98f63e124431c927/68747470733a2f2f692e6962622e636f2f734a37526847472f696d6167652d34312e706e67",
        "https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb",
        "https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4-Conversational.ipynb",
        "https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb",
        "https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma2_(9B)-Alpaca.ipynb",
        "https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_(7B)-Alpaca.ipynb",
        "https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_(7B)-Conversational.ipynb",
        "https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb",
        "https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-ORPO.ipynb",
        "https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Zephyr_(7B)-DPO.ipynb",
        "https://docs.unsloth.ai/get-started/unsloth-notebooks",
        "https://docs.unsloth.ai/get-started/all-our-models",
        "https://www.kaggle.com/danielhanchen/kaggle-llama-3-2-1b-3b-unsloth-notebook",
        "https://www.kaggle.com/danielhanchen/kaggle-llama-3-1-8b-unsloth-notebook",
        "https://www.kaggle.com/code/danielhanchen/kaggle-gemma-7b-unsloth-notebook/",
        "https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook",
        "https://colab.research.google.com/drive/15OyFkGoCImV9dSsewU1wa2JuKB4-mDE_?usp=sharing",
        "https://colab.research.google.com/drive/15F1xyn8497_dUbxZP4zWmPZ3PJx1Oymv?usp=sharing",
        "https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_(7B)-Text_Completion.ipynb",
        "https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_(7B)-CPT.ipynb",
        "https://docs.unsloth.ai/",
        "https://unsloth.ai/blog/deepseek-r1",
        "https://huggingface.co/collections/unsloth/deepseek-r1-all-versions-678e1c48f5d2fce87892ace5",
        "https://unsloth.ai/blog/phi4",
        "https://huggingface.co/collections/unsloth/phi-4-all-versions-677eecf93784e61afe762afa",
        "https://huggingface.co/collections/unsloth/llama-33-all-versions-67535d7d994794b9d7cf5e9f",
        "https://arxiv.org/abs/2411.09009",
        "https://unsloth.ai/blog/dynamic-4bit",
        "https://huggingface.co/collections/unsloth/unsloth-4-bit-dynamic-quants-67503bb873f89e15276c44e7",
        "https://unsloth.ai/blog/vision",
        "https://colab.research.google.com/drive/1whHb54GNZMrNxIsi2wm2EY_-Pvo2QyKh?usp=sharing",
        "https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_VL_(7B)-Vision.ipynb",
        "https://unsloth.ai/blog/gradient",
        "https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Unsloth_Studio.ipynb",
        "https://unsloth.ai/blog/qwen-coder",
        "https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_Coder_(14B)-Conversational.ipynb",
        "https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_Small_(22B)-Alpaca.ipynb",
        "https://pypi.org/project/unsloth/",
        "https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Inference.ipynb",
        "https://unsloth.ai/blog/long-context",
        "https://camo.githubusercontent.com/2d318d2de87051f80a781377bcf094af5e9a9648beb1f93cb3471cf4d5da0af0/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f362f36662f4c6f676f5f6f665f547769747465722e737667",
        "https://twitter.com/unslothai",
        "https://unsloth.ai/blog",
        "https://camo.githubusercontent.com/366c9dc2ccd72d171a31e5af3cdfb0b42ce61a6f22a20d3e53609ad9ccb47655/68747470733a2f2f726564646974696e632e636f6d2f68732d66732f68756266732f526564646974253230496e632f4272616e642f5265646469745f4c6f676f2e706e67",
        "https://reddit.com/r/unsloth",
        "https://openai.com/research/triton",
        "https://developer.nvidia.com/cuda-gpus",
        "https://unsloth.ai/",
        "https://raw.githubusercontent.com/unslothai/unsloth/main/images/made with unsloth.png",
        "https://unsloth.ai/blog/llama3-3",
        "https://huggingface.co/blog/unsloth-trl",
        "https://docs.anaconda.com/miniconda/",
        "https://pytorch.org",
        "https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth",
        "https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth",
        "https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing",
        "https://www.githubstatus.com/"
    ]
}
LinkedIn and 3rd parties use essential and non-essential cookies to provide, secure, analyze and improve our Services, and to show you relevant ads (including **professional and job ads**) on and off LinkedIn. Learn more in our [Cookie Policy](https://www.linkedin.com/legal/cookie-policy).

Select Accept to consent or Reject to decline non-essential cookies for this use. You can update your choices at any time in your [settings](https://www.linkedin.com/mypreferences/g/guest-cookies).

Accept  Reject 

Agree & Join LinkedIn 

By clicking Continue to join or sign in, you agree to LinkedIn’s [User Agreement](/legal/user-agreement?trk=linkedin-tc_auth-button_user-agreement), [Privacy Policy](/legal/privacy-policy?trk=linkedin-tc_auth-button_privacy-policy), and [Cookie Policy](/legal/cookie-policy?trk=linkedin-tc_auth-button_cookie-policy). 

[ Skip to main content ](#main-content) [ LinkedIn ](/?trk=public_post_nav-header-logo)

  * [ Articles  ](https://www.linkedin.com/pulse/topics/home/?trk=public_post_guest_nav_menu_articles)
  * [ People  ](https://www.linkedin.com/pub/dir/+/+?trk=public_post_guest_nav_menu_people)
  * [ Learning  ](https://www.linkedin.com/learning/search?trk=public_post_guest_nav_menu_learning)
  * [ Jobs  ](https://www.linkedin.com/jobs/search?trk=public_post_guest_nav_menu_jobs)
  * [ Games  ](https://www.linkedin.com/games?trk=public_post_guest_nav_menu_games)
  * [ Get the app  ](ms-windows-store://pdp/?ProductId=9WZDNCRFJ4Q7&mode=mini&cid=guest_nav_upsell&trk=public_post_guest_nav_menu_windows)



[ Join now ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_nav-header-join) [ Sign in ](https://www.linkedin.com/login?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&fromSignIn=true&trk=public_post_nav-header-signin)

#  Ashish Patel 🇮🇳’s Post

[ ](https://in.linkedin.com/in/ashishpatel2604?trk=public_post_feed-actor-image)

[ Ashish Patel 🇮🇳 ](https://in.linkedin.com/in/ashishpatel2604?trk=public_post_feed-actor-name)

🔥 6x LinkedIn Top Voice | Sr AWS AI ML Solution Architect at IBM | Generative AI Expert | Author - Hands-on Time Series Analytics with Python | IBM Quantum ML Certified | 12+ Years in AI | MLOps | IIMA | 100k+Followers 

2mo 

  * [ Report this post ](/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting)



Training a 405B LLM took 16,000 GPUs and 61 days—here’s the real math behind it. Alright, every ML engineer has been there. You’re sitting in a meeting, and someone drops the classic, "So… how long will it take to train this model?" At first, I had no idea how to answer it, and when I tried finding answers, most articles threw a ton of jargon without giving me the actual numbers I needed. Frustrating, right? I decided to dig into it myself and figured out how to do a rough back-of-the-napkin calculation that actually works. Let’s break down the key stuff. 𝗧𝗵𝗲 𝗠𝗮𝘁𝗵: ([https://lnkd.in/dWvgWvXM](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2FdWvgWvXM&urlhash=Tvqa&trk=public_post-text)) ▸ It’s all about FLOPs (floating point operations) and GPU power. Basically, you calculate how many FLOPs your model and data require, then divide it by how much power your GPU setup can handle. ▸ For example, the LLaMA 3.1 model has 405 billion parameters and was trained on 15.6 trillion tokens. In plain English: that means it needed 3.8 × 10²⁵ FLOPs to train (yep, that's an insane number). ▸ To train this beast, they used 16,000 H100 GPUs, each working at about 400 teraflops per second. But here’s the catch—not all GPUs run at full speed. In fact, in this case, the GPUs were running at 38% efficiency due to various network and memory bottlenecks. So, how long did it take to train? Let’s calculate it: 3.8 × 10²⁵ FLOPs ÷ 6.4 × 10¹⁸ FLOPs per second = 61 days But, What About the Cost? 💸 This part always gets me. It took 26 million GPU hours to train LLaMA 3.1. With each GPU costing about $2/hour, the total came out to $52 million! That’s not a typo. I know, it’s wild. 𝗦𝗼, 𝗛𝗼𝘄 𝗗𝗼 𝗬𝗼𝘂 𝗘𝘀𝘁𝗶𝗺𝗮𝘁𝗲 𝗬𝗼𝘂𝗿 𝗢𝘄𝗻 𝗧𝗿𝗮𝗶𝗻𝗶𝗻𝗴 𝗧𝗶𝗺𝗲? ✦ Total FLOPs – Calculate the size of your model (parameters) and multiply it by the size of your dataset (tokens). The formula’s in the images I shared. ✦ GPU FLOPs – Find out how many FLOPs your GPUs can handle (and be honest about that efficiency drop!). ✦ Do the division – FLOPs needed ÷ GPU power = training time. The cool part? Once you know how to calculate this, you stop guessing and start making solid predictions, avoiding those "ummm, not sure" moments with the team. Lessons from LLaMA 3 ⇉ If training a 405B parameter model takes 16,000 GPUs and 61 days, scaling up or down from there gets pretty straightforward. But be warned: don’t just trust theoretical max numbers for your hardware. Use the real-world throughput (MFU) you’re actually getting, or you’ll end up way off. ⇉ This method isn’t flawless, but it’s miles better than guessing. When you’re dropping millions on GPU hours, you definitely want more than a ballpark guess. Would love to hear your thoughts if you've run similar calculations or hit similar roadblocks! Let’s get a conversation going. 👇 [#LLMs](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fllms&trk=public_post-text) [#DataScience](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fdatascience&trk=public_post-text) [#ArtificialIntelligence](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fartificialintelligence&trk=public_post-text)

[ 444  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_social-actions-reactions) [ 36 Comments ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_social-actions-comments)

[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_like-cta) [ Comment  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment-cta)

Share 

  * Copy
  * LinkedIn
  * Facebook
  * Twitter



[ ](https://at.linkedin.com/in/stefan-komornyik?trk=public_post_comment_actor-image)

[ Stefan Komornyik ](https://at.linkedin.com/in/stefan-komornyik?trk=public_post_comment_actor-name)

It is my mission to make time series data valuable.

2mo 

  * [ Report this comment ](/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)



Thank you-Super interesting! Did you also think about considering the energy consumption? ⚡️

[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_like) [ Reply  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_reply) [ 5 Reactions ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_reactions) 6 Reactions 

[ ](https://ch.linkedin.com/in/isabella-kosch-zh?trk=public_post_comment_actor-image)

[ Isabella Kosch ](https://ch.linkedin.com/in/isabella-kosch-zh?trk=public_post_comment_actor-name) 2mo 

  * [ Report this comment ](/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)



Thank you! I just learned something. Really interesting.

[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_like) [ Reply  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_reply) [ 2 Reactions ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_reactions) 3 Reactions 

[ ](https://it.linkedin.com/in/danielebaranzini?trk=public_post_comment_actor-image)

[ Daniele Baranzini ](https://it.linkedin.com/in/danielebaranzini?trk=public_post_comment_actor-name)

HAIKAI

2mo 

  * [ Report this comment ](/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)



totally agree with frends here about energy and consumption issues...but I focus on this other subtle but serious aspect: this AI trajectory is sustainable (and doable) only by FAANG and incoming emerging big AI tech monopolies...this is opposite to AI democratization ...opposite to open source at effort level at least....this instead is towards AI concentration in the hands of a few people in FB Google AWS, NVidia and a few others..... what is partially given out is leaked weights of various (a lot) minor versions of LLM or Multimondal alike ...if you (AI citizen or business company) want to excell on such AI systems you HAVE TO BUY THE API OR THE CLOUD by AWS, IBM, ORACLE GOOGLE etc.... Do not even think to use 1/10 of the effort in the picture above.... the LLM economy is shaping the researchers in AI towards such systems.... problem is not AI ....problem is the economy scale that the FAANG are trying (succesfully) to bring forward : ))) Daniele

[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_like) [ Reply  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_reply) [ 6 Reactions ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_reactions) 7 Reactions 

[ ](https://il.linkedin.com/in/peleg-zborovsky?trk=public_post_comment_actor-image)

[ Peleg Zborovsky ](https://il.linkedin.com/in/peleg-zborovsky?trk=public_post_comment_actor-name)

Machine Learning Engineer at Libonea | B.Sc. | PyTorch🚀

2mo 

  * [ Report this comment ](/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)



[Itzhak Hirschman](https://il.linkedin.com/in/itzhak-hirschman-b16269193?trk=public_post_comment-text) [Dor Getter](https://il.linkedin.com/in/dor-getter-9707711b1?trk=public_post_comment-text) [Zeev Kaminsky](https://il.linkedin.com/in/zeev-kaminsky-024690150?trk=public_post_comment-text) לשאלתכם יש תשובה

[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_like) [ Reply  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_reply) [ 1 Reaction ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_reactions) 2 Reactions 

[ ](https://se.linkedin.com/in/jerry-%C3%A4r-p-189438188?trk=public_post_comment_actor-image)

[ Jerry är P. ](https://se.linkedin.com/in/jerry-%C3%A4r-p-189438188?trk=public_post_comment_actor-name)

AI Advisor @ AiReplyIt Inc | AI Expert, Startup Enthusiast

2mo 

  * [ Report this comment ](/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)



It should be interesting to know the real bottleneck percentage at inference also

[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_like) [ Reply  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_reply) [ 1 Reaction ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_reactions) 2 Reactions 

[ ](https://www.linkedin.com/in/madwesh?trk=public_post_comment_actor-image)

[ Ajay Madwesh ](https://www.linkedin.com/in/madwesh?trk=public_post_comment_actor-name)

Technology Offering Leadership in AI/ML | IoT | Cloud AI/ML | GenAI/LLM

2mo 

  * [ Report this comment ](/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)



This is an unsustainable approach.. can’t wait for the next generation of GenAI (pardon the pun)

[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_like) [ Reply  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_reply) 1 Reaction 

[ ](https://www.linkedin.com/in/visinha?trk=public_post_comment_actor-image)

[ Vishal Sinha ](https://www.linkedin.com/in/visinha?trk=public_post_comment_actor-name) 2mo 

  * [ Report this comment ](/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)



TY for sharing these details. From where did you get 38% GPU utilization data? It looks pretty high.

[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_like) [ Reply  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_reply) 1 Reaction 

[ ](https://www.linkedin.com/company/oktaneai?trk=public_post_comment_actor-image)

[ OKTANE.AI ](https://www.linkedin.com/company/oktaneai?trk=public_post_comment_actor-name) 2mo 

  * [ Report this comment ](/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)



Excellent analysis [Ashish](https://in.linkedin.com/in/ashishpatel2604?trk=public_post_comment-text)! We believe most of the enterprise AI empowered requirements could be solved with RAG (Retrieval Augmented Generation) for a good ROI.

[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_like) [ Reply  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_reply) [ 1 Reaction ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_reactions) 2 Reactions 

[ ](https://ro.linkedin.com/in/pauliusztin?trk=public_post_comment_actor-image)

[ Paul Iusztin ](https://ro.linkedin.com/in/pauliusztin?trk=public_post_comment_actor-name)

Senior ML/AI Engineer • MLOps • Founder @ Decoding ML ~ Posts and articles about building production-grade ML/AI systems.

2mo 

  * [ Report this comment ](/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)



Great summary, man. Love it 😻

[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_like) [ Reply  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_reply) 1 Reaction 

[ See more comments ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_see-more-comments)

To view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_feed-cta-banner-cta)

##  More Relevant Posts 

  * [](https://www.linkedin.com/posts/daemonbehr_genai-aionnutanix-nutanix-activity-7259202801782681600-LAt3)

[ ](https://ca.linkedin.com/in/daemonbehr?trk=public_post_feed-actor-image)

[ Daemon B. ](https://ca.linkedin.com/in/daemonbehr?trk=public_post_feed-actor-name)

Field CTO - Americas - Nutanix | NCX #50 - Enterprise AI Strategic Advisor 

2mo 

    * [ Report this post ](/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdaemonbehr_genai-aionnutanix-nutanix-activity-7259202801782681600-LAt3&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting)

The truth is that most organizations will not train their own models, and if they do, it will not be a 405B parameter model either. Meta's 405B model is not really meant for inferencing anyway. It serves as a "teaching model" that can distill its knowledge to smaller, more efficient models. This process, known as model distillation, allows for the creation of high-performing smaller models with synthetic training data that are more feasible for deployment. Then the small model can actually outperform many larger models. The GPU resources for distillation are a very small fraction of what would be required for training. The other alternatives for using your own data are RAG approaches, which are the most common and least GPU intensive. The questions will always be around what is the desired outcome? What are the steps to get there? What is involved in maintaining it? And what does the cost / benefit / constraint Venn diagram look like? Nutanix provides the easiest on-ramp for organizations to leverage their own data with Enterprise AI, using models that are best suited for your use case. They can be LLMs, or SLMs, distilled, or with RAG, or both. Link here: [https://lnkd.in/ej6g_b5E](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2Fej6g_b5E&urlhash=4gHz&trk=public_post-text) [#GenAI](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fgenai&trk=public_post-text) [#AIonNutanix](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Faionnutanix&trk=public_post-text) [#Nutanix](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fnutanix&trk=public_post-text)

[ ](https://in.linkedin.com/in/ashishpatel2604?trk=public_post_reshare_feed-actor-image)

[ Ashish Patel 🇮🇳 ](https://in.linkedin.com/in/ashishpatel2604?trk=public_post_reshare_feed-actor-name)

🔥 6x LinkedIn Top Voice | Sr AWS AI ML Solution Architect at IBM | Generative AI Expert | Author - Hands-on Time Series Analytics with Python | IBM Quantum ML Certified | 12+ Years in AI | MLOps | IIMA | 100k+Followers 

2mo 

Training a 405B LLM took 16,000 GPUs and 61 days—here’s the real math behind it. Alright, every ML engineer has been there. You’re sitting in a meeting, and someone drops the classic, "So… how long will it take to train this model?" At first, I had no idea how to answer it, and when I tried finding answers, most articles threw a ton of jargon without giving me the actual numbers I needed. Frustrating, right? I decided to dig into it myself and figured out how to do a rough back-of-the-napkin calculation that actually works. Let’s break down the key stuff. 𝗧𝗵𝗲 𝗠𝗮𝘁𝗵: ([https://lnkd.in/dWvgWvXM](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2FdWvgWvXM&urlhash=Tvqa&trk=public_post_reshare-text)) ▸ It’s all about FLOPs (floating point operations) and GPU power. Basically, you calculate how many FLOPs your model and data require, then divide it by how much power your GPU setup can handle. ▸ For example, the LLaMA 3.1 model has 405 billion parameters and was trained on 15.6 trillion tokens. In plain English: that means it needed 3.8 × 10²⁵ FLOPs to train (yep, that's an insane number). ▸ To train this beast, they used 16,000 H100 GPUs, each working at about 400 teraflops per second. But here’s the catch—not all GPUs run at full speed. In fact, in this case, the GPUs were running at 38% efficiency due to various network and memory bottlenecks. So, how long did it take to train? Let’s calculate it: 3.8 × 10²⁵ FLOPs ÷ 6.4 × 10¹⁸ FLOPs per second = 61 days But, What About the Cost? 💸 This part always gets me. It took 26 million GPU hours to train LLaMA 3.1. With each GPU costing about $2/hour, the total came out to $52 million! That’s not a typo. I know, it’s wild. 𝗦𝗼, 𝗛𝗼𝘄 𝗗𝗼 𝗬𝗼𝘂 𝗘𝘀𝘁𝗶𝗺𝗮𝘁𝗲 𝗬𝗼𝘂𝗿 𝗢𝘄𝗻 𝗧𝗿𝗮𝗶𝗻𝗶𝗻𝗴 𝗧𝗶𝗺𝗲? ✦ Total FLOPs – Calculate the size of your model (parameters) and multiply it by the size of your dataset (tokens). The formula’s in the images I shared. ✦ GPU FLOPs – Find out how many FLOPs your GPUs can handle (and be honest about that efficiency drop!). ✦ Do the division – FLOPs needed ÷ GPU power = training time. The cool part? Once you know how to calculate this, you stop guessing and start making solid predictions, avoiding those "ummm, not sure" moments with the team. Lessons from LLaMA 3 ⇉ If training a 405B parameter model takes 16,000 GPUs and 61 days, scaling up or down from there gets pretty straightforward. But be warned: don’t just trust theoretical max numbers for your hardware. Use the real-world throughput (MFU) you’re actually getting, or you’ll end up way off. ⇉ This method isn’t flawless, but it’s miles better than guessing. When you’re dropping millions on GPU hours, you definitely want more than a ballpark guess. Would love to hear your thoughts if you've run similar calculations or hit similar roadblocks! Let’s get a conversation going. 👇 [#LLMs](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fllms&trk=public_post_reshare-text) [#DataScience](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fdatascience&trk=public_post_reshare-text) [#ArtificialIntelligence](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fartificialintelligence&trk=public_post_reshare-text)

[ 9  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdaemonbehr_genai-aionnutanix-nutanix-activity-7259202801782681600-LAt3&trk=public_post_social-actions-reactions) [ 1 Comment ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdaemonbehr_genai-aionnutanix-nutanix-activity-7259202801782681600-LAt3&trk=public_post_social-actions-comments)

[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdaemonbehr_genai-aionnutanix-nutanix-activity-7259202801782681600-LAt3&trk=public_post_like-cta) [ Comment  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdaemonbehr_genai-aionnutanix-nutanix-activity-7259202801782681600-LAt3&trk=public_post_comment-cta)

Share 
    * Copy
    * LinkedIn
    * Facebook
    * Twitter

To view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdaemonbehr_genai-aionnutanix-nutanix-activity-7259202801782681600-LAt3&trk=public_post_feed-cta-banner-cta)

  * [](https://www.linkedin.com/posts/ganeshjagadeesan_llms-artificialintelligence-datascience-activity-7257283675287953409-NxTr)

[ ](https://in.linkedin.com/in/ganeshjagadeesan?trk=public_post_feed-actor-image)

[ Ganesh Jagadeesan ](https://in.linkedin.com/in/ganeshjagadeesan?trk=public_post_feed-actor-name)

Enterprise Data Science Specialist @Mastech Digital | NLP | NER | Deep Learning | Gen AI | MLops 

2mo 

    * [ Report this post ](/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fganeshjagadeesan_llms-artificialintelligence-datascience-activity-7257283675287953409-NxTr&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting)

This post is a goldmine! 💡 Accurately calculating training time for LLMs has always been tricky, and it’s awesome to see a practical breakdown like this. 🔢 FLOPs and GPU throughput aren’t usually discussed in this level of detail—it’s refreshing to see the real-world math behind training something as massive as LLaMA 3.1. 🧠 The efficiency drop you highlighted is such a critical point. Many overlook how network and memory bottlenecks impact real-world GPU performance, and that 38% efficiency makes a big difference in cost and timelines. ⚙️💸 Also, $52 million in GPU hours is mind-blowing! 😲 It really emphasizes how planning and optimization are essential when training models at this scale. Knowing the true throughput you’re getting (instead of trusting theoretical max speeds) is key to avoiding costly surprises. 🏗️📉 I love the step-by-step approach for estimating your own training time—super helpful for anyone navigating those awkward “how long will this take?” moments. 😂 Thanks for sharing the formulas too, they’ll be a lifesaver for future projects. Looking forward to hearing more about others' experiences with training large models—these insights will definitely make our jobs a bit easier! 🙌 [#LLMs](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fllms&trk=public_post-text) [#ArtificialIntelligence](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fartificialintelligence&trk=public_post-text) [#DataScience](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fdatascience&trk=public_post-text) [#MachineLearning](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fmachinelearning&trk=public_post-text) [#GPU](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fgpu&trk=public_post-text) [#ModelTraining](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fmodeltraining&trk=public_post-text) [#AIInsights](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Faiinsights&trk=public_post-text) [#DeepLearning](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fdeeplearning&trk=public_post-text) [#AIOptimization](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Faioptimization&trk=public_post-text)

[ ](https://in.linkedin.com/in/ashishpatel2604?trk=public_post_reshare_feed-actor-image)

[ Ashish Patel 🇮🇳 ](https://in.linkedin.com/in/ashishpatel2604?trk=public_post_reshare_feed-actor-name)

🔥 6x LinkedIn Top Voice | Sr AWS AI ML Solution Architect at IBM | Generative AI Expert | Author - Hands-on Time Series Analytics with Python | IBM Quantum ML Certified | 12+ Years in AI | MLOps | IIMA | 100k+Followers 

2mo 

Training a 405B LLM took 16,000 GPUs and 61 days—here’s the real math behind it. Alright, every ML engineer has been there. You’re sitting in a meeting, and someone drops the classic, "So… how long will it take to train this model?" At first, I had no idea how to answer it, and when I tried finding answers, most articles threw a ton of jargon without giving me the actual numbers I needed. Frustrating, right? I decided to dig into it myself and figured out how to do a rough back-of-the-napkin calculation that actually works. Let’s break down the key stuff. 𝗧𝗵𝗲 𝗠𝗮𝘁𝗵: ([https://lnkd.in/dWvgWvXM](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2FdWvgWvXM&urlhash=Tvqa&trk=public_post_reshare-text)) ▸ It’s all about FLOPs (floating point operations) and GPU power. Basically, you calculate how many FLOPs your model and data require, then divide it by how much power your GPU setup can handle. ▸ For example, the LLaMA 3.1 model has 405 billion parameters and was trained on 15.6 trillion tokens. In plain English: that means it needed 3.8 × 10²⁵ FLOPs to train (yep, that's an insane number). ▸ To train this beast, they used 16,000 H100 GPUs, each working at about 400 teraflops per second. But here’s the catch—not all GPUs run at full speed. In fact, in this case, the GPUs were running at 38% efficiency due to various network and memory bottlenecks. So, how long did it take to train? Let’s calculate it: 3.8 × 10²⁵ FLOPs ÷ 6.4 × 10¹⁸ FLOPs per second = 61 days But, What About the Cost? 💸 This part always gets me. It took 26 million GPU hours to train LLaMA 3.1. With each GPU costing about $2/hour, the total came out to $52 million! That’s not a typo. I know, it’s wild. 𝗦𝗼, 𝗛𝗼𝘄 𝗗𝗼 𝗬𝗼𝘂 𝗘𝘀𝘁𝗶𝗺𝗮𝘁𝗲 𝗬𝗼𝘂𝗿 𝗢𝘄𝗻 𝗧𝗿𝗮𝗶𝗻𝗶𝗻𝗴 𝗧𝗶𝗺𝗲? ✦ Total FLOPs – Calculate the size of your model (parameters) and multiply it by the size of your dataset (tokens). The formula’s in the images I shared. ✦ GPU FLOPs – Find out how many FLOPs your GPUs can handle (and be honest about that efficiency drop!). ✦ Do the division – FLOPs needed ÷ GPU power = training time. The cool part? Once you know how to calculate this, you stop guessing and start making solid predictions, avoiding those "ummm, not sure" moments with the team. Lessons from LLaMA 3 ⇉ If training a 405B parameter model takes 16,000 GPUs and 61 days, scaling up or down from there gets pretty straightforward. But be warned: don’t just trust theoretical max numbers for your hardware. Use the real-world throughput (MFU) you’re actually getting, or you’ll end up way off. ⇉ This method isn’t flawless, but it’s miles better than guessing. When you’re dropping millions on GPU hours, you definitely want more than a ballpark guess. Would love to hear your thoughts if you've run similar calculations or hit similar roadblocks! Let’s get a conversation going. 👇 [#LLMs](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fllms&trk=public_post_reshare-text) [#DataScience](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fdatascience&trk=public_post_reshare-text) [#ArtificialIntelligence](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fartificialintelligence&trk=public_post_reshare-text)

[ 8  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fganeshjagadeesan_llms-artificialintelligence-datascience-activity-7257283675287953409-NxTr&trk=public_post_social-actions-reactions)

[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fganeshjagadeesan_llms-artificialintelligence-datascience-activity-7257283675287953409-NxTr&trk=public_post_like-cta) [ Comment  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fganeshjagadeesan_llms-artificialintelligence-datascience-activity-7257283675287953409-NxTr&trk=public_post_comment-cta)

Share 
    * Copy
    * LinkedIn
    * Facebook
    * Twitter

To view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fganeshjagadeesan_llms-artificialintelligence-datascience-activity-7257283675287953409-NxTr&trk=public_post_feed-cta-banner-cta)

  * [](https://www.linkedin.com/posts/dylan-poh_the-post-sheds-light-on-the-staggering-costs-activity-7258089394434170880-Y08V)

[ ](https://sg.linkedin.com/in/dylan-poh?trk=public_post_feed-actor-image)

[ Dylan Poh ](https://sg.linkedin.com/in/dylan-poh?trk=public_post_feed-actor-name)

AI Engineer at AI Singapore 

2mo 

    * [ Report this post ](/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdylan-poh_the-post-sheds-light-on-the-staggering-costs-activity-7258089394434170880-Y08V&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting)

The post sheds light on the staggering costs and resources needed to train a 405 billion-parameter model: tens of thousands of GPUs, millions of dollars, and massive energy consumption. This scale pushes us to ask tough questions – are we moving toward genuine problem-solving, or just chasing size? Can we make AI more accessible and sustainable, or are we building barriers with resource-heavy giants? As we push these boundaries, balancing innovation with responsibility becomes more critical than ever.

[ ](https://in.linkedin.com/in/ashishpatel2604?trk=public_post_reshare_feed-actor-image)

[ Ashish Patel 🇮🇳 ](https://in.linkedin.com/in/ashishpatel2604?trk=public_post_reshare_feed-actor-name)

🔥 6x LinkedIn Top Voice | Sr AWS AI ML Solution Architect at IBM | Generative AI Expert | Author - Hands-on Time Series Analytics with Python | IBM Quantum ML Certified | 12+ Years in AI | MLOps | IIMA | 100k+Followers 

2mo 

Training a 405B LLM took 16,000 GPUs and 61 days—here’s the real math behind it. Alright, every ML engineer has been there. You’re sitting in a meeting, and someone drops the classic, "So… how long will it take to train this model?" At first, I had no idea how to answer it, and when I tried finding answers, most articles threw a ton of jargon without giving me the actual numbers I needed. Frustrating, right? I decided to dig into it myself and figured out how to do a rough back-of-the-napkin calculation that actually works. Let’s break down the key stuff. 𝗧𝗵𝗲 𝗠𝗮𝘁𝗵: ([https://lnkd.in/dWvgWvXM](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2FdWvgWvXM&urlhash=Tvqa&trk=public_post_reshare-text)) ▸ It’s all about FLOPs (floating point operations) and GPU power. Basically, you calculate how many FLOPs your model and data require, then divide it by how much power your GPU setup can handle. ▸ For example, the LLaMA 3.1 model has 405 billion parameters and was trained on 15.6 trillion tokens. In plain English: that means it needed 3.8 × 10²⁵ FLOPs to train (yep, that's an insane number). ▸ To train this beast, they used 16,000 H100 GPUs, each working at about 400 teraflops per second. But here’s the catch—not all GPUs run at full speed. In fact, in this case, the GPUs were running at 38% efficiency due to various network and memory bottlenecks. So, how long did it take to train? Let’s calculate it: 3.8 × 10²⁵ FLOPs ÷ 6.4 × 10¹⁸ FLOPs per second = 61 days But, What About the Cost? 💸 This part always gets me. It took 26 million GPU hours to train LLaMA 3.1. With each GPU costing about $2/hour, the total came out to $52 million! That’s not a typo. I know, it’s wild. 𝗦𝗼, 𝗛𝗼𝘄 𝗗𝗼 𝗬𝗼𝘂 𝗘𝘀𝘁𝗶𝗺𝗮𝘁𝗲 𝗬𝗼𝘂𝗿 𝗢𝘄𝗻 𝗧𝗿𝗮𝗶𝗻𝗶𝗻𝗴 𝗧𝗶𝗺𝗲? ✦ Total FLOPs – Calculate the size of your model (parameters) and multiply it by the size of your dataset (tokens). The formula’s in the images I shared. ✦ GPU FLOPs – Find out how many FLOPs your GPUs can handle (and be honest about that efficiency drop!). ✦ Do the division – FLOPs needed ÷ GPU power = training time. The cool part? Once you know how to calculate this, you stop guessing and start making solid predictions, avoiding those "ummm, not sure" moments with the team. Lessons from LLaMA 3 ⇉ If training a 405B parameter model takes 16,000 GPUs and 61 days, scaling up or down from there gets pretty straightforward. But be warned: don’t just trust theoretical max numbers for your hardware. Use the real-world throughput (MFU) you’re actually getting, or you’ll end up way off. ⇉ This method isn’t flawless, but it’s miles better than guessing. When you’re dropping millions on GPU hours, you definitely want more than a ballpark guess. Would love to hear your thoughts if you've run similar calculations or hit similar roadblocks! Let’s get a conversation going. 👇 [#LLMs](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fllms&trk=public_post_reshare-text) [#DataScience](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fdatascience&trk=public_post_reshare-text) [#ArtificialIntelligence](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fartificialintelligence&trk=public_post_reshare-text)

[ 6  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdylan-poh_the-post-sheds-light-on-the-staggering-costs-activity-7258089394434170880-Y08V&trk=public_post_social-actions-reactions)

[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdylan-poh_the-post-sheds-light-on-the-staggering-costs-activity-7258089394434170880-Y08V&trk=public_post_like-cta) [ Comment  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdylan-poh_the-post-sheds-light-on-the-staggering-costs-activity-7258089394434170880-Y08V&trk=public_post_comment-cta)

Share 
    * Copy
    * LinkedIn
    * Facebook
    * Twitter

To view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdylan-poh_the-post-sheds-light-on-the-staggering-costs-activity-7258089394434170880-Y08V&trk=public_post_feed-cta-banner-cta)

  * [](https://www.linkedin.com/posts/mazadgole_back-of-the-envelope-calculation-for-gpu-activity-7209639861891317761-VmX7)

[ ](https://ca.linkedin.com/in/mazadgole?trk=public_post_feed-actor-image)

[ Mehdi Azad ](https://ca.linkedin.com/in/mazadgole?trk=public_post_feed-actor-name)

Machine Learning | Data Science | Educator 

7mo  Edited 

    * [ Report this post ](/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmazadgole_back-of-the-envelope-calculation-for-gpu-activity-7209639861891317761-VmX7&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting)

I'm sure that when you want to work with LLMs, these questions come to mind: 1- How many GPUs are required to store and fine-tune LLMs? (Given an NVIDIA V100 GPU with 32GB memory) 2- How long did it take to pre-train LLMs? 3- How can compute-efficient methods, such as quantization or LoRA, reduce the GPU requirements ? To answer the questions, let's do a simple math: "Storage": 1 parameter = 4 bytes. A model with 1B parameters requires 4 GB of memory. With 1 GPU (32 GB), I can perform inference using a model with up to 8 billion parameters. (I can do better with quantization method.) For GPT-3 with 175B parameters, you need ~22 GPUs for just storing the weights. "Fine-Tuning": Considering overheads (gradients and optimizer states), fine-tuning GPT-3 requires approximately x6 times more memory than just storing the weights, around 132 GPUs. So I need ~100 more GPUs for full fine tuning. Good news: with LoRA you can to the fine tuning in just 1 GPU!!! "Pre-training": What about pre-training GPT-3 on a vast amount of data? How many GPUs do I need, and how long does it take? The answer is ~1000 GPUs running for 30 days. Can I do better?? Yes. You can see my simple calculations and interesting results in this blog post. [#LLMs](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fllms&trk=public_post-text) [#GPUs](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fgpus&trk=public_post-text) [#Quantization](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fquantization&trk=public_post-text) [#LoRA](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Flora&trk=public_post-text) [#MachineLearning](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fmachinelearning&trk=public_post-text)

## [ Back-of-the-envelope calculation for GPU requirements of LLMs  mabbasiazad.github.io  ](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Fmabbasiazad%2Egithub%2Eio%2Fportfolio%2Fposts%2Fllms_gpu%2F&urlhash=AWz1&trk=public_post_feed-article-content)

[ 9  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmazadgole_back-of-the-envelope-calculation-for-gpu-activity-7209639861891317761-VmX7&trk=public_post_social-actions-reactions)

[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmazadgole_back-of-the-envelope-calculation-for-gpu-activity-7209639861891317761-VmX7&trk=public_post_like-cta) [ Comment  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmazadgole_back-of-the-envelope-calculation-for-gpu-activity-7209639861891317761-VmX7&trk=public_post_comment-cta)

Share 
    * Copy
    * LinkedIn
    * Facebook
    * Twitter

To view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmazadgole_back-of-the-envelope-calculation-for-gpu-activity-7209639861891317761-VmX7&trk=public_post_feed-cta-banner-cta)

  * [](https://www.linkedin.com/posts/avi-chawla_how-to-synchronize-gpus-with-multi-gpu-training-activity-7222547319672463360-uXQ-)

[ ](https://in.linkedin.com/in/avi-chawla?trk=public_post_feed-actor-image)

[ Avi Chawla ](https://in.linkedin.com/in/avi-chawla?trk=public_post_feed-actor-name)

Co-founder DailyDoseofDS | IIT Varanasi | ex-AI Engineer MastercardAI | Newsletter (130k+) 

5mo 

    * [ Report this post ](/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Favi-chawla_how-to-synchronize-gpus-with-multi-gpu-training-activity-7222547319672463360-uXQ-&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting)

How to synchronize GPUs with multi-GPU training 🧩? A significant run-time bottleneck of multi-GPU training is observed during model synchronization. Consider data parallelism, which: - Replicates the model across all GPUs. - Divides the available data into smaller batches, and each batch is processed by a separate GPU. - Computes the gradients on each GPU and then communicates them to every other GPU. Since every GPU processes a different chunk of the data, the gradients will also be different across devices. Thus, before updating the model parameters on each GPU device, local gradients must be communicated to all other devices. I covered two strategies for intermediate-sized models in yesterday's issue of the [Daily Dose of Data Science](https://in.linkedin.com/company/daily-dose-of-ds?trk=public_post-text) newsletter: [https://lnkd.in/g56-7HsZ](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2Fg56-7HsZ&urlhash=JNnU&trk=public_post-text). 1) All-reduce: - One way is to transfer gradient from every GPU to every other GPU (naive approach). - Another way is to transfer gradients to one GPU, average them, and communicate back to all GPUs (better but not fully optimal and scalable). 2) Ring reduce: - While the total transfers are the same as the second procedure above, ring-reduce is much more scalable and efficient. - It does this with a ring formulation, then segments the local gradients on each device and transfers a segment to the next GPU. We covered it in detail here: [https://lnkd.in/g56-7HsZ](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2Fg56-7HsZ&urlhash=JNnU&trk=public_post-text). 👉 Over to you: Can you optimize ring-reduce even further?

[ 109  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Favi-chawla_how-to-synchronize-gpus-with-multi-gpu-training-activity-7222547319672463360-uXQ-&trk=public_post_social-actions-reactions) [ 1 Comment ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Favi-chawla_how-to-synchronize-gpus-with-multi-gpu-training-activity-7222547319672463360-uXQ-&trk=public_post_social-actions-comments)

[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Favi-chawla_how-to-synchronize-gpus-with-multi-gpu-training-activity-7222547319672463360-uXQ-&trk=public_post_like-cta) [ Comment  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Favi-chawla_how-to-synchronize-gpus-with-multi-gpu-training-activity-7222547319672463360-uXQ-&trk=public_post_comment-cta)

Share 
    * Copy
    * LinkedIn
    * Facebook
    * Twitter

To view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Favi-chawla_how-to-synchronize-gpus-with-multi-gpu-training-activity-7222547319672463360-uXQ-&trk=public_post_feed-cta-banner-cta)

  * [](https://www.linkedin.com/posts/benjamin-j-todd_how-much-ai-inference-can-we-do-activity-7195268236697915393-SFAH)

[ ](https://uk.linkedin.com/in/benjamin-j-todd?trk=public_post_feed-actor-image)

[ Benjamin Todd ](https://uk.linkedin.com/in/benjamin-j-todd?trk=public_post_feed-actor-name)

Founder of 80,000 Hours | author | Writing about AI, careers and doing good 

8mo 

    * [ Report this post ](/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fbenjamin-j-todd_how-much-ai-inference-can-we-do-activity-7195268236697915393-SFAH&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting)

If someone has a bunch of GPUs, how much AI inference can they do with them? The approach I most often see non-experts take is to look up FLOP/s for the GPUs and then divide that by the FLOP per forward pass. E.g. The A100 is listed at 312 teraflop per second on its spec sheet (FP16 tensor), and a forward pass of GPT-4 requires 5.6e11 FLOP per forward pass, which would be 560 per second. But this turns out to be way too high. Even if spec sheet efficiency could be achieved (it can't), the model parameters also need to pass through the GPU's memory. The A100 only has 2000 GB/s of memory bandwidth – only enough for 4 forward passes! But that turns out to be way too low. In reality, multiple GPUs are parallelised and forward passes are processed in batches. This means real world efficiency is somewhere between these upper and lower bounds. Semianalysis estimates for GPT-4 on A100s, it's around 10x the lower bound and 10% of the upper. In the full post, I extend this to more advanced chips, and speculate about it might change in the future: [https://lnkd.in/e5DNbZDK](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2Fe5DNbZDK&urlhash=8KaZ&trk=public_post-text)

## [ How much AI inference can we do?  benjamintodd.substack.com  ](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Fbenjamintodd%2Esubstack%2Ecom%2Fp%2Fhow-much-ai-inference-can-we-do&urlhash=QrZ1&trk=public_post_feed-article-content)

[ 6  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fbenjamin-j-todd_how-much-ai-inference-can-we-do-activity-7195268236697915393-SFAH&trk=public_post_social-actions-reactions)

[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fbenjamin-j-todd_how-much-ai-inference-can-we-do-activity-7195268236697915393-SFAH&trk=public_post_like-cta) [ Comment  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fbenjamin-j-todd_how-much-ai-inference-can-we-do-activity-7195268236697915393-SFAH&trk=public_post_comment-cta)

Share 
    * Copy
    * LinkedIn
    * Facebook
    * Twitter

To view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fbenjamin-j-todd_how-much-ai-inference-can-we-do-activity-7195268236697915393-SFAH&trk=public_post_feed-cta-banner-cta)

  * [](https://www.linkedin.com/posts/martechrichard_training-ai-models-on-cpu-activity-7236981179684466688-zQ9z)

[ ](https://www.linkedin.com/company/martechrichard?trk=public_post_feed-actor-image)

[ MarTechRichard ](https://www.linkedin.com/company/martechrichard?trk=public_post_feed-actor-name)

9 followers 

4mo  Edited 

    * [ Report this post ](/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmartechrichard_training-ai-models-on-cpu-activity-7236981179684466688-zQ9z&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting)

🚀 Exciting times in AI! As GPU demand soars, let’s not overlook the power of CPUs! 🤖💻 🔑 Key insights from our latest article: 1️⃣ CPUs can still excel in ML when GPUs are scarce. 2️⃣ Performance optimization with Intel Xeon & PyTorch can boost efficiency. 3️⃣ Smart data preprocessing minimizes CPU load. 💡 Businesses stand to gain: - Cost-effective solutions for smaller projects 💰 - Scalability without GPU constraints 📈 - Enhanced development productivity ⏲️ How are YOU optimizing CPU performance in your ML tasks? Let’s discuss! 💬 🔗 Read more here: [Training AI Models on CPU - Towards Data Science]([https://lnkd.in/eB5w7N8U](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2FeB5w7N8U&urlhash=0hjW&trk=public_post-text)) 📞 For deeper insights, contact us via WhatsApp: [[https://lnkd.in/e9sTptsu](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2Fe9sTptsu&urlhash=QLXV&trk=public_post-text)) or connect on LinkedIn! 🔗 

## [ Exploring CPU Training for AI Models Amid GPU Resource Limitations  towardsdatascience.com  ](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Ftowardsdatascience%2Ecom%2Ftraining-ai-models-on-cpu-3903adc9f388&urlhash=gBHg&trk=public_post_feed-article-content)

[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmartechrichard_training-ai-models-on-cpu-activity-7236981179684466688-zQ9z&trk=public_post_like-cta) [ Comment  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmartechrichard_training-ai-models-on-cpu-activity-7236981179684466688-zQ9z&trk=public_post_comment-cta)

Share 
    * Copy
    * LinkedIn
    * Facebook
    * Twitter

To view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmartechrichard_training-ai-models-on-cpu-activity-7236981179684466688-zQ9z&trk=public_post_feed-cta-banner-cta)

  * [](https://www.linkedin.com/posts/jay-shah-335689167_cutlass-tutorial-persistent-kernels-and-activity-7276341827874562048-Fxzm)

[ ](https://www.linkedin.com/in/jay-shah-335689167?trk=public_post_feed-actor-image)

[ Jay Shah ](https://www.linkedin.com/in/jay-shah-335689167?trk=public_post_feed-actor-name)

Research Scientist at Colfax International 

4w 

    * [ Report this post ](/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fjay-shah-335689167_cutlass-tutorial-persistent-kernels-and-activity-7276341827874562048-Fxzm&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting)

I'm happy to share the final part of our three part series on writing optimized GEMM kernels for NVIDIA GPUs using CUTLASS library abstractions. This last installment explains the Stream-K algorithm for scheduling work over threadblocks and how it surmounts the problem of wave quantization. To explain the problem, consider a naive "data-parallel" approach to GEMM that partitions the output matrix as a grid of tiles and assigns tiles to threadblocks one-to-one. A discrete number of waves will be launched to process the computation, quantized according to the total number of streaming multiprocessors (SMs). If the number of tiles is then not evenly divisible by the number of SMs, the last tail wave will process a reduced number of work tiles while still taking the time of a full wave, severely degrading overall TFLOPS performance. Roughly speaking, Stream-K addresses this issue by introducing parallelism along the inner K-dimension and enabling threadblocks to "fractionally" process output tiles, rebalancing the distribution of total work across the SMs. Our tutorial also describes CUTLASS's tile scheduler abstraction, which we leverage to effectively implement scheduling strategies like Stream-K. This separates scheduling optimizations that work at the grid level from those that target the inner load and compute loops, like the pipelining and warp specialization strategies we discussed in part 2 of this series. We both implement a simplified version of the Stream-K tile scheduler for a custom GEMM kernel and discuss the inner workings of CUTLASS's more advanced version. Work done in collaboration with my wonderful colleagues at Colfax Research! Happy holidays! [https://lnkd.in/gZkRqYeN](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2FgZkRqYeN&urlhash=DQnO&trk=public_post-text)

## [ CUTLASS Tutorial: Persistent Kernels and Stream-K  https://research.colfax-intl.com  ](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Fresearch%2Ecolfax-intl%2Ecom%2Fcutlass-tutorial-persistent-kernels-and-stream-k%2F&urlhash=tx4R&trk=public_post_feed-article-content)

[ 563  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fjay-shah-335689167_cutlass-tutorial-persistent-kernels-and-activity-7276341827874562048-Fxzm&trk=public_post_social-actions-reactions) [ 5 Comments ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fjay-shah-335689167_cutlass-tutorial-persistent-kernels-and-activity-7276341827874562048-Fxzm&trk=public_post_social-actions-comments)

[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fjay-shah-335689167_cutlass-tutorial-persistent-kernels-and-activity-7276341827874562048-Fxzm&trk=public_post_like-cta) [ Comment  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fjay-shah-335689167_cutlass-tutorial-persistent-kernels-and-activity-7276341827874562048-Fxzm&trk=public_post_comment-cta)

Share 
    * Copy
    * LinkedIn
    * Facebook
    * Twitter

To view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fjay-shah-335689167_cutlass-tutorial-persistent-kernels-and-activity-7276341827874562048-Fxzm&trk=public_post_feed-cta-banner-cta)

  * [](https://www.linkedin.com/posts/rajat-walia_mechanicalengineering-mechanical-aerospace-activity-7205433434528256000-dyOJ)

[ ](https://in.linkedin.com/in/rajat-walia?trk=public_post_feed-actor-image)

[ Rajat Walia ](https://in.linkedin.com/in/rajat-walia?trk=public_post_feed-actor-name) Rajat Walia is an Influencer

CFD Engineer @ Mercedes-Benz | Aerodynamics | Thermal | Aero-Thermal | Computational Fluid Dynamics | Valeo | Formula Student 

7mo 

    * [ Report this post ](/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Frajat-walia_mechanicalengineering-mechanical-aerospace-activity-7205433434528256000-dyOJ&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting)

CPU vs GPU vs TPU! What is the difference between CPU , GPU and TPU and what are the impacts of it in machine learning computations! In the rapidly evolving world of machine learning, the hardware you choose can significantly influence the performance and efficiency of your computations. Let's break down the differences between CPUs, GPUs, and TPUs, and see how each contributes to machine learning. CPU (Central Processing Unit) General-purpose Processor: CPUs are designed to handle a wide range of tasks. They are versatile and capable of performing complex calculations sequentially. Core Count: Typically, CPUs have fewer cores (4-16 cores in most consumer-grade processors) but they are powerful and optimized for a variety of tasks. Machine Learning: Suitable for small-scale machine learning tasks, data preprocessing, and tasks requiring complex decision-making. GPU (Graphics Processing Unit) Parallel Processing Powerhouse: GPUs are designed for parallel processing, making them ideal for handling large-scale computations simultaneously. Core Count: GPUs have thousands of smaller, efficient cores, perfect for tasks that can be parallelized. Machine Learning: Excellent for training deep learning models, image and video processing, and large-scale data operations. GPUs accelerate the training process by handling multiple computations in parallel. TPU (Tensor Processing Unit) ML-Specific Accelerator: TPUs are specialized hardware designed by Google specifically for accelerating machine learning workloads. Optimized for TensorFlow: TPUs are particularly optimized for TensorFlow, Google's open-source machine learning framework. Machine Learning: They provide significant speed-ups for training and inference of neural networks, especially for models with large-scale matrix multiplications. Training Speed: GPUs and TPUs dramatically reduce training time compared to CPUs. Model Complexity: With GPUs and TPUs, you can train more complex models in less time. Scalability: GPUs and TPUs enable scalable solutions for extensive data and model requirements. Mention your thoughts on CPU, GPU & TPU in the comment section! [#mechanicalengineering](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fmechanicalengineering&trk=public_post-text) [#mechanical](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fmechanical&trk=public_post-text) [#aerospace](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Faerospace&trk=public_post-text) [#automotive](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fautomotive&trk=public_post-text) [#cfd](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fcfd&trk=public_post-text)

[ 72  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Frajat-walia_mechanicalengineering-mechanical-aerospace-activity-7205433434528256000-dyOJ&trk=public_post_social-actions-reactions)

[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Frajat-walia_mechanicalengineering-mechanical-aerospace-activity-7205433434528256000-dyOJ&trk=public_post_like-cta) [ Comment  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Frajat-walia_mechanicalengineering-mechanical-aerospace-activity-7205433434528256000-dyOJ&trk=public_post_comment-cta)

Share 
    * Copy
    * LinkedIn
    * Facebook
    * Twitter

To view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Frajat-walia_mechanicalengineering-mechanical-aerospace-activity-7205433434528256000-dyOJ&trk=public_post_feed-cta-banner-cta)

  * [](https://www.linkedin.com/posts/marek-bar%C3%A1k-31977a55_cutlass-tutorial-persistent-kernels-and-activity-7276513592210821120-tuTT)

[ ](https://at.linkedin.com/in/marek-bar%C3%A1k-31977a55?trk=public_post_feed-actor-image)

[ Marek Barák ](https://at.linkedin.com/in/marek-bar%C3%A1k-31977a55?trk=public_post_feed-actor-name)

Data Alchemist & code:Breaker 

4w 

    * [ Report this post ](/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmarek-bar%25C3%25A1k-31977a55_cutlass-tutorial-persistent-kernels-and-activity-7276513592210821120-tuTT&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting)

Again an resource that can be ignored, especially if you are in GPU programming, the skill of 2025+. [#GPU](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fgpu&trk=public_post-text) [#NVIDIA](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fnvidia&trk=public_post-text)

[ ](https://www.linkedin.com/in/jay-shah-335689167?trk=public_post_reshare_feed-actor-image)

[ Jay Shah ](https://www.linkedin.com/in/jay-shah-335689167?trk=public_post_reshare_feed-actor-name)

Research Scientist at Colfax International 

4w 

I'm happy to share the final part of our three part series on writing optimized GEMM kernels for NVIDIA GPUs using CUTLASS library abstractions. This last installment explains the Stream-K algorithm for scheduling work over threadblocks and how it surmounts the problem of wave quantization. To explain the problem, consider a naive "data-parallel" approach to GEMM that partitions the output matrix as a grid of tiles and assigns tiles to threadblocks one-to-one. A discrete number of waves will be launched to process the computation, quantized according to the total number of streaming multiprocessors (SMs). If the number of tiles is then not evenly divisible by the number of SMs, the last tail wave will process a reduced number of work tiles while still taking the time of a full wave, severely degrading overall TFLOPS performance. Roughly speaking, Stream-K addresses this issue by introducing parallelism along the inner K-dimension and enabling threadblocks to "fractionally" process output tiles, rebalancing the distribution of total work across the SMs. Our tutorial also describes CUTLASS's tile scheduler abstraction, which we leverage to effectively implement scheduling strategies like Stream-K. This separates scheduling optimizations that work at the grid level from those that target the inner load and compute loops, like the pipelining and warp specialization strategies we discussed in part 2 of this series. We both implement a simplified version of the Stream-K tile scheduler for a custom GEMM kernel and discuss the inner workings of CUTLASS's more advanced version. Work done in collaboration with my wonderful colleagues at Colfax Research! Happy holidays! [https://lnkd.in/gZkRqYeN](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2FgZkRqYeN&urlhash=DQnO&trk=public_post_reshare-text)

## [ CUTLASS Tutorial: Persistent Kernels and Stream-K  https://research.colfax-intl.com  ](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Fresearch%2Ecolfax-intl%2Ecom%2Fcutlass-tutorial-persistent-kernels-and-stream-k%2F&urlhash=tx4R&trk=public_post_reshare_feed-article-content)

[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmarek-bar%25C3%25A1k-31977a55_cutlass-tutorial-persistent-kernels-and-activity-7276513592210821120-tuTT&trk=public_post_like-cta) [ Comment  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmarek-bar%25C3%25A1k-31977a55_cutlass-tutorial-persistent-kernels-and-activity-7276513592210821120-tuTT&trk=public_post_comment-cta)

Share 
    * Copy
    * LinkedIn
    * Facebook
    * Twitter

To view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmarek-bar%25C3%25A1k-31977a55_cutlass-tutorial-persistent-kernels-and-activity-7276513592210821120-tuTT&trk=public_post_feed-cta-banner-cta)




![](https://media.licdn.com/dms/image/v2/D4D16AQGKG5t7j7VjNw/profile-displaybackgroundimage-shrink_200_800/profile-displaybackgroundimage-shrink_200_800/0/1677477533507?e=2147483647&v=beta&t=BEOsmY67kMP7Ai1g8vbKMu1EV3nNFMPPnIGGO4aHJJU)

95,683 followers 

  * [ 3000+ Posts ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fin%2Fashishpatel2604%2Frecent-activity%2F&trk=public_post_follow-posts)
  * [ 32 Articles ](https://www.linkedin.com/today/author/ashishpatel2604?trk=public_post_follow-articles)



[ View Profile ](https://in.linkedin.com/in/ashishpatel2604?trk=public_post_follow-view-profile) [ Follow ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Ffeed%2Fupdate%2Furn%3Ali%3Aactivity%3A7257270599444512769&trk=public_post_follow)

##  More from this author 

  * ### [ Memory in LLMs Cuts Training Time by 30%—And Here’s What That Means for AI Agent Development  Ashish Patel 🇮🇳  2w  ](https://www.linkedin.com/pulse/memory-llms-cuts-training-time-30and-heres-what-means-patel--hcqaf?trk=public_post)
  * ### [ Over 62% of AI Teams Struggle with Model Deployment — PyTorch’s New Features Solve This, Saving Millions on Development  Ashish Patel 🇮🇳  2mo  ](https://www.linkedin.com/pulse/over-62-ai-teams-struggle-model-deployment-pytorchs-new-patel--nizrf?trk=public_post)
  * ### [ Why Companies Deploying RAG-Powered AI on Kubernetes See a 3x Boost in Customer Personalization  Ashish Patel 🇮🇳  2mo  ](https://www.linkedin.com/pulse/why-companies-deploying-rag-powered-ai-kubernetes-see-patel--qlduc?trk=public_post)



##  Explore topics 

  * [ Sales ](https://www.linkedin.com/pulse/topics/sales-s5/)
  * [ Marketing ](https://www.linkedin.com/pulse/topics/marketing-s2461/)
  * [ IT Services ](https://www.linkedin.com/pulse/topics/it-services-s57547/)
  * [ Business Administration ](https://www.linkedin.com/pulse/topics/business-administration-s50111/)
  * [ HR Management ](https://www.linkedin.com/pulse/topics/hr-management-s50359/)
  * [ Engineering ](https://www.linkedin.com/pulse/topics/engineering-s166/)
  * [ Soft Skills ](https://www.linkedin.com/pulse/topics/soft-skills-s2976/)
  * [ See All ](https://www.linkedin.com/pulse/topics/home/)



  * LinkedIn © 2025
  * [ About ](https://about.linkedin.com?trk=d_public_post_footer-about)
  * [ Accessibility ](https://www.linkedin.com/accessibility?trk=d_public_post_footer-accessibility)
  * [ User Agreement ](https://www.linkedin.com/legal/user-agreement?trk=d_public_post_footer-user-agreement)
  * [ Privacy Policy ](https://www.linkedin.com/legal/privacy-policy?trk=d_public_post_footer-privacy-policy)
  * [ Cookie Policy ](https://www.linkedin.com/legal/cookie-policy?trk=d_public_post_footer-cookie-policy)
  * [ Copyright Policy ](https://www.linkedin.com/legal/copyright-policy?trk=d_public_post_footer-copyright-policy)
  * [ Brand Policy ](https://brand.linkedin.com/policies?trk=d_public_post_footer-brand-policy)
  * [ Guest Controls ](https://www.linkedin.com/psettings/guest-controls?trk=d_public_post_footer-guest-controls)
  * [ Community Guidelines ](https://www.linkedin.com/legal/professional-community-policies?trk=d_public_post_footer-community-guide)
  *     * العربية (Arabic) 
    * বাংলা (Bangla) 
    * Čeština (Czech) 
    * Dansk (Danish) 
    * Deutsch (German) 
    * Ελληνικά (Greek) 
    * **English (English)**
    * Español (Spanish) 
    * فارسی (Persian) 
    * Suomi (Finnish) 
    * Français (French) 
    * हिंदी (Hindi) 
    * Magyar (Hungarian) 
    * Bahasa Indonesia (Indonesian) 
    * Italiano (Italian) 
    * עברית (Hebrew) 
    * 日本語 (Japanese) 
    * 한국어 (Korean) 
    * मराठी (Marathi) 
    * Bahasa Malaysia (Malay) 
    * Nederlands (Dutch) 
    * Norsk (Norwegian) 
    * ਪੰਜਾਬੀ (Punjabi) 
    * Polski (Polish) 
    * Português (Portuguese) 
    * Română (Romanian) 
    * Русский (Russian) 
    * Svenska (Swedish) 
    * తెలుగు (Telugu) 
    * ภาษาไทย (Thai) 
    * Tagalog (Tagalog) 
    * Türkçe (Turkish) 
    * Українська (Ukrainian) 
    * Tiếng Việt (Vietnamese) 
    * 简体中文 (Chinese (Simplified)) 
    * 正體中文 (Chinese (Traditional)) 
Language 




LinkedIn 

Never miss a beat on the app 

Don’t have the app? Get it in the Microsoft Store. 

[ Open the app ](ms-windows-store://pdp/?ProductId=9WZDNCRFJ4Q7&mode=mini&cid=guest_desktop_upsell_post2)

##  Sign in to view more content 

Create your free account or sign in to continue your search 

Sign in 

##  Welcome back 

Email or phone 

Password 

Show

[Forgot password?](https://www.linkedin.com/uas/request-password-reset?trk=public_post_contextual-sign-in-modal_sign-in-modal_forgot_password) Sign in 

or 

By clicking Continue to join or sign in, you agree to LinkedIn’s [User Agreement](/legal/user-agreement?trk=public_post_contextual-sign-in-modal_sign-in-modal_auth-button_user-agreement), [Privacy Policy](/legal/privacy-policy?trk=public_post_contextual-sign-in-modal_sign-in-modal_auth-button_privacy-policy), and [Cookie Policy](/legal/cookie-policy?trk=public_post_contextual-sign-in-modal_sign-in-modal_auth-button_cookie-policy). 

New to LinkedIn? [Join now](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_contextual-sign-in-modal_sign-in-modal_join-link)

or 

New to LinkedIn? [Join now](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_contextual-sign-in-modal_join-link)

By clicking Continue to join or sign in, you agree to LinkedIn’s [User Agreement](/legal/user-agreement?trk=linkedin-tc_auth-button_user-agreement), [Privacy Policy](/legal/privacy-policy?trk=linkedin-tc_auth-button_privacy-policy), and [Cookie Policy](/legal/cookie-policy?trk=linkedin-tc_auth-button_cookie-policy). 

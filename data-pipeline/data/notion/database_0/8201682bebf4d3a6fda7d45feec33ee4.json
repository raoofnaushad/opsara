{
    "id": "8201682bebf4d3a6fda7d45feec33ee4",
    "metadata": {
        "id": "8201682bebf4d3a6fda7d45feec33ee4",
        "url": "https://www.notion.so/Evaluate-LLMs-RAG-8201682bebf4d3a6fda7d45feec33ee4",
        "title": "Evaluate LLMs & RAG",
        "properties": {
            "Type": "Leaf"
        }
    },
    "parent_metadata": {
        "id": "6ffcccecaacdadb42644c95de49a9cd5",
        "url": "",
        "title": "",
        "properties": {}
    },
    "content": "# Articles\n\n\t- [A Metrics-First Approach to LLM Evaluation](https://www.rungalileo.io/blog/metrics-first-approach-to-llm-evaluation?utm_medium=email&_hsmi=304542585&utm_content=304542585&utm_source=hs_automation)\n\n# Repositories\n\n\t- [https://github.com/openai/evals](https://github.com/openai/evals)\n\t- [https://github.com/Giskard-AI/giskard](https://github.com/Giskard-AI/giskard)\n\t- [https://github.com/explodinggradients/ragas](https://github.com/explodinggradients/ragas)\n\n# Videos\n\n\t- [How to evaluate an LLM-powered RAG application automatically.](https://www.youtube.com/watch?v=ZPX3W77h_1E&t=1s)\n\n# Methods to evaluate text tasks\n\n\t- BLEU (bilingual evaluation understudy → Precision-Oriented)\n\t- ROUGE (Recall-Oriented Understudy for Gisting Evaluation)\n\t- BertScore:\n\t\t- [BERTScore: Evaluating Text Generation with BERT](https://arxiv.org/abs/1904.09675)\n\t\n\t- MoverScore:\n\t\t- [MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance](https://arxiv.org/abs/1909.02622)\n\t\n\t- LLM-based metrics:\n\t\t- [https://github.com/openai/evals/blob/main/evals/registry/modelgraded/fact.yaml](https://github.com/openai/evals/blob/main/evals/registry/modelgraded/fact.yaml)\n\t\t- [https://github.com/openai/evals](https://github.com/openai/evals)\n\n\n---\n\n# Benchmarks\n\n\t# General purpose (pre-trained - before SFT)\n\t\n\t- MMLU (knowledge): Tests models on multiple-choice questions across 57 subjects, from\n\telementary to professional levels\n\t- HellaSwag (reasoning): Challenges models to complete a given situation with the most\n\tplausible ending from multiple choices\n\t- ARC-C (reasoning): Evaluates models on grade-school-level multiple-choice science\n\t\tquestions requiring causal reasoning\n\t\n\t- Winogrande (reasoning): Assesses common sense reasoning through pronoun resolution\n\tin carefully crafted sentences\n\t- PIQA (reasoning): Measures physical common sense understanding through questions\n\tabout everyday physical interactions\n\t# General purpose (after SFT)\n\t\n\t- IFEval (instruction following): Assesses a model’s ability to follow instructions with\n\tparticular constraints, like not outputting any commas in your answer\n\t- Chatbot Arena (conversation): A framework where humans vote for the best answer to\n\tan instruction, comparing two models in head-to-head conversations\n\t- AlpacaEval (instruction following): Automatic evaluation for fine-tuned models that is\n\thighly correlated with Chatbot Arena\n\t- MT-Bench (conversation): Evaluates models on multi-turn conversations, testing their\n\tability to maintain context and provide coherent responses\n\t- GAIA (agentic): Tests a wide range of abilities like tool use and web browsing, in a multi-\n\tstep fashion\n\t# Domain-specific (after SFT)\n\t\n\t- Open Medical-LLM Leaderboard: Evaluates the performance of LLMs in medical ques-\n\ttion-answering tasks. It regroups 9 metrics, with 1,273 questions from the US medical li-\n\tcense exams (MedQA), 500 questions from PubMed articles (PubMedQA), 4,183 questions\n\tfrom Indian medical entrance exams (MedMCQA), and 1,089 questions from 6 sub-cate-\n\tgories of MMLU (clinical knowledge, medical genetics, anatomy, professional medicine,\n\tcollege biology, and college medicine).\n\t- BigCodeBench Leaderboard: Evaluates the performance of code LLMs, featuring two main\n\tcategories: BigCodeBench-Complete for code completion based on structured docstrings,\n\tand BigCodeBench-Instruct for code generation from natural language instructions. Mod-\n\tels are ranked by their Pass@1 scores using greedy decoding, with an additional Elo rating\n\tfor the Complete variant. It covers a wide range of programming scenarios that test LLMs’\n\tcompositional reasoning and instruction-following capabilities.\n\t- Hallucinations Leaderboard: Evaluates LLMs’ tendency to produce false or unsupported\n\tinformation across 16 diverse tasks spanning 5 categories. These include Question Answer-\n\ting (with datasets like NQ Open, TruthfulQA, and SQuADv2), Reading Comprehension (using\n\tTriviaQA and RACE), Summarization (employing HaluEval Summ, XSum, and CNN/DM),\n\tDialogue (featuring HaluEval Dial and FaithDial), and Fact Checking (utilizing MemoTrap,\n\tSelfCheckGPT, FEVER, and TrueFalse). The leaderboard also assesses instruction-follow-\n\ting ability using IFEval.\n\t- Enterprise Scenarios Leaderboard: Evaluates the performance of LLMs on six real-world\n\tenterprise use cases, covering diverse tasks relevant to business applications. The bench-\n\tmarks include FinanceBench (100 financial questions with retrieved context), Legal Con-\n\tfidentiality (100 prompts from LegalBench for legal reasoning), Writing Prompts (cre-\n\tative writing evaluation), Customer Support Dialogue (relevance in customer service\n\tinteractions), Toxic Prompts (safety assessment for harmful content generation), and\n\tEnterprise PII (business safety for sensitive information protection). Some test sets are\n\tclosed-source to prevent gaming of the leaderboard. The evaluation focuses on specific\n\tcapabilities such as answer accuracy, legal reasoning, creative writing, contextual rele-\n\tvance, and safety measures, providing a comprehensive assessment of LLMs’ suitability\n\tfor enterprise environments.\n\n# Tools\n\n\t# LLMs\n\t\n\t[Link Preview](https://github.com/EleutherAI/lm-evaluation-harness)\n\t[Link Preview](https://github.com/huggingface/lighteval)\n\t\n\t# RAG / Apps\n\t\n\t[https://aligneval.com/](https://aligneval.com/)\n\t[Link Preview](https://github.com/truera/trulens)\n\t[Link Preview](https://github.com/stanford-futuredata/ARES)",
    "content_quality_score": null,
    "summary": null,
    "child_urls": [
        "https://github.com/explodinggradients/ragas/",
        "https://www.rungalileo.io/blog/metrics-first-approach-to-llm-evaluation?utm_medium=email&_hsmi=304542585&utm_content=304542585&utm_source=hs_automation/",
        "https://github.com/openai/evals/",
        "https://github.com/openai/evals/blob/main/evals/registry/modelgraded/fact.yaml/",
        "https://github.com/huggingface/lighteval/",
        "https://github.com/Giskard-AI/giskard/",
        "https://www.youtube.com/watch?v=ZPX3W77h_1E&t=1s/",
        "https://github.com/truera/trulens/",
        "https://aligneval.com/",
        "https://github.com/EleutherAI/lm-evaluation-harness/",
        "https://github.com/stanford-futuredata/ARES/",
        "https://arxiv.org/abs/1904.09675/",
        "https://arxiv.org/abs/1909.02622/"
    ]
}